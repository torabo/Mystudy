{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J3ynW3BPCC6"
   },
   "source": [
    "# [실습6] 오버피팅 방지 기법 적용 및 최신 딥러닝 모델 TabNet 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk-lDCUMPZW0"
   },
   "source": [
    "## 실습 목표\n",
    "---\n",
    "- MLP 모델에 각종 오버피팅 방지 기법을 적용해 봅니다.\n",
    "- TabNet을 활용해 학습을 진행하고, MLP 모델과 성능을 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cd5eAiXPZIw"
   },
   "source": [
    "## 실습 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-wLf8HYPftn"
   },
   "source": [
    "1. **MLP를 활용한 잉크젯 생성 예측에 오버피팅 방지기법 적용** : 5일차 코드베이스에 오버피팅 방지 기법을 추가로 구현합니다.\n",
    "2. **TabNet 활용한 잉크젯 생성 예측 구현 및 실험** : 정형 데이터에 딥러닝을 활용한 TabNet 모델을 구현하고 성능을 확인합니다.\n",
    "3. **연습문제** : 연습문제를 통해 실제 모델을 구현해보고, 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wus5H9pJPfeP"
   },
   "source": [
    "## 1. MLP를 활용한 잉크젯 생성 데이터 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A1ElIU2PfM0"
   },
   "source": [
    "- 5일차에 구현한 MLP (Multi Layer Perceptron) 모델에 오버피팅 방지 기법을 적용해봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfaD6b1UZG6L"
   },
   "source": [
    "먼저 필요한 라이브러리를 불러옵니다.\n",
    "- **tensorflow의 kerasAPI** 를 활용해 실습을 진행하겠습니다.\n",
    "- **텐서플로(TensorFlow)** : 구글(Google)에서 만든, 딥러닝 프로그램을 쉽게 구현할 수 있도록 다양한 기능을 제공해주는 라이브러리입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2wKG18bmZKY6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 06:48:08.470515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-11 06:48:08.586571: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-11 06:48:08.616276: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-11 06:48:09.324094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-02-11 06:48:09.324164: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-02-11 06:48:09.324171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8AOJurEA79Y"
   },
   "source": [
    "### 1.1 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrZgwklpZPj6"
   },
   "source": [
    "이전 시간에 전처리했던 잉크젯 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M282wXSFZQGx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('InkjetDB_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9gxhjmJuaTMf",
    "outputId": "60158684-1632-4ceb-c375-3ad184590db5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Viscosity</th>\n",
       "      <th>Velocity</th>\n",
       "      <th>PrintingSpeed</th>\n",
       "      <th>PatternSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>175</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Viscosity  Velocity  PrintingSpeed  PatternSize\n",
       "0          8         5            150           19\n",
       "1          5         7             50          224\n",
       "2          8         5            125           29\n",
       "3          8         7             25           88\n",
       "4          5         6            175           61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LfbnP_ugaTJx"
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, 'Viscosity': 'PrintingSpeed']\n",
    "Y = df['PatternSize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIP7ZV9waTHQ",
    "outputId": "4d9023c5-7bc1-44cf-a84b-2b0feda8f6c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "Viscosity          6.080000\n",
      "Velocity           6.537778\n",
      "PrintingSpeed    129.777778\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "Viscosity           3.493571\n",
      "Velocity            1.356825\n",
      "PrintingSpeed    4728.745040\n",
      "dtype: float64\n",
      "\n",
      "=========== 표준화 ==============\n",
      "\n",
      "(Train) feature 들의 평균 값\n",
      "Viscosity       -1.973730e-16\n",
      "Velocity        -1.973730e-17\n",
      "PrintingSpeed   -1.875043e-16\n",
      "dtype: float64\n",
      "\n",
      "(Train) feature 들의 분산 값\n",
      "Viscosity        1.005587\n",
      "Velocity         1.005587\n",
      "PrintingSpeed    1.005587\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 원래 데이터들의 평균과 분산 확인 \n",
    "print('feature 들의 평균 값')\n",
    "print(X.mean())\n",
    "print('\\nfeature 들의 분산 값')\n",
    "print(X.var())\n",
    "\n",
    "# train : val 비율을 8:2로 나누었습니다.\n",
    "X_train = X.loc[:int(len(X)*0.8)-1]\n",
    "X_val = X.loc[int(len(X)*0.8):]\n",
    "Y_train = Y.loc[:int(len(Y)*0.8)-1]\n",
    "Y_val = Y.loc[int(len(Y)*0.8):]\n",
    "print(\"\\n=========== 표준화 ==============\\n\")\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  \n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled=scaler.transform(X_val)\n",
    "\n",
    "#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "X_df_scaled = pd.DataFrame(data=X_train_scaled, columns=X.columns)\n",
    "print('(Train) feature 들의 평균 값')\n",
    "print(X_df_scaled.mean())\n",
    "print('\\n(Train) feature 들의 분산 값')\n",
    "print(X_df_scaled.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1vaAz8oaTEq",
    "outputId": "22dcfcff-72ff-4d4f-c2c1-eddfc8ce9856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_shape :  (180, 3)\n",
      "X_val_shape :  (45, 3)\n",
      "Y_train_shape :  (180,)\n",
      "Y_val_shape : (45,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 06:48:12.901837: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-02-11 06:48:12.901882: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-02-11 06:48:12.901929: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2a0ea417a514): /proc/driver/nvidia/version does not exist\n",
      "2025-02-11 06:48:12.902446: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "X_train=tf.convert_to_tensor(X_train_scaled)\n",
    "X_val=tf.convert_to_tensor(X_val_scaled)\n",
    "Y_train=tf.convert_to_tensor(Y_train)\n",
    "Y_val=tf.convert_to_tensor(Y_val)\n",
    "\n",
    "\n",
    "print('X_train_shape : ', X_train.shape)\n",
    "print('X_val_shape : ', X_val.shape)\n",
    "print('Y_train_shape : ', Y_train.shape)\n",
    "print('Y_val_shape :', Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8A7SjXZBAmy"
   },
   "source": [
    "### 1.2 MLP 베이스 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eS_Eg554ajBj"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.Input(shape = 3))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(300))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WXDG2sQSai-r",
    "outputId": "47e86071-cb81-4fdb-cf44-a5bab1cfabfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               400       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 300)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "loss = MeanSquaredError()\n",
    "epochs = 1000\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQ29MY-fai70",
    "outputId": "11fdd833-8e5e-4c8e-f370-f50da8f440f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 67ms/step - loss: 23044.6230 - mse: 23044.6230 - mae: 102.0755 - val_loss: 34040.4766 - val_mse: 34040.4766 - val_mae: 126.8913\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22911.5742 - mse: 22911.5742 - mae: 101.4699 - val_loss: 33766.8320 - val_mse: 33766.8320 - val_mae: 125.9389\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22605.3691 - mse: 22605.3691 - mae: 100.1040 - val_loss: 33149.7266 - val_mse: 33149.7266 - val_mae: 123.7277\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 50ms/step - loss: 21940.3730 - mse: 21940.3730 - mae: 97.0632 - val_loss: 31756.4609 - val_mse: 31756.4609 - val_mae: 118.7513\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20580.2324 - mse: 20580.2324 - mae: 90.7118 - val_loss: 28724.2168 - val_mse: 28724.2168 - val_mae: 108.5666\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17566.6328 - mse: 17566.6328 - mae: 80.0896 - val_loss: 23013.1191 - val_mse: 23013.1191 - val_mae: 88.7632\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13269.2969 - mse: 13269.2969 - mae: 68.5876 - val_loss: 15479.7490 - val_mse: 15479.7490 - val_mae: 68.5145\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8958.0586 - mse: 8958.0586 - mae: 66.8502 - val_loss: 10491.2637 - val_mse: 10491.2637 - val_mae: 69.5937\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7000.0806 - mse: 7000.0806 - mae: 65.8920 - val_loss: 8113.7891 - val_mse: 8113.7891 - val_mae: 61.8905\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4945.1694 - mse: 4945.1694 - mae: 53.4281 - val_loss: 6525.3027 - val_mse: 6525.3037 - val_mae: 48.4376\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3635.8857 - mse: 3635.8857 - mae: 39.2883 - val_loss: 5791.7153 - val_mse: 5791.7158 - val_mae: 39.6291\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2807.0356 - mse: 2807.0356 - mae: 31.8494 - val_loss: 4630.6006 - val_mse: 4630.6006 - val_mae: 36.2818\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2244.2095 - mse: 2244.2095 - mae: 30.5902 - val_loss: 3961.9368 - val_mse: 3961.9368 - val_mae: 37.7914\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2025.8367 - mse: 2025.8367 - mae: 29.4762 - val_loss: 3603.3694 - val_mse: 3603.3694 - val_mae: 32.3026\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1707.5917 - mse: 1707.5917 - mae: 26.0240 - val_loss: 3286.4761 - val_mse: 3286.4756 - val_mae: 31.4625\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1543.3815 - mse: 1543.3815 - mae: 25.0493 - val_loss: 3106.2837 - val_mse: 3106.2837 - val_mae: 29.4395\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1423.5955 - mse: 1423.5955 - mae: 24.0354 - val_loss: 3023.4661 - val_mse: 3023.4661 - val_mae: 27.9281\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1339.6357 - mse: 1339.6357 - mae: 23.3718 - val_loss: 2818.0974 - val_mse: 2818.0974 - val_mae: 28.1538\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1270.3105 - mse: 1270.3105 - mae: 22.8400 - val_loss: 2735.3364 - val_mse: 2735.3364 - val_mae: 27.2122\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1206.2798 - mse: 1206.2798 - mae: 22.4320 - val_loss: 2622.5708 - val_mse: 2622.5708 - val_mae: 26.8035\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1158.6707 - mse: 1158.6707 - mae: 21.4846 - val_loss: 2647.9111 - val_mse: 2647.9111 - val_mae: 25.5529\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1121.0686 - mse: 1121.0686 - mae: 20.6455 - val_loss: 2544.7300 - val_mse: 2544.7300 - val_mae: 25.1945\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1045.4481 - mse: 1045.4481 - mae: 20.6063 - val_loss: 2351.7595 - val_mse: 2351.7595 - val_mae: 25.5536\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1045.4299 - mse: 1045.4299 - mae: 20.6362 - val_loss: 2349.8667 - val_mse: 2349.8667 - val_mae: 24.6206\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1013.3253 - mse: 1013.3253 - mae: 20.2430 - val_loss: 2185.5117 - val_mse: 2185.5117 - val_mae: 25.0376\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 928.3555 - mse: 928.3555 - mae: 19.2316 - val_loss: 2334.5540 - val_mse: 2334.5540 - val_mae: 24.0718\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 916.6531 - mse: 916.6531 - mae: 18.2853 - val_loss: 2342.4587 - val_mse: 2342.4587 - val_mae: 23.8973\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 894.4583 - mse: 894.4583 - mae: 17.9410 - val_loss: 2178.4661 - val_mse: 2178.4661 - val_mae: 23.3978\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 888.5539 - mse: 888.5539 - mae: 18.5626 - val_loss: 2068.5583 - val_mse: 2068.5583 - val_mae: 23.1622\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 849.4867 - mse: 849.4867 - mae: 17.5818 - val_loss: 2249.2588 - val_mse: 2249.2588 - val_mae: 23.2369\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 806.9479 - mse: 806.9479 - mae: 17.2685 - val_loss: 2025.7610 - val_mse: 2025.7610 - val_mae: 22.6389\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 800.4463 - mse: 800.4463 - mae: 17.7224 - val_loss: 1977.6251 - val_mse: 1977.6251 - val_mae: 22.6768\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 797.6721 - mse: 797.6721 - mae: 17.1490 - val_loss: 2105.1331 - val_mse: 2105.1331 - val_mae: 22.9797\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 766.6172 - mse: 766.6172 - mae: 16.8555 - val_loss: 1951.5653 - val_mse: 1951.5653 - val_mae: 22.2721\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 751.2648 - mse: 751.2648 - mae: 16.9289 - val_loss: 1976.1674 - val_mse: 1976.1674 - val_mae: 22.1387\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 758.0099 - mse: 758.0099 - mae: 16.4925 - val_loss: 2049.8057 - val_mse: 2049.8057 - val_mae: 21.7225\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 739.6635 - mse: 739.6635 - mae: 16.6433 - val_loss: 1816.1594 - val_mse: 1816.1594 - val_mae: 21.5429\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 727.3196 - mse: 727.3196 - mae: 16.6410 - val_loss: 1942.9326 - val_mse: 1942.9326 - val_mae: 21.7938\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 719.0679 - mse: 719.0679 - mae: 16.1633 - val_loss: 1862.7051 - val_mse: 1862.7051 - val_mae: 21.3450\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 696.6426 - mse: 696.6426 - mae: 16.1277 - val_loss: 1929.6382 - val_mse: 1929.6382 - val_mae: 21.0010\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 691.5854 - mse: 691.5854 - mae: 15.9807 - val_loss: 1873.8326 - val_mse: 1873.8326 - val_mae: 20.7509\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 683.0193 - mse: 683.0193 - mae: 15.8902 - val_loss: 1738.4545 - val_mse: 1738.4545 - val_mae: 20.7754\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 676.9056 - mse: 676.9056 - mae: 15.8115 - val_loss: 1714.2365 - val_mse: 1714.2365 - val_mae: 20.7512\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 670.6813 - mse: 670.6813 - mae: 15.6636 - val_loss: 1746.8236 - val_mse: 1746.8236 - val_mae: 20.5591\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 653.9436 - mse: 653.9436 - mae: 15.4236 - val_loss: 1783.5588 - val_mse: 1783.5588 - val_mae: 20.2871\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 658.8870 - mse: 658.8870 - mae: 15.3887 - val_loss: 1712.1299 - val_mse: 1712.1299 - val_mae: 19.9943\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 646.7612 - mse: 646.7612 - mae: 15.2159 - val_loss: 1762.5470 - val_mse: 1762.5470 - val_mae: 19.9438\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 632.5897 - mse: 632.5897 - mae: 14.9708 - val_loss: 1607.4374 - val_mse: 1607.4374 - val_mae: 19.6289\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 648.7862 - mse: 648.7862 - mae: 15.1014 - val_loss: 1634.0599 - val_mse: 1634.0597 - val_mae: 19.9062\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 649.6550 - mse: 649.6550 - mae: 14.8771 - val_loss: 1776.8405 - val_mse: 1776.8405 - val_mae: 20.0405\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 625.4114 - mse: 625.4114 - mae: 14.7070 - val_loss: 1592.4840 - val_mse: 1592.4840 - val_mae: 19.5619\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 631.0151 - mse: 631.0151 - mae: 14.8475 - val_loss: 1623.1547 - val_mse: 1623.1545 - val_mae: 19.2256\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 648.0366 - mse: 648.0366 - mae: 14.6069 - val_loss: 1677.3837 - val_mse: 1677.3837 - val_mae: 19.2695\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.5939 - mse: 597.5939 - mae: 14.2352 - val_loss: 1472.9589 - val_mse: 1472.9590 - val_mae: 18.7823\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 609.8692 - mse: 609.8692 - mae: 14.1914 - val_loss: 1561.2825 - val_mse: 1561.2825 - val_mae: 18.9205\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 579.8467 - mse: 579.8467 - mae: 13.8538 - val_loss: 1580.1903 - val_mse: 1580.1903 - val_mae: 18.8525\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 576.9957 - mse: 576.9957 - mae: 13.6071 - val_loss: 1603.6431 - val_mse: 1603.6431 - val_mae: 18.6525\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 580.8250 - mse: 580.8250 - mae: 13.6215 - val_loss: 1497.6274 - val_mse: 1497.6276 - val_mae: 18.5958\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 564.9549 - mse: 564.9549 - mae: 13.4990 - val_loss: 1601.3453 - val_mse: 1601.3453 - val_mae: 18.5084\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 601.9831 - mse: 601.9831 - mae: 13.5782 - val_loss: 1643.3116 - val_mse: 1643.3116 - val_mae: 18.5645\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 547.4804 - mse: 547.4804 - mae: 13.4016 - val_loss: 1394.5132 - val_mse: 1394.5132 - val_mae: 17.8972\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 545.5599 - mse: 545.5599 - mae: 13.1575 - val_loss: 1557.3181 - val_mse: 1557.3181 - val_mae: 18.6965\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 540.6065 - mse: 540.6065 - mae: 12.7254 - val_loss: 1550.5990 - val_mse: 1550.5990 - val_mae: 17.9295\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 529.8772 - mse: 529.8772 - mae: 12.8208 - val_loss: 1564.5934 - val_mse: 1564.5934 - val_mae: 17.6858\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 523.8647 - mse: 523.8647 - mae: 12.4408 - val_loss: 1466.5219 - val_mse: 1466.5219 - val_mae: 17.4367\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.8263 - mse: 509.8263 - mae: 12.1468 - val_loss: 1508.5884 - val_mse: 1508.5885 - val_mae: 17.6118\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 506.0670 - mse: 506.0670 - mae: 12.1103 - val_loss: 1468.9781 - val_mse: 1468.9781 - val_mae: 17.3103\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.9545 - mse: 509.9545 - mae: 12.0142 - val_loss: 1511.9983 - val_mse: 1511.9983 - val_mae: 17.3736\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 486.5471 - mse: 486.5471 - mae: 11.9132 - val_loss: 1309.2227 - val_mse: 1309.2227 - val_mae: 16.6095\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 495.8795 - mse: 495.8795 - mae: 11.8768 - val_loss: 1459.4938 - val_mse: 1459.4938 - val_mae: 17.9340\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 490.7026 - mse: 490.7026 - mae: 11.5628 - val_loss: 1541.5963 - val_mse: 1541.5963 - val_mae: 17.4680\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 497.6453 - mse: 497.6453 - mae: 12.3494 - val_loss: 1325.5256 - val_mse: 1325.5256 - val_mae: 16.5600\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 468.9249 - mse: 468.9249 - mae: 11.4523 - val_loss: 1453.7063 - val_mse: 1453.7063 - val_mae: 17.9097\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 485.5338 - mse: 485.5338 - mae: 11.4012 - val_loss: 1395.5533 - val_mse: 1395.5532 - val_mae: 16.6340\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 456.1630 - mse: 456.1630 - mae: 11.1143 - val_loss: 1388.6138 - val_mse: 1388.6138 - val_mae: 16.7795\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 466.0547 - mse: 466.0547 - mae: 10.9468 - val_loss: 1311.6801 - val_mse: 1311.6801 - val_mae: 16.6360\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 481.8094 - mse: 481.8094 - mae: 11.1296 - val_loss: 1318.1298 - val_mse: 1318.1298 - val_mae: 17.0234\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 468.6673 - mse: 468.6673 - mae: 10.8948 - val_loss: 1558.6016 - val_mse: 1558.6016 - val_mae: 17.3407\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 482.2871 - mse: 482.2871 - mae: 11.8055 - val_loss: 1232.7520 - val_mse: 1232.7520 - val_mae: 16.1555\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 433.8150 - mse: 433.8150 - mae: 11.1169 - val_loss: 1372.2924 - val_mse: 1372.2925 - val_mae: 17.4168\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 419.0018 - mse: 419.0018 - mae: 10.5488 - val_loss: 1379.3151 - val_mse: 1379.3152 - val_mae: 16.4580\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 433.8901 - mse: 433.8901 - mae: 10.7808 - val_loss: 1352.1285 - val_mse: 1352.1285 - val_mae: 16.4456\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 421.3799 - mse: 421.3799 - mae: 10.4224 - val_loss: 1254.5645 - val_mse: 1254.5645 - val_mae: 16.7964\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 414.8043 - mse: 414.8043 - mae: 10.5309 - val_loss: 1337.4210 - val_mse: 1337.4210 - val_mae: 16.3567\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 400.9013 - mse: 400.9013 - mae: 10.8752 - val_loss: 1230.3896 - val_mse: 1230.3896 - val_mae: 15.9442\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 389.9466 - mse: 389.9466 - mae: 10.1137 - val_loss: 1357.9301 - val_mse: 1357.9299 - val_mae: 17.0381\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 396.3952 - mse: 396.3952 - mae: 10.1640 - val_loss: 1219.1666 - val_mse: 1219.1666 - val_mae: 15.7068\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 387.7977 - mse: 387.7977 - mae: 9.8979 - val_loss: 1352.3877 - val_mse: 1352.3877 - val_mae: 17.5826\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 395.0089 - mse: 395.0089 - mae: 10.1989 - val_loss: 1289.9653 - val_mse: 1289.9653 - val_mae: 15.8795\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.9654 - mse: 367.9654 - mae: 10.2941 - val_loss: 1179.2378 - val_mse: 1179.2378 - val_mae: 15.8086\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 368.4923 - mse: 368.4923 - mae: 9.9423 - val_loss: 1316.7623 - val_mse: 1316.7623 - val_mae: 16.7924\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.0909 - mse: 367.0909 - mae: 9.6858 - val_loss: 1230.6915 - val_mse: 1230.6915 - val_mae: 15.4947\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 391.6173 - mse: 391.6173 - mae: 10.5353 - val_loss: 1152.3083 - val_mse: 1152.3083 - val_mae: 16.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 383.4637 - mse: 383.4637 - mae: 10.2351 - val_loss: 1221.4025 - val_mse: 1221.4025 - val_mae: 15.4350\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 364.6259 - mse: 364.6259 - mae: 10.6244 - val_loss: 1137.0573 - val_mse: 1137.0573 - val_mae: 15.2394\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 336.1565 - mse: 336.1565 - mae: 9.8314 - val_loss: 1240.1919 - val_mse: 1240.1919 - val_mae: 16.8795\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 327.6776 - mse: 327.6776 - mae: 9.4503 - val_loss: 1111.6229 - val_mse: 1111.6229 - val_mae: 15.1227\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 365.6926 - mse: 365.6926 - mae: 10.0338 - val_loss: 1163.2178 - val_mse: 1163.2178 - val_mae: 15.5219\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 305.8746 - mse: 305.8746 - mae: 8.8791 - val_loss: 936.7612 - val_mse: 936.7612 - val_mae: 14.3463\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 360.9781 - mse: 360.9781 - mae: 9.4583 - val_loss: 1143.1354 - val_mse: 1143.1354 - val_mae: 15.2708\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 323.2184 - mse: 323.2184 - mae: 9.1170 - val_loss: 1127.4269 - val_mse: 1127.4269 - val_mae: 15.0203\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 356.3565 - mse: 356.3565 - mae: 9.2814 - val_loss: 953.7115 - val_mse: 953.7115 - val_mae: 14.8404\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.4644 - mse: 288.4644 - mae: 9.5545 - val_loss: 1209.5419 - val_mse: 1209.5419 - val_mae: 16.0705\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 318.2960 - mse: 318.2960 - mae: 9.7052 - val_loss: 1199.6174 - val_mse: 1199.6174 - val_mae: 15.2829\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 302.9048 - mse: 302.9048 - mae: 9.8485 - val_loss: 1006.1413 - val_mse: 1006.1413 - val_mae: 15.1254\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 326.1218 - mse: 326.1218 - mae: 10.2576 - val_loss: 1121.1880 - val_mse: 1121.1880 - val_mae: 15.1767\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 278.6600 - mse: 278.6600 - mae: 8.8391 - val_loss: 961.3874 - val_mse: 961.3874 - val_mae: 14.0410\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 316.3032 - mse: 316.3032 - mae: 9.6845 - val_loss: 1159.9261 - val_mse: 1159.9261 - val_mae: 15.7959\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 279.0302 - mse: 279.0302 - mae: 8.5639 - val_loss: 891.3610 - val_mse: 891.3610 - val_mae: 13.8312\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 272.5458 - mse: 272.5458 - mae: 8.5997 - val_loss: 1072.3224 - val_mse: 1072.3224 - val_mae: 14.7314\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 269.4948 - mse: 269.4948 - mae: 8.0799 - val_loss: 923.5956 - val_mse: 923.5956 - val_mae: 13.7951\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.7628 - mse: 262.7628 - mae: 8.0879 - val_loss: 1074.4305 - val_mse: 1074.4305 - val_mae: 14.1830\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 300.3406 - mse: 300.3406 - mae: 8.9600 - val_loss: 913.5305 - val_mse: 913.5305 - val_mae: 13.9750\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.9201 - mse: 242.9201 - mae: 8.4652 - val_loss: 1095.1763 - val_mse: 1095.1763 - val_mae: 14.8120\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 256.8101 - mse: 256.8101 - mae: 8.2564 - val_loss: 958.3509 - val_mse: 958.3509 - val_mae: 13.5560\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.8792 - mse: 237.8792 - mae: 8.1157 - val_loss: 986.4011 - val_mse: 986.4011 - val_mae: 14.7956\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.3086 - mse: 244.3086 - mae: 8.4165 - val_loss: 927.6055 - val_mse: 927.6055 - val_mae: 13.5260\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.2078 - mse: 228.2078 - mae: 7.9433 - val_loss: 991.4656 - val_mse: 991.4656 - val_mae: 13.7318\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 250.7543 - mse: 250.7543 - mae: 7.8928 - val_loss: 932.6045 - val_mse: 932.6045 - val_mae: 13.3243\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.6164 - mse: 240.6164 - mae: 8.1296 - val_loss: 847.2773 - val_mse: 847.2773 - val_mae: 13.5434\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.9430 - mse: 207.9430 - mae: 7.5261 - val_loss: 1087.9236 - val_mse: 1087.9236 - val_mae: 14.4421\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.5535 - mse: 230.5535 - mae: 8.1780 - val_loss: 859.0601 - val_mse: 859.0601 - val_mae: 12.6986\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 213.1120 - mse: 213.1120 - mae: 7.7268 - val_loss: 933.3853 - val_mse: 933.3853 - val_mae: 13.7865\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.1577 - mse: 235.1577 - mae: 8.0839 - val_loss: 785.7479 - val_mse: 785.7479 - val_mae: 12.6589\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 177.8308 - mse: 177.8308 - mae: 7.1402 - val_loss: 1120.2700 - val_mse: 1120.2700 - val_mae: 15.3091\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 259.0287 - mse: 259.0287 - mae: 8.4818 - val_loss: 943.4913 - val_mse: 943.4913 - val_mae: 13.1516\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266.1063 - mse: 266.1063 - mae: 9.1629 - val_loss: 736.2681 - val_mse: 736.2681 - val_mae: 14.9321\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.5537 - mse: 248.5537 - mae: 10.9233 - val_loss: 1001.3954 - val_mse: 1001.3954 - val_mae: 14.8496\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.1977 - mse: 210.1977 - mae: 9.1331 - val_loss: 839.8612 - val_mse: 839.8612 - val_mae: 12.8376\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 196.0419 - mse: 196.0419 - mae: 7.9737 - val_loss: 918.9227 - val_mse: 918.9227 - val_mae: 14.1743\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 188.1143 - mse: 188.1143 - mae: 7.8299 - val_loss: 751.6962 - val_mse: 751.6962 - val_mae: 12.1502\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 166.0729 - mse: 166.0729 - mae: 7.0780 - val_loss: 944.5248 - val_mse: 944.5248 - val_mae: 13.5010\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 173.0878 - mse: 173.0878 - mae: 6.9890 - val_loss: 853.3635 - val_mse: 853.3635 - val_mae: 12.3582\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 166.1694 - mse: 166.1694 - mae: 6.6470 - val_loss: 741.9811 - val_mse: 741.9811 - val_mae: 12.2874\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 181.3101 - mse: 181.3101 - mae: 7.2935 - val_loss: 906.4116 - val_mse: 906.4116 - val_mae: 12.5630\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 171.6660 - mse: 171.6660 - mae: 7.5837 - val_loss: 727.3079 - val_mse: 727.3079 - val_mae: 11.7553\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 170.8889 - mse: 170.8889 - mae: 7.4360 - val_loss: 823.3440 - val_mse: 823.3440 - val_mae: 12.8926\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 156.1118 - mse: 156.1118 - mae: 7.0614 - val_loss: 732.1573 - val_mse: 732.1573 - val_mae: 11.6378\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 146.3893 - mse: 146.3893 - mae: 6.3716 - val_loss: 890.8405 - val_mse: 890.8405 - val_mae: 13.3883\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 145.3266 - mse: 145.3266 - mae: 6.5655 - val_loss: 713.7682 - val_mse: 713.7683 - val_mae: 11.3238\n",
      "Epoch 141/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 143.8549 - mse: 143.8549 - mae: 6.7120 - val_loss: 766.2333 - val_mse: 766.2333 - val_mae: 12.6525\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 141.5122 - mse: 141.5122 - mae: 6.7145 - val_loss: 751.6144 - val_mse: 751.6144 - val_mae: 11.6269\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144.8189 - mse: 144.8189 - mae: 6.9976 - val_loss: 763.7007 - val_mse: 763.7007 - val_mae: 12.3430\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 149.0712 - mse: 149.0712 - mae: 6.7069 - val_loss: 829.4526 - val_mse: 829.4526 - val_mae: 12.4708\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 127.9111 - mse: 127.9111 - mae: 6.5156 - val_loss: 647.4346 - val_mse: 647.4346 - val_mae: 11.2186\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 130.1726 - mse: 130.1726 - mae: 6.6467 - val_loss: 784.0629 - val_mse: 784.0629 - val_mae: 13.2817\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 124.6130 - mse: 124.6130 - mae: 6.3533 - val_loss: 744.9174 - val_mse: 744.9175 - val_mae: 11.5356\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.8461 - mse: 120.8461 - mae: 6.4785 - val_loss: 763.0169 - val_mse: 763.0170 - val_mae: 12.8592\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 131.3616 - mse: 131.3616 - mae: 6.5710 - val_loss: 723.5096 - val_mse: 723.5096 - val_mae: 11.6788\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117.8561 - mse: 117.8561 - mae: 6.2038 - val_loss: 697.2606 - val_mse: 697.2606 - val_mae: 11.8284\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 113.9548 - mse: 113.9548 - mae: 6.0787 - val_loss: 693.4004 - val_mse: 693.4004 - val_mae: 11.7310\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104.7977 - mse: 104.7977 - mae: 5.7595 - val_loss: 691.4189 - val_mse: 691.4189 - val_mae: 11.6983\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 102.1593 - mse: 102.1593 - mae: 5.6353 - val_loss: 744.7463 - val_mse: 744.7463 - val_mae: 12.2763\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 99.5520 - mse: 99.5520 - mae: 5.5841 - val_loss: 663.1865 - val_mse: 663.1865 - val_mae: 11.5597\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 99.3991 - mse: 99.3991 - mae: 5.5975 - val_loss: 729.0847 - val_mse: 729.0847 - val_mae: 12.0520\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 97.0039 - mse: 97.0039 - mae: 5.5829 - val_loss: 677.3380 - val_mse: 677.3380 - val_mae: 11.6336\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.6348 - mse: 98.6348 - mae: 5.5479 - val_loss: 708.1404 - val_mse: 708.1404 - val_mae: 12.5062\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104.2775 - mse: 104.2775 - mae: 5.7410 - val_loss: 599.7344 - val_mse: 599.7344 - val_mae: 11.0041\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 99.7234 - mse: 99.7234 - mae: 5.8980 - val_loss: 715.3154 - val_mse: 715.3154 - val_mae: 13.0818\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 99.9378 - mse: 99.9378 - mae: 6.0038 - val_loss: 661.4616 - val_mse: 661.4616 - val_mae: 11.3498\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87.1770 - mse: 87.1770 - mae: 5.4603 - val_loss: 765.6045 - val_mse: 765.6045 - val_mae: 12.2504\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81.5683 - mse: 81.5683 - mae: 5.1809 - val_loss: 625.0043 - val_mse: 625.0043 - val_mae: 11.3313\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85.3007 - mse: 85.3007 - mae: 5.4653 - val_loss: 646.3387 - val_mse: 646.3387 - val_mae: 11.8674\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81.7280 - mse: 81.7280 - mae: 5.3354 - val_loss: 651.3584 - val_mse: 651.3584 - val_mae: 11.3159\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 84.8571 - mse: 84.8571 - mae: 5.3356 - val_loss: 688.6924 - val_mse: 688.6924 - val_mae: 12.6982\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 92.1235 - mse: 92.1235 - mae: 5.6692 - val_loss: 679.7049 - val_mse: 679.7048 - val_mae: 11.1269\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 78.1240 - mse: 78.1240 - mae: 5.3602 - val_loss: 602.9600 - val_mse: 602.9600 - val_mae: 11.4766\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74.4462 - mse: 74.4462 - mae: 5.2922 - val_loss: 682.0016 - val_mse: 682.0016 - val_mae: 12.0485\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74.6632 - mse: 74.6632 - mae: 5.3008 - val_loss: 639.2399 - val_mse: 639.2399 - val_mae: 11.3021\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68.9179 - mse: 68.9179 - mae: 5.0123 - val_loss: 646.8106 - val_mse: 646.8106 - val_mae: 12.4728\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68.7542 - mse: 68.7542 - mae: 4.9723 - val_loss: 580.6048 - val_mse: 580.6048 - val_mae: 10.8848\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66.1276 - mse: 66.1276 - mae: 4.8572 - val_loss: 669.8530 - val_mse: 669.8530 - val_mae: 12.3827\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 67.6425 - mse: 67.6425 - mae: 4.7465 - val_loss: 619.3303 - val_mse: 619.3303 - val_mae: 11.3035\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 62.4056 - mse: 62.4056 - mae: 4.6999 - val_loss: 630.2068 - val_mse: 630.2068 - val_mae: 11.8208\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 63.6289 - mse: 63.6289 - mae: 4.7242 - val_loss: 604.9496 - val_mse: 604.9496 - val_mae: 11.9828\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 56.8627 - mse: 56.8627 - mae: 4.5743 - val_loss: 702.4692 - val_mse: 702.4693 - val_mae: 12.0464\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.3429 - mse: 64.3429 - mae: 4.6754 - val_loss: 588.9710 - val_mse: 588.9710 - val_mae: 11.3551\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 62.4965 - mse: 62.4965 - mae: 4.8250 - val_loss: 577.3749 - val_mse: 577.3749 - val_mae: 11.4357\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66.9834 - mse: 66.9834 - mae: 5.2671 - val_loss: 637.3987 - val_mse: 637.3987 - val_mae: 12.5472\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.6626 - mse: 64.6626 - mae: 4.8984 - val_loss: 575.9629 - val_mse: 575.9629 - val_mae: 11.0294\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.5299 - mse: 64.5299 - mae: 4.8666 - val_loss: 725.6714 - val_mse: 725.6714 - val_mae: 13.8931\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.2417 - mse: 65.2417 - mae: 5.2291 - val_loss: 561.0908 - val_mse: 561.0908 - val_mae: 11.0583\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 54.4959 - mse: 54.4959 - mae: 4.7952 - val_loss: 619.3103 - val_mse: 619.3103 - val_mae: 12.5136\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 51.6478 - mse: 51.6478 - mae: 4.4281 - val_loss: 542.1000 - val_mse: 542.1000 - val_mae: 10.9946\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 53.0534 - mse: 53.0534 - mae: 4.4920 - val_loss: 630.8438 - val_mse: 630.8438 - val_mae: 11.9430\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 51.1975 - mse: 51.1975 - mae: 4.3785 - val_loss: 557.7401 - val_mse: 557.7401 - val_mae: 11.6108\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 47.9598 - mse: 47.9598 - mae: 4.2909 - val_loss: 608.5810 - val_mse: 608.5810 - val_mae: 11.8666\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.1486 - mse: 48.1486 - mae: 4.1797 - val_loss: 540.9504 - val_mse: 540.9504 - val_mae: 11.1039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 51.9605 - mse: 51.9605 - mae: 4.4175 - val_loss: 614.4159 - val_mse: 614.4159 - val_mae: 12.1862\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.4086 - mse: 48.4086 - mae: 4.2109 - val_loss: 581.7614 - val_mse: 581.7614 - val_mae: 11.7518\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 45.1997 - mse: 45.1997 - mae: 4.0976 - val_loss: 543.9628 - val_mse: 543.9628 - val_mae: 11.4082\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45.3278 - mse: 45.3278 - mae: 4.0756 - val_loss: 611.2523 - val_mse: 611.2523 - val_mae: 12.2357\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 50.3673 - mse: 50.3673 - mae: 4.5038 - val_loss: 539.7898 - val_mse: 539.7898 - val_mae: 10.8717\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.2368 - mse: 43.2368 - mae: 4.2270 - val_loss: 578.2372 - val_mse: 578.2372 - val_mae: 12.6741\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.2032 - mse: 44.2032 - mae: 4.0451 - val_loss: 506.9591 - val_mse: 506.9591 - val_mae: 11.0137\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45.5502 - mse: 45.5502 - mae: 4.3611 - val_loss: 563.8956 - val_mse: 563.8956 - val_mae: 11.9617\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.5901 - mse: 40.5901 - mae: 4.1622 - val_loss: 500.5932 - val_mse: 500.5932 - val_mae: 11.0723\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.7016 - mse: 40.7016 - mae: 4.1098 - val_loss: 583.6741 - val_mse: 583.6741 - val_mae: 11.8624\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.7857 - mse: 44.7857 - mae: 4.0441 - val_loss: 497.9867 - val_mse: 497.9867 - val_mae: 10.6815\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.6677 - mse: 40.6677 - mae: 4.1135 - val_loss: 620.1915 - val_mse: 620.1915 - val_mae: 12.4448\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 41.4792 - mse: 41.4792 - mae: 4.1067 - val_loss: 533.9659 - val_mse: 533.9659 - val_mae: 11.1225\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.3913 - mse: 40.3913 - mae: 3.9379 - val_loss: 533.3103 - val_mse: 533.3103 - val_mae: 11.6468\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.6358 - mse: 41.6358 - mae: 3.9872 - val_loss: 612.3425 - val_mse: 612.3425 - val_mae: 12.8912\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.3848 - mse: 38.3848 - mae: 4.0189 - val_loss: 453.8535 - val_mse: 453.8535 - val_mae: 10.3647\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 42.3084 - mse: 42.3084 - mae: 4.0693 - val_loss: 571.6143 - val_mse: 571.6143 - val_mae: 12.4283\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.3934 - mse: 39.3934 - mae: 3.9311 - val_loss: 565.3248 - val_mse: 565.3248 - val_mae: 11.8114\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.7907 - mse: 39.7907 - mae: 3.9577 - val_loss: 521.2569 - val_mse: 521.2569 - val_mae: 10.9695\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.2019 - mse: 36.2019 - mae: 3.7515 - val_loss: 576.5658 - val_mse: 576.5658 - val_mae: 12.5135\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.1265 - mse: 34.1265 - mae: 3.6537 - val_loss: 537.1302 - val_mse: 537.1302 - val_mae: 11.1799\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.8117 - mse: 32.8117 - mae: 3.6057 - val_loss: 544.4309 - val_mse: 544.4309 - val_mae: 11.9071\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.9478 - mse: 32.9478 - mae: 3.6544 - val_loss: 500.5012 - val_mse: 500.5012 - val_mae: 11.0116\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.7715 - mse: 33.7715 - mae: 3.7824 - val_loss: 547.7927 - val_mse: 547.7927 - val_mae: 11.7585\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.4425 - mse: 34.4425 - mae: 3.6489 - val_loss: 544.6710 - val_mse: 544.6710 - val_mae: 11.9090\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.6887 - mse: 32.6887 - mae: 3.6660 - val_loss: 553.8276 - val_mse: 553.8276 - val_mae: 11.1760\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.0928 - mse: 33.0928 - mae: 3.7963 - val_loss: 572.5198 - val_mse: 572.5198 - val_mae: 12.5372\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.2228 - mse: 34.2228 - mae: 3.9480 - val_loss: 514.1591 - val_mse: 514.1591 - val_mae: 10.9514\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.6944 - mse: 34.6944 - mae: 4.1361 - val_loss: 605.5474 - val_mse: 605.5474 - val_mae: 12.8562\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.9448 - mse: 34.9448 - mae: 4.1056 - val_loss: 470.6328 - val_mse: 470.6328 - val_mae: 10.3016\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.8906 - mse: 31.8906 - mae: 3.6884 - val_loss: 645.0541 - val_mse: 645.0541 - val_mae: 13.3683\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.6051 - mse: 35.6051 - mae: 3.8601 - val_loss: 490.2083 - val_mse: 490.2083 - val_mae: 10.6490\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.9952 - mse: 29.9952 - mae: 3.6984 - val_loss: 592.5528 - val_mse: 592.5528 - val_mae: 12.7623\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.2190 - mse: 35.2190 - mae: 3.9164 - val_loss: 466.6714 - val_mse: 466.6714 - val_mae: 10.5321\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 35.7856 - mse: 35.7856 - mae: 3.8667 - val_loss: 572.7093 - val_mse: 572.7093 - val_mae: 12.3752\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.9586 - mse: 30.9586 - mae: 3.6210 - val_loss: 565.7303 - val_mse: 565.7303 - val_mae: 11.5051\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.1936 - mse: 29.1936 - mae: 3.4721 - val_loss: 493.2006 - val_mse: 493.2006 - val_mae: 10.8095\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.0698 - mse: 29.0698 - mae: 3.2596 - val_loss: 618.0091 - val_mse: 618.0091 - val_mae: 12.6775\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.2262 - mse: 32.2262 - mae: 3.6390 - val_loss: 473.7103 - val_mse: 473.7103 - val_mae: 10.7042\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.0952 - mse: 31.0952 - mae: 3.7388 - val_loss: 546.1603 - val_mse: 546.1603 - val_mae: 11.6170\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.1646 - mse: 27.1646 - mae: 3.4864 - val_loss: 539.7341 - val_mse: 539.7341 - val_mae: 11.4217\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.0528 - mse: 26.0528 - mae: 3.2467 - val_loss: 514.3441 - val_mse: 514.3441 - val_mae: 11.3077\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.2324 - mse: 26.2324 - mae: 3.1415 - val_loss: 515.7007 - val_mse: 515.7007 - val_mae: 10.9711\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 26.2433 - mse: 26.2433 - mae: 3.3870 - val_loss: 556.6315 - val_mse: 556.6315 - val_mae: 12.2090\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.1794 - mse: 29.1794 - mae: 3.6820 - val_loss: 485.5907 - val_mse: 485.5907 - val_mae: 10.5831\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.2520 - mse: 28.2520 - mae: 3.5255 - val_loss: 540.1014 - val_mse: 540.1014 - val_mae: 12.0323\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.4452 - mse: 25.4452 - mae: 3.3340 - val_loss: 511.9096 - val_mse: 511.9096 - val_mae: 11.0781\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.5427 - mse: 26.5427 - mae: 3.4560 - val_loss: 553.2280 - val_mse: 553.2280 - val_mae: 12.2773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.4831 - mse: 23.4831 - mae: 3.1575 - val_loss: 494.1924 - val_mse: 494.1924 - val_mae: 10.8028\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.2304 - mse: 25.2304 - mae: 3.4450 - val_loss: 552.7701 - val_mse: 552.7701 - val_mae: 12.2859\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.1833 - mse: 24.1833 - mae: 3.2677 - val_loss: 456.7340 - val_mse: 456.7340 - val_mae: 10.3884\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.2001 - mse: 23.2001 - mae: 3.1928 - val_loss: 595.8636 - val_mse: 595.8636 - val_mae: 12.6701\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 26.1026 - mse: 26.1026 - mae: 3.3340 - val_loss: 474.4590 - val_mse: 474.4590 - val_mae: 10.5004\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.4979 - mse: 23.4979 - mae: 3.1820 - val_loss: 537.4509 - val_mse: 537.4509 - val_mae: 11.9672\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.6729 - mse: 22.6729 - mae: 3.1290 - val_loss: 505.5066 - val_mse: 505.5066 - val_mae: 10.7641\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.7700 - mse: 22.7700 - mae: 3.1616 - val_loss: 516.8978 - val_mse: 516.8978 - val_mae: 11.5990\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.9101 - mse: 24.9101 - mae: 3.2139 - val_loss: 585.2087 - val_mse: 585.2087 - val_mae: 11.9004\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.9443 - mse: 25.9443 - mae: 3.2716 - val_loss: 493.3488 - val_mse: 493.3488 - val_mae: 10.7455\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 22.4809 - mse: 22.4809 - mae: 3.1385 - val_loss: 532.0894 - val_mse: 532.0894 - val_mae: 11.5976\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.0465 - mse: 22.0465 - mae: 3.1049 - val_loss: 493.6923 - val_mse: 493.6923 - val_mae: 10.9309\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.4974 - mse: 21.4974 - mae: 2.9850 - val_loss: 508.9868 - val_mse: 508.9868 - val_mae: 11.1091\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.6866 - mse: 22.6866 - mae: 3.1373 - val_loss: 563.3432 - val_mse: 563.3432 - val_mae: 11.9232\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.7357 - mse: 24.7357 - mae: 3.2654 - val_loss: 459.1556 - val_mse: 459.1556 - val_mae: 10.4643\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.7589 - mse: 23.7589 - mae: 3.2592 - val_loss: 571.5011 - val_mse: 571.5011 - val_mae: 12.2355\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.1233 - mse: 24.1233 - mae: 3.2748 - val_loss: 495.3241 - val_mse: 495.3241 - val_mae: 11.1918\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.4386 - mse: 20.4386 - mae: 2.8677 - val_loss: 506.0099 - val_mse: 506.0099 - val_mae: 11.0397\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22.5528 - mse: 22.5528 - mae: 3.0802 - val_loss: 546.7906 - val_mse: 546.7906 - val_mae: 12.0585\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.3845 - mse: 20.3845 - mae: 2.9653 - val_loss: 482.5817 - val_mse: 482.5817 - val_mae: 10.7293\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.6127 - mse: 19.6127 - mae: 2.8873 - val_loss: 549.1773 - val_mse: 549.1773 - val_mae: 12.0059\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.9928 - mse: 19.9928 - mae: 2.7981 - val_loss: 466.5502 - val_mse: 466.5502 - val_mae: 10.6070\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.5888 - mse: 21.5888 - mae: 3.2452 - val_loss: 589.3663 - val_mse: 589.3663 - val_mae: 12.4848\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 23.4936 - mse: 23.4936 - mae: 3.2826 - val_loss: 469.2795 - val_mse: 469.2795 - val_mae: 10.4261\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.5157 - mse: 23.5157 - mae: 3.4620 - val_loss: 564.6823 - val_mse: 564.6823 - val_mae: 12.4699\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.8474 - mse: 20.8474 - mae: 3.0502 - val_loss: 478.5480 - val_mse: 478.5480 - val_mae: 10.6216\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.5088 - mse: 21.5088 - mae: 3.0878 - val_loss: 527.0092 - val_mse: 527.0092 - val_mae: 11.5181\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.3495 - mse: 20.3495 - mae: 2.9405 - val_loss: 493.9686 - val_mse: 493.9686 - val_mae: 11.1805\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.9465 - mse: 21.9465 - mae: 3.0498 - val_loss: 481.4908 - val_mse: 481.4908 - val_mae: 10.8901\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.0409 - mse: 20.0409 - mae: 3.1540 - val_loss: 569.0173 - val_mse: 569.0173 - val_mae: 12.3863\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.8039 - mse: 21.8039 - mae: 3.1671 - val_loss: 404.5108 - val_mse: 404.5108 - val_mae: 10.1739\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.2721 - mse: 33.2721 - mae: 3.8972 - val_loss: 608.9261 - val_mse: 608.9261 - val_mae: 12.3169\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.9001 - mse: 35.9001 - mae: 4.1014 - val_loss: 527.1379 - val_mse: 527.1379 - val_mae: 12.6180\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.0312 - mse: 30.0312 - mae: 4.1613 - val_loss: 416.6966 - val_mse: 416.6966 - val_mae: 10.4237\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.8053 - mse: 31.8053 - mae: 4.3123 - val_loss: 652.3288 - val_mse: 652.3288 - val_mae: 13.3211\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.9439 - mse: 34.9439 - mae: 3.7778 - val_loss: 385.8127 - val_mse: 385.8127 - val_mae: 10.0477\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.6665 - mse: 30.6665 - mae: 3.5709 - val_loss: 493.9258 - val_mse: 493.9258 - val_mae: 11.3572\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.7679 - mse: 21.7679 - mae: 3.3744 - val_loss: 484.0066 - val_mse: 484.0066 - val_mae: 11.4712\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.9916 - mse: 19.9916 - mae: 2.9802 - val_loss: 512.5439 - val_mse: 512.5439 - val_mae: 11.5190\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19.6908 - mse: 19.6908 - mae: 2.9142 - val_loss: 494.1174 - val_mse: 494.1174 - val_mae: 10.9603\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.5296 - mse: 19.5296 - mae: 2.9401 - val_loss: 478.4412 - val_mse: 478.4412 - val_mae: 11.2555\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.8656 - mse: 17.8656 - mae: 2.7944 - val_loss: 486.8765 - val_mse: 486.8765 - val_mae: 10.7696\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.2054 - mse: 17.2054 - mae: 2.7404 - val_loss: 492.6062 - val_mse: 492.6062 - val_mae: 11.6981\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.3910 - mse: 19.3910 - mae: 2.9837 - val_loss: 432.2192 - val_mse: 432.2192 - val_mae: 10.4694\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.0637 - mse: 20.0637 - mae: 3.1065 - val_loss: 540.5080 - val_mse: 540.5080 - val_mae: 12.0751\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 18.7924 - mse: 18.7924 - mae: 2.8078 - val_loss: 462.5007 - val_mse: 462.5007 - val_mae: 10.7437\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.5418 - mse: 17.5418 - mae: 2.6946 - val_loss: 514.3330 - val_mse: 514.3330 - val_mae: 11.4794\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.5851 - mse: 16.5851 - mae: 2.6148 - val_loss: 498.4648 - val_mse: 498.4648 - val_mae: 11.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.6162 - mse: 15.6162 - mae: 2.5179 - val_loss: 501.9587 - val_mse: 501.9587 - val_mae: 11.3404\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.5265 - mse: 15.5265 - mae: 2.5087 - val_loss: 483.2582 - val_mse: 483.2582 - val_mae: 11.1141\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.6900 - mse: 15.6900 - mae: 2.5911 - val_loss: 524.4401 - val_mse: 524.4401 - val_mae: 11.8363\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.1980 - mse: 16.1980 - mae: 2.6284 - val_loss: 443.9360 - val_mse: 443.9360 - val_mae: 10.8228\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.9592 - mse: 17.9592 - mae: 2.7605 - val_loss: 545.1970 - val_mse: 545.1970 - val_mae: 11.8858\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.0452 - mse: 25.0452 - mae: 3.0965 - val_loss: 540.5430 - val_mse: 540.5430 - val_mae: 12.1757\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.1803 - mse: 17.1803 - mae: 2.7672 - val_loss: 465.0940 - val_mse: 465.0940 - val_mae: 10.9276\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.7916 - mse: 17.7916 - mae: 2.9278 - val_loss: 543.5811 - val_mse: 543.5811 - val_mae: 12.1309\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.6977 - mse: 17.6977 - mae: 2.8198 - val_loss: 522.4430 - val_mse: 522.4430 - val_mae: 11.5929\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 18.8412 - mse: 18.8412 - mae: 2.8062 - val_loss: 424.7630 - val_mse: 424.7630 - val_mae: 10.5058\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.9808 - mse: 20.9808 - mae: 3.0720 - val_loss: 528.3488 - val_mse: 528.3488 - val_mae: 12.2274\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.3006 - mse: 18.3006 - mae: 2.8303 - val_loss: 505.0981 - val_mse: 505.0981 - val_mae: 11.6274\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.8697 - mse: 18.8697 - mae: 2.7996 - val_loss: 474.3417 - val_mse: 474.3417 - val_mae: 10.7198\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.9744 - mse: 18.9744 - mae: 2.9726 - val_loss: 554.1824 - val_mse: 554.1824 - val_mae: 12.4576\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.7429 - mse: 18.7429 - mae: 3.0290 - val_loss: 518.2513 - val_mse: 518.2513 - val_mae: 11.4334\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.1161 - mse: 15.1161 - mae: 2.5111 - val_loss: 456.6947 - val_mse: 456.6947 - val_mae: 11.0463\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.2010 - mse: 17.2010 - mae: 2.7286 - val_loss: 563.7410 - val_mse: 563.7410 - val_mae: 11.9562\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.5792 - mse: 16.5792 - mae: 2.6655 - val_loss: 434.3113 - val_mse: 434.3113 - val_mae: 10.5650\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.4519 - mse: 16.4519 - mae: 2.7010 - val_loss: 577.2941 - val_mse: 577.2941 - val_mae: 12.1935\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.2128 - mse: 18.2128 - mae: 2.9350 - val_loss: 457.3683 - val_mse: 457.3683 - val_mae: 11.1719\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.1281 - mse: 17.1281 - mae: 2.7982 - val_loss: 480.8148 - val_mse: 480.8148 - val_mae: 10.9663\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.9584 - mse: 16.9584 - mae: 2.8820 - val_loss: 536.2258 - val_mse: 536.2258 - val_mae: 11.9955\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.1348 - mse: 14.1348 - mae: 2.5715 - val_loss: 507.9260 - val_mse: 507.9260 - val_mae: 11.5092\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0298 - mse: 15.0298 - mae: 2.4572 - val_loss: 507.1248 - val_mse: 507.1248 - val_mae: 11.4347\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.4292 - mse: 13.4292 - mae: 2.3814 - val_loss: 513.5767 - val_mse: 513.5767 - val_mae: 11.5198\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.2645 - mse: 13.2645 - mae: 2.3174 - val_loss: 487.9581 - val_mse: 487.9581 - val_mae: 11.1608\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3737 - mse: 14.3737 - mae: 2.4514 - val_loss: 485.0908 - val_mse: 485.0908 - val_mae: 11.2807\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4373 - mse: 13.4373 - mae: 2.3431 - val_loss: 522.1757 - val_mse: 522.1757 - val_mae: 11.7998\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.5889 - mse: 13.5889 - mae: 2.4180 - val_loss: 480.3149 - val_mse: 480.3149 - val_mae: 11.0697\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0423 - mse: 14.0423 - mae: 2.5631 - val_loss: 554.3644 - val_mse: 554.3644 - val_mae: 12.1193\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0798 - mse: 14.0798 - mae: 2.4296 - val_loss: 504.2551 - val_mse: 504.2551 - val_mae: 11.2364\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.3623 - mse: 12.3623 - mae: 2.2413 - val_loss: 503.4731 - val_mse: 503.4731 - val_mae: 11.4741\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.1823 - mse: 14.1823 - mae: 2.3629 - val_loss: 530.0688 - val_mse: 530.0688 - val_mae: 11.9356\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.8542 - mse: 15.8542 - mae: 2.6231 - val_loss: 438.9036 - val_mse: 438.9036 - val_mae: 10.8604\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.1004 - mse: 18.1004 - mae: 2.8588 - val_loss: 547.4565 - val_mse: 547.4565 - val_mae: 11.6702\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.9199 - mse: 17.9199 - mae: 3.0907 - val_loss: 486.0870 - val_mse: 486.0870 - val_mae: 11.4661\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.4901 - mse: 16.4901 - mae: 2.9295 - val_loss: 490.8804 - val_mse: 490.8804 - val_mae: 11.0695\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.3549 - mse: 16.3549 - mae: 2.9677 - val_loss: 517.0472 - val_mse: 517.0472 - val_mae: 11.8031\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3362 - mse: 13.3362 - mae: 2.3535 - val_loss: 516.1387 - val_mse: 516.1387 - val_mae: 11.5601\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4819 - mse: 13.4819 - mae: 2.3509 - val_loss: 506.5605 - val_mse: 506.5605 - val_mae: 11.4628\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.5578 - mse: 13.5578 - mae: 2.3214 - val_loss: 464.7825 - val_mse: 464.7825 - val_mae: 10.9780\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4743 - mse: 13.4743 - mae: 2.5748 - val_loss: 589.4511 - val_mse: 589.4511 - val_mae: 12.6317\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.9448 - mse: 15.9448 - mae: 2.8502 - val_loss: 455.5772 - val_mse: 455.5772 - val_mae: 10.7759\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.9841 - mse: 15.9841 - mae: 2.6977 - val_loss: 481.0670 - val_mse: 481.0670 - val_mae: 11.2266\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.6000 - mse: 12.6000 - mae: 2.3451 - val_loss: 510.0307 - val_mse: 510.0307 - val_mae: 11.5687\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.2937 - mse: 13.2937 - mae: 2.2926 - val_loss: 497.4546 - val_mse: 497.4546 - val_mae: 11.6008\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.1984 - mse: 15.1984 - mae: 2.5717 - val_loss: 444.7341 - val_mse: 444.7341 - val_mae: 10.8220\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.6709 - mse: 16.6709 - mae: 2.6696 - val_loss: 577.4503 - val_mse: 577.4503 - val_mae: 11.7068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.0213 - mse: 20.0213 - mae: 3.0703 - val_loss: 529.2317 - val_mse: 529.2317 - val_mae: 12.3137\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.1630 - mse: 15.1630 - mae: 2.6835 - val_loss: 463.7500 - val_mse: 463.7500 - val_mae: 11.1324\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0390 - mse: 15.0390 - mae: 2.6208 - val_loss: 452.3119 - val_mse: 452.3119 - val_mae: 11.4510\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.9257 - mse: 12.9257 - mae: 2.4064 - val_loss: 552.2744 - val_mse: 552.2744 - val_mae: 11.9935\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.4773 - mse: 15.4773 - mae: 2.6822 - val_loss: 474.6824 - val_mse: 474.6824 - val_mae: 11.0762\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.8538 - mse: 13.8538 - mae: 2.5693 - val_loss: 439.6019 - val_mse: 439.6019 - val_mae: 10.8918\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.8305 - mse: 12.8305 - mae: 2.3685 - val_loss: 572.5876 - val_mse: 572.5876 - val_mae: 12.7992\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.4955 - mse: 16.4955 - mae: 2.8020 - val_loss: 502.5186 - val_mse: 502.5186 - val_mae: 11.3140\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.4404 - mse: 12.4404 - mae: 2.2575 - val_loss: 480.2594 - val_mse: 480.2594 - val_mae: 11.0219\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3827 - mse: 13.3827 - mae: 2.3005 - val_loss: 529.4171 - val_mse: 529.4171 - val_mae: 11.8430\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.5889 - mse: 13.5889 - mae: 2.3920 - val_loss: 547.9060 - val_mse: 547.9060 - val_mae: 12.0205\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0738 - mse: 15.0738 - mae: 2.5710 - val_loss: 514.3321 - val_mse: 514.3321 - val_mae: 11.5694\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.9377 - mse: 12.9377 - mae: 2.3490 - val_loss: 490.4277 - val_mse: 490.4277 - val_mae: 11.0181\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7069 - mse: 10.7069 - mae: 2.1253 - val_loss: 523.9464 - val_mse: 523.9464 - val_mae: 11.7546\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.2379 - mse: 11.2379 - mae: 2.1171 - val_loss: 508.1026 - val_mse: 508.1026 - val_mae: 11.4237\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7453 - mse: 10.7453 - mae: 2.0776 - val_loss: 478.0743 - val_mse: 478.0743 - val_mae: 11.2059\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.2613 - mse: 13.2613 - mae: 2.3186 - val_loss: 516.8187 - val_mse: 516.8187 - val_mae: 11.5607\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.2041 - mse: 14.2041 - mae: 2.5802 - val_loss: 585.6234 - val_mse: 585.6234 - val_mae: 12.8591\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.1267 - mse: 16.1267 - mae: 2.8002 - val_loss: 425.8707 - val_mse: 425.8707 - val_mae: 10.5644\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 33.6075 - mse: 33.6075 - mae: 3.7159 - val_loss: 404.2864 - val_mse: 404.2864 - val_mae: 10.4442\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 30.4329 - mse: 30.4329 - mae: 3.8126 - val_loss: 606.7491 - val_mse: 606.7491 - val_mae: 13.2029\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 21.2823 - mse: 21.2823 - mae: 3.3220 - val_loss: 497.9931 - val_mse: 497.9931 - val_mae: 11.9262\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 15.3658 - mse: 15.3658 - mae: 2.7261 - val_loss: 441.1476 - val_mse: 441.1476 - val_mae: 10.6196\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 15.0571 - mse: 15.0571 - mae: 2.8199 - val_loss: 584.2478 - val_mse: 584.2478 - val_mae: 12.4006\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 17.1175 - mse: 17.1175 - mae: 2.7339 - val_loss: 573.5238 - val_mse: 573.5238 - val_mae: 12.2826\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 18.1103 - mse: 18.1103 - mae: 2.6793 - val_loss: 426.8216 - val_mse: 426.8216 - val_mae: 10.5361\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 20.8260 - mse: 20.8260 - mae: 3.0658 - val_loss: 581.5479 - val_mse: 581.5479 - val_mae: 12.4019\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 20.5895 - mse: 20.5895 - mae: 3.2925 - val_loss: 556.8941 - val_mse: 556.8941 - val_mae: 13.0251\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 20.4142 - mse: 20.4142 - mae: 3.3321 - val_loss: 411.8909 - val_mse: 411.8909 - val_mae: 10.5901\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 19.8076 - mse: 19.8076 - mae: 3.3632 - val_loss: 593.5769 - val_mse: 593.5769 - val_mae: 13.0829\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 21.4696 - mse: 21.4696 - mae: 3.2089 - val_loss: 540.7206 - val_mse: 540.7206 - val_mae: 11.4677\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 20.9750 - mse: 20.9750 - mae: 3.1071 - val_loss: 382.7160 - val_mse: 382.7160 - val_mae: 10.4841\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 38.5889 - mse: 38.5889 - mae: 3.9656 - val_loss: 598.9910 - val_mse: 598.9910 - val_mae: 11.7584\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 27.5898 - mse: 27.5898 - mae: 3.8921 - val_loss: 619.0818 - val_mse: 619.0818 - val_mae: 14.0311\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 25.5464 - mse: 25.5464 - mae: 3.7211 - val_loss: 468.2106 - val_mse: 468.2106 - val_mae: 10.6317\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 23.1355 - mse: 23.1355 - mae: 3.2729 - val_loss: 548.3265 - val_mse: 548.3265 - val_mae: 12.0729\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 21.4350 - mse: 21.4350 - mae: 2.8374 - val_loss: 655.6215 - val_mse: 655.6215 - val_mae: 13.0660\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 27.0363 - mse: 27.0363 - mae: 3.3193 - val_loss: 415.2549 - val_mse: 415.2549 - val_mae: 11.2630\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 15.4389 - mse: 15.4389 - mae: 2.7794 - val_loss: 464.0740 - val_mse: 464.0740 - val_mae: 11.2060\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 13.0826 - mse: 13.0826 - mae: 2.5861 - val_loss: 461.8299 - val_mse: 461.8299 - val_mae: 11.8410\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12.7062 - mse: 12.7062 - mae: 2.4600 - val_loss: 444.4603 - val_mse: 444.4603 - val_mae: 10.5969\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12.0906 - mse: 12.0906 - mae: 2.4022 - val_loss: 501.7120 - val_mse: 501.7120 - val_mae: 11.8310\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.8488 - mse: 10.8488 - mae: 2.2174 - val_loss: 535.4753 - val_mse: 535.4753 - val_mae: 11.8732\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2717 - mse: 10.2717 - mae: 2.1122 - val_loss: 465.1197 - val_mse: 465.1197 - val_mae: 11.2321\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8553 - mse: 11.8553 - mae: 2.2313 - val_loss: 455.9347 - val_mse: 455.9347 - val_mae: 11.0009\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2678 - mse: 10.2678 - mae: 2.1934 - val_loss: 520.1111 - val_mse: 520.1111 - val_mae: 11.9205\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6251 - mse: 10.6251 - mae: 2.1217 - val_loss: 493.8154 - val_mse: 493.8154 - val_mae: 11.3517\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5935 - mse: 9.5935 - mae: 1.9374 - val_loss: 478.1268 - val_mse: 478.1268 - val_mae: 11.5418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7931 - mse: 8.7931 - mae: 1.8950 - val_loss: 505.9745 - val_mse: 505.9745 - val_mae: 11.8097\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2939 - mse: 9.2939 - mae: 1.9544 - val_loss: 464.5501 - val_mse: 464.5501 - val_mae: 11.2876\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2439 - mse: 9.2439 - mae: 1.9661 - val_loss: 491.3846 - val_mse: 491.3846 - val_mae: 11.6090\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9884 - mse: 8.9884 - mae: 1.9051 - val_loss: 434.5436 - val_mse: 434.5436 - val_mae: 10.8380\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0409 - mse: 10.0409 - mae: 2.0819 - val_loss: 527.8900 - val_mse: 527.8900 - val_mae: 12.0384\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9430 - mse: 9.9430 - mae: 2.1067 - val_loss: 491.9457 - val_mse: 491.9457 - val_mae: 11.3982\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7337 - mse: 8.7337 - mae: 1.9133 - val_loss: 488.0414 - val_mse: 488.0414 - val_mae: 11.4254\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8458 - mse: 8.8458 - mae: 1.9470 - val_loss: 493.9040 - val_mse: 493.9040 - val_mae: 11.6496\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6216 - mse: 9.6216 - mae: 2.0084 - val_loss: 495.7453 - val_mse: 495.7453 - val_mae: 11.4720\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.4771 - mse: 10.4771 - mae: 2.0648 - val_loss: 459.7407 - val_mse: 459.7407 - val_mae: 11.0431\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6111 - mse: 10.6111 - mae: 2.1384 - val_loss: 522.4212 - val_mse: 522.4212 - val_mae: 11.5221\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6720 - mse: 9.6720 - mae: 2.1285 - val_loss: 498.9811 - val_mse: 498.9811 - val_mae: 11.9520\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2630 - mse: 9.2630 - mae: 2.1046 - val_loss: 500.6945 - val_mse: 500.6945 - val_mae: 11.3297\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0472 - mse: 9.0472 - mae: 2.0922 - val_loss: 499.8940 - val_mse: 499.8940 - val_mae: 12.0040\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.6846 - mse: 8.6846 - mae: 1.9578 - val_loss: 464.6507 - val_mse: 464.6507 - val_mae: 11.1510\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7368 - mse: 9.7368 - mae: 1.9878 - val_loss: 455.7422 - val_mse: 455.7422 - val_mae: 11.0608\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.9928 - mse: 7.9928 - mae: 1.8396 - val_loss: 551.6406 - val_mse: 551.6406 - val_mae: 12.0014\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6407 - mse: 9.6407 - mae: 2.0298 - val_loss: 460.7202 - val_mse: 460.7202 - val_mae: 11.0945\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5905 - mse: 10.5905 - mae: 2.2467 - val_loss: 441.2607 - val_mse: 441.2607 - val_mae: 10.8820\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.7078 - mse: 11.7078 - mae: 2.3820 - val_loss: 512.3463 - val_mse: 512.3463 - val_mae: 11.9653\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7238 - mse: 8.7238 - mae: 1.8308 - val_loss: 514.3989 - val_mse: 514.3989 - val_mae: 11.8193\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4439 - mse: 9.4439 - mae: 1.9848 - val_loss: 426.4680 - val_mse: 426.4680 - val_mae: 10.6015\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1468 - mse: 13.1468 - mae: 2.4556 - val_loss: 533.3248 - val_mse: 533.3248 - val_mae: 12.3283\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7170 - mse: 9.7170 - mae: 2.1069 - val_loss: 539.1558 - val_mse: 539.1558 - val_mae: 11.6764\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.4803 - mse: 10.4803 - mae: 2.0552 - val_loss: 436.0011 - val_mse: 436.0011 - val_mae: 11.1327\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5646 - mse: 10.5646 - mae: 2.0517 - val_loss: 498.8458 - val_mse: 498.8458 - val_mae: 11.2846\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1263 - mse: 13.1263 - mae: 2.4382 - val_loss: 579.8389 - val_mse: 579.8389 - val_mae: 12.5263\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.2362 - mse: 14.2362 - mae: 2.3612 - val_loss: 524.0079 - val_mse: 524.0079 - val_mae: 12.2189\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.9223 - mse: 15.9223 - mae: 2.7638 - val_loss: 464.1743 - val_mse: 464.1743 - val_mae: 11.2594\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 13.6623 - mse: 13.6623 - mae: 2.5948 - val_loss: 500.9440 - val_mse: 500.9440 - val_mae: 11.0710\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.3263 - mse: 12.3263 - mae: 2.3413 - val_loss: 552.7972 - val_mse: 552.7972 - val_mae: 12.1444\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0490 - mse: 11.0490 - mae: 2.2814 - val_loss: 483.5824 - val_mse: 483.5824 - val_mae: 11.5507\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0729 - mse: 9.0729 - mae: 2.0040 - val_loss: 450.6447 - val_mse: 450.6447 - val_mae: 11.0898\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2207 - mse: 10.2207 - mae: 2.1794 - val_loss: 457.7583 - val_mse: 457.7583 - val_mae: 11.3418\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5397 - mse: 9.5397 - mae: 2.0248 - val_loss: 559.6674 - val_mse: 559.6674 - val_mae: 12.1138\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2819 - mse: 10.2819 - mae: 2.1307 - val_loss: 521.7436 - val_mse: 521.7436 - val_mae: 12.0986\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2259 - mse: 9.2259 - mae: 2.0735 - val_loss: 449.9577 - val_mse: 449.9577 - val_mae: 11.0769\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.0639 - mse: 8.0639 - mae: 1.9705 - val_loss: 529.3708 - val_mse: 529.3708 - val_mae: 11.8949\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.9001 - mse: 8.9001 - mae: 1.9701 - val_loss: 523.0750 - val_mse: 523.0750 - val_mae: 11.8468\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.8757 - mse: 9.8757 - mae: 1.9794 - val_loss: 449.2492 - val_mse: 449.2492 - val_mae: 11.1664\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2767 - mse: 10.2767 - mae: 2.0269 - val_loss: 458.5741 - val_mse: 458.5741 - val_mae: 10.9727\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.5436 - mse: 12.5436 - mae: 2.4295 - val_loss: 536.4151 - val_mse: 536.4151 - val_mae: 12.0181\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2895 - mse: 10.2895 - mae: 2.1245 - val_loss: 581.6235 - val_mse: 581.6235 - val_mae: 12.8057\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.0624 - mse: 21.0624 - mae: 3.2306 - val_loss: 409.0637 - val_mse: 409.0637 - val_mae: 10.5988\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.8264 - mse: 16.8264 - mae: 2.9824 - val_loss: 455.7511 - val_mse: 455.7511 - val_mae: 10.9204\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3905 - mse: 14.3905 - mae: 2.6271 - val_loss: 515.3251 - val_mse: 515.3251 - val_mae: 11.9566\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7367 - mse: 10.7367 - mae: 2.1440 - val_loss: 480.7297 - val_mse: 480.7297 - val_mae: 11.7684\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0843 - mse: 7.0843 - mae: 1.7154 - val_loss: 487.3867 - val_mse: 487.3867 - val_mae: 11.7316\n",
      "Epoch 429/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5377 - mse: 7.5377 - mae: 1.7585 - val_loss: 434.3429 - val_mse: 434.3429 - val_mae: 11.0205\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6201 - mse: 10.6201 - mae: 2.2401 - val_loss: 490.5183 - val_mse: 490.5183 - val_mae: 11.2966\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5947 - mse: 8.5947 - mae: 2.0472 - val_loss: 519.3651 - val_mse: 519.3651 - val_mae: 12.2815\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6928 - mse: 7.6928 - mae: 1.8701 - val_loss: 503.2841 - val_mse: 503.2841 - val_mae: 11.5130\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4591 - mse: 7.4591 - mae: 1.8576 - val_loss: 452.9880 - val_mse: 452.9880 - val_mae: 11.2879\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.2876 - mse: 8.2876 - mae: 1.8361 - val_loss: 515.9293 - val_mse: 515.9293 - val_mae: 11.4854\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1214 - mse: 7.1214 - mae: 1.7547 - val_loss: 500.6625 - val_mse: 500.6625 - val_mae: 11.9832\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3049 - mse: 7.3049 - mae: 1.8649 - val_loss: 489.8806 - val_mse: 489.8806 - val_mae: 11.2660\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.0947 - mse: 8.0947 - mae: 1.9358 - val_loss: 510.5468 - val_mse: 510.5468 - val_mae: 12.0024\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1604 - mse: 7.1604 - mae: 1.7558 - val_loss: 499.4692 - val_mse: 499.4692 - val_mae: 11.4543\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0855 - mse: 7.0855 - mae: 1.7949 - val_loss: 458.8016 - val_mse: 458.8016 - val_mae: 11.6430\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8267 - mse: 8.8267 - mae: 2.0443 - val_loss: 473.1523 - val_mse: 473.1523 - val_mae: 11.0676\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.9156 - mse: 6.9156 - mae: 1.8008 - val_loss: 502.5130 - val_mse: 502.5130 - val_mae: 11.9237\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7390 - mse: 6.7390 - mae: 1.6581 - val_loss: 521.5650 - val_mse: 521.5650 - val_mae: 11.6281\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8281 - mse: 5.8281 - mae: 1.5727 - val_loss: 486.5825 - val_mse: 486.5825 - val_mae: 11.7932\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6190 - mse: 6.6190 - mae: 1.6786 - val_loss: 482.3969 - val_mse: 482.3969 - val_mae: 11.2998\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.1256 - mse: 6.1256 - mae: 1.6883 - val_loss: 527.4559 - val_mse: 527.4559 - val_mae: 12.1645\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.1690 - mse: 9.1690 - mae: 1.9753 - val_loss: 510.8856 - val_mse: 510.8856 - val_mae: 11.9938\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.8961 - mse: 10.8961 - mae: 2.3849 - val_loss: 417.6241 - val_mse: 417.6241 - val_mae: 10.6794\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0802 - mse: 12.0802 - mae: 2.6217 - val_loss: 504.4294 - val_mse: 504.4294 - val_mae: 12.3566\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5139 - mse: 10.5139 - mae: 2.2033 - val_loss: 690.1483 - val_mse: 690.1484 - val_mae: 12.8808\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.5958 - mse: 16.5958 - mae: 2.5492 - val_loss: 488.4033 - val_mse: 488.4033 - val_mae: 11.1291\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4015 - mse: 9.4015 - mae: 2.2864 - val_loss: 525.4885 - val_mse: 525.4885 - val_mae: 11.5021\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0858 - mse: 10.0858 - mae: 2.2542 - val_loss: 497.0172 - val_mse: 497.0172 - val_mae: 11.8960\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.9062 - mse: 6.9062 - mae: 1.6731 - val_loss: 489.3799 - val_mse: 489.3799 - val_mae: 11.6568\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3101 - mse: 7.3101 - mae: 1.7708 - val_loss: 544.0554 - val_mse: 544.0554 - val_mae: 12.4605\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7094 - mse: 9.7094 - mae: 2.2046 - val_loss: 445.3800 - val_mse: 445.3800 - val_mae: 11.4038\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.7069 - mse: 7.7069 - mae: 1.9430 - val_loss: 482.6628 - val_mse: 482.6628 - val_mae: 11.3683\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3179 - mse: 7.3179 - mae: 1.8714 - val_loss: 500.6803 - val_mse: 500.6803 - val_mae: 11.9383\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8250 - mse: 5.8250 - mae: 1.5079 - val_loss: 523.9819 - val_mse: 523.9819 - val_mae: 11.6613\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5534 - mse: 6.5534 - mae: 1.7025 - val_loss: 484.5941 - val_mse: 484.5941 - val_mae: 11.8209\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8603 - mse: 6.8603 - mae: 1.8083 - val_loss: 498.2372 - val_mse: 498.2372 - val_mae: 11.4656\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6761 - mse: 6.6761 - mae: 1.7317 - val_loss: 463.3604 - val_mse: 463.3604 - val_mae: 11.3352\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.4617 - mse: 8.4617 - mae: 1.8822 - val_loss: 482.0527 - val_mse: 482.0527 - val_mae: 11.2588\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1029 - mse: 7.1029 - mae: 1.8504 - val_loss: 538.2442 - val_mse: 538.2442 - val_mae: 12.2133\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3263 - mse: 9.3263 - mae: 2.2033 - val_loss: 470.2762 - val_mse: 470.2762 - val_mae: 11.6349\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9935 - mse: 9.9935 - mae: 2.1163 - val_loss: 475.7481 - val_mse: 475.7481 - val_mae: 10.9879\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6235 - mse: 11.6235 - mae: 2.3605 - val_loss: 469.6462 - val_mse: 469.6462 - val_mae: 11.2897\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.3341 - mse: 10.3341 - mae: 2.2049 - val_loss: 556.0337 - val_mse: 556.0337 - val_mae: 12.7026\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8290 - mse: 7.8290 - mae: 2.0549 - val_loss: 475.6810 - val_mse: 475.6810 - val_mae: 11.7755\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9472 - mse: 9.9472 - mae: 2.2111 - val_loss: 435.3616 - val_mse: 435.3616 - val_mae: 10.5505\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.5230 - mse: 16.5230 - mae: 2.8626 - val_loss: 531.9807 - val_mse: 531.9807 - val_mae: 12.6380\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.8361 - mse: 12.8361 - mae: 2.5641 - val_loss: 492.7747 - val_mse: 492.7747 - val_mae: 11.7447\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3065 - mse: 10.3065 - mae: 1.9588 - val_loss: 490.8719 - val_mse: 490.8719 - val_mae: 12.0110\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0715 - mse: 10.0715 - mae: 2.2089 - val_loss: 408.7325 - val_mse: 408.7325 - val_mae: 11.2022\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9411 - mse: 11.9411 - mae: 2.3421 - val_loss: 501.3030 - val_mse: 501.3030 - val_mae: 11.6376\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0826 - mse: 9.0826 - mae: 2.2252 - val_loss: 481.8879 - val_mse: 481.8879 - val_mae: 12.4351\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.2836 - mse: 8.2836 - mae: 2.0203 - val_loss: 482.1609 - val_mse: 482.1609 - val_mae: 11.3487\n",
      "Epoch 477/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 6.5246 - mse: 6.5246 - mae: 1.7480 - val_loss: 451.5135 - val_mse: 451.5135 - val_mae: 11.4195\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7444 - mse: 6.7444 - mae: 1.6953 - val_loss: 561.6833 - val_mse: 561.6833 - val_mae: 12.0521\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.1393 - mse: 12.1393 - mae: 2.2439 - val_loss: 453.6848 - val_mse: 453.6848 - val_mae: 11.9924\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7424 - mse: 10.7424 - mae: 2.4269 - val_loss: 441.7126 - val_mse: 441.7126 - val_mae: 11.0427\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3461 - mse: 8.3461 - mae: 2.0286 - val_loss: 475.6084 - val_mse: 475.6084 - val_mae: 11.7545\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5334 - mse: 7.5334 - mae: 1.8404 - val_loss: 454.9066 - val_mse: 454.9066 - val_mae: 10.9874\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0191 - mse: 11.0191 - mae: 2.6094 - val_loss: 469.1746 - val_mse: 469.1746 - val_mae: 11.5136\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4513 - mse: 9.4513 - mae: 2.3034 - val_loss: 542.3550 - val_mse: 542.3550 - val_mae: 13.1038\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.1545 - mse: 9.1545 - mae: 2.0636 - val_loss: 582.2722 - val_mse: 582.2722 - val_mae: 11.9019\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2376 - mse: 10.2376 - mae: 1.8585 - val_loss: 475.1759 - val_mse: 475.1759 - val_mae: 11.6948\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5062 - mse: 7.5062 - mae: 1.9577 - val_loss: 514.2372 - val_mse: 514.2372 - val_mae: 11.6448\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2650 - mse: 6.2650 - mae: 1.8466 - val_loss: 499.7058 - val_mse: 499.7058 - val_mae: 12.3395\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.1291 - mse: 6.1291 - mae: 1.6503 - val_loss: 529.4729 - val_mse: 529.4729 - val_mae: 11.7250\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5949 - mse: 7.5949 - mae: 1.8578 - val_loss: 527.2418 - val_mse: 527.2418 - val_mae: 12.6255\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0990 - mse: 7.0990 - mae: 1.9566 - val_loss: 520.0576 - val_mse: 520.0576 - val_mae: 11.7999\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7775 - mse: 5.7775 - mae: 1.6043 - val_loss: 513.6685 - val_mse: 513.6685 - val_mae: 12.0558\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.2714 - mse: 5.2714 - mae: 1.5745 - val_loss: 489.7419 - val_mse: 489.7419 - val_mae: 11.5438\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9740 - mse: 4.9740 - mae: 1.5091 - val_loss: 492.3647 - val_mse: 492.3647 - val_mae: 11.4880\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6917 - mse: 5.6917 - mae: 1.7218 - val_loss: 513.4263 - val_mse: 513.4263 - val_mae: 12.1341\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 6.7551 - mse: 6.7551 - mae: 1.6273 - val_loss: 573.1019 - val_mse: 573.1019 - val_mae: 12.3876\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0234 - mse: 10.0234 - mae: 2.0242 - val_loss: 521.5010 - val_mse: 521.5010 - val_mae: 12.4037\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3053 - mse: 6.3053 - mae: 1.7604 - val_loss: 489.6990 - val_mse: 489.6990 - val_mae: 11.3449\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5717 - mse: 6.5717 - mae: 1.7750 - val_loss: 413.2880 - val_mse: 413.2880 - val_mae: 11.0503\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6049 - mse: 10.6049 - mae: 2.0744 - val_loss: 501.4872 - val_mse: 501.4872 - val_mae: 11.2387\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2570 - mse: 10.2570 - mae: 2.3925 - val_loss: 525.0452 - val_mse: 525.0452 - val_mae: 13.2831\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.7204 - mse: 10.7204 - mae: 2.5825 - val_loss: 525.0660 - val_mse: 525.0660 - val_mae: 11.8511\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0680 - mse: 9.0680 - mae: 2.1074 - val_loss: 438.0828 - val_mse: 438.0828 - val_mae: 11.2002\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5924 - mse: 7.5924 - mae: 1.7896 - val_loss: 477.2266 - val_mse: 477.2266 - val_mae: 10.8932\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9972 - mse: 5.9972 - mae: 1.7169 - val_loss: 467.5290 - val_mse: 467.5290 - val_mae: 11.3534\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4757 - mse: 5.4757 - mae: 1.6600 - val_loss: 484.3094 - val_mse: 484.3094 - val_mae: 11.7221\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3554 - mse: 5.3554 - mae: 1.5183 - val_loss: 504.4513 - val_mse: 504.4513 - val_mae: 11.6182\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5605 - mse: 5.5605 - mae: 1.5411 - val_loss: 508.6518 - val_mse: 508.6518 - val_mae: 12.0210\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6755 - mse: 4.6755 - mae: 1.4378 - val_loss: 468.6978 - val_mse: 468.6978 - val_mae: 11.2065\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8197 - mse: 5.8197 - mae: 1.6698 - val_loss: 438.2172 - val_mse: 438.2172 - val_mae: 10.9416\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6576 - mse: 9.6576 - mae: 2.0502 - val_loss: 477.2486 - val_mse: 477.2486 - val_mae: 11.2037\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.7956 - mse: 11.7956 - mae: 2.3076 - val_loss: 522.4681 - val_mse: 522.4681 - val_mae: 12.3478\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.1260 - mse: 10.1260 - mae: 1.9853 - val_loss: 550.6343 - val_mse: 550.6343 - val_mae: 12.5020\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3509 - mse: 8.3509 - mae: 1.9219 - val_loss: 516.0694 - val_mse: 516.0694 - val_mae: 12.5340\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.1179 - mse: 10.1179 - mae: 2.1854 - val_loss: 484.8629 - val_mse: 484.8629 - val_mae: 11.8115\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.2968 - mse: 7.2968 - mae: 1.9888 - val_loss: 502.1135 - val_mse: 502.1135 - val_mae: 12.0575\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2067 - mse: 6.2067 - mae: 1.7575 - val_loss: 456.6030 - val_mse: 456.6030 - val_mae: 11.0324\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2657 - mse: 6.2657 - mae: 1.7884 - val_loss: 493.5540 - val_mse: 493.5540 - val_mae: 11.8198\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4992 - mse: 5.4992 - mae: 1.6818 - val_loss: 553.1353 - val_mse: 553.1353 - val_mae: 12.0890\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7814 - mse: 5.7814 - mae: 1.7943 - val_loss: 457.4062 - val_mse: 457.4062 - val_mae: 11.6753\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3103 - mse: 6.3103 - mae: 1.7527 - val_loss: 497.2680 - val_mse: 497.2680 - val_mae: 11.5736\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0493 - mse: 6.0493 - mae: 1.8566 - val_loss: 537.5753 - val_mse: 537.5753 - val_mae: 12.7945\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2053 - mse: 6.2053 - mae: 1.8121 - val_loss: 491.4121 - val_mse: 491.4121 - val_mae: 11.3640\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4109 - mse: 4.4109 - mae: 1.4752 - val_loss: 469.4060 - val_mse: 469.4060 - val_mae: 11.7706\n",
      "Epoch 525/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7396 - mse: 3.7396 - mae: 1.2920 - val_loss: 480.6827 - val_mse: 480.6827 - val_mae: 11.6214\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9840 - mse: 3.9840 - mae: 1.3760 - val_loss: 478.8189 - val_mse: 478.8189 - val_mae: 11.9166\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3642 - mse: 3.3642 - mae: 1.2617 - val_loss: 469.3500 - val_mse: 469.3500 - val_mae: 11.5853\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2166 - mse: 3.2166 - mae: 1.2258 - val_loss: 467.1587 - val_mse: 467.1587 - val_mae: 11.6619\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4303 - mse: 3.4303 - mae: 1.2317 - val_loss: 477.3832 - val_mse: 477.3832 - val_mae: 11.4183\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5220 - mse: 3.5220 - mae: 1.2437 - val_loss: 495.5045 - val_mse: 495.5045 - val_mae: 11.8491\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5010 - mse: 3.5010 - mae: 1.2435 - val_loss: 463.9052 - val_mse: 463.9052 - val_mae: 11.6294\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7027 - mse: 4.7027 - mae: 1.5292 - val_loss: 453.5559 - val_mse: 453.5559 - val_mae: 11.1539\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.1835 - mse: 5.1835 - mae: 1.5537 - val_loss: 452.2863 - val_mse: 452.2863 - val_mae: 11.5319\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7262 - mse: 4.7262 - mae: 1.4547 - val_loss: 442.7359 - val_mse: 442.7359 - val_mae: 11.2736\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8337 - mse: 4.8337 - mae: 1.5239 - val_loss: 493.2359 - val_mse: 493.2359 - val_mae: 11.6891\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2797 - mse: 6.2797 - mae: 1.7781 - val_loss: 540.3235 - val_mse: 540.3235 - val_mae: 12.9589\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.7725 - mse: 7.7725 - mae: 2.0931 - val_loss: 475.2100 - val_mse: 475.2100 - val_mae: 12.1072\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8721 - mse: 5.8721 - mae: 1.7405 - val_loss: 491.3174 - val_mse: 491.3174 - val_mae: 11.3850\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0279 - mse: 5.0279 - mae: 1.5276 - val_loss: 465.1342 - val_mse: 465.1342 - val_mae: 11.7946\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2388 - mse: 4.2388 - mae: 1.4608 - val_loss: 475.0095 - val_mse: 475.0095 - val_mae: 11.4079\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6819 - mse: 4.6819 - mae: 1.4428 - val_loss: 459.7625 - val_mse: 459.7625 - val_mae: 11.8441\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3380 - mse: 6.3380 - mae: 1.5984 - val_loss: 487.8917 - val_mse: 487.8917 - val_mae: 11.3313\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8794 - mse: 4.8794 - mae: 1.6821 - val_loss: 539.2159 - val_mse: 539.2159 - val_mae: 12.2973\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.1864 - mse: 9.1864 - mae: 2.1506 - val_loss: 552.2262 - val_mse: 552.2262 - val_mae: 13.1272\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1433 - mse: 11.1433 - mae: 2.3735 - val_loss: 468.6042 - val_mse: 468.6042 - val_mae: 12.3491\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.8854 - mse: 12.8854 - mae: 2.7103 - val_loss: 429.8488 - val_mse: 429.8488 - val_mae: 10.7235\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.9863 - mse: 15.9863 - mae: 2.6850 - val_loss: 387.2727 - val_mse: 387.2727 - val_mae: 10.5423\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.9199 - mse: 25.9199 - mae: 2.7490 - val_loss: 602.6121 - val_mse: 602.6121 - val_mae: 12.0686\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.4005 - mse: 29.4005 - mae: 3.9474 - val_loss: 552.5103 - val_mse: 552.5103 - val_mae: 13.8107\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.1718 - mse: 24.1718 - mae: 3.5561 - val_loss: 567.8307 - val_mse: 567.8307 - val_mae: 12.9964\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.7480 - mse: 23.7480 - mae: 2.9235 - val_loss: 574.0808 - val_mse: 574.0808 - val_mae: 13.4155\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.4036 - mse: 19.4036 - mae: 3.1610 - val_loss: 462.2372 - val_mse: 462.2372 - val_mae: 11.4053\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.7499 - mse: 16.7499 - mae: 3.1055 - val_loss: 361.4443 - val_mse: 361.4443 - val_mae: 10.1850\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.3426 - mse: 29.3426 - mae: 3.5451 - val_loss: 455.7198 - val_mse: 455.7198 - val_mae: 11.0040\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.6299 - mse: 48.6299 - mae: 3.9846 - val_loss: 864.9077 - val_mse: 864.9077 - val_mae: 14.9817\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 73.5793 - mse: 73.5793 - mae: 5.8143 - val_loss: 558.4818 - val_mse: 558.4818 - val_mae: 14.8307\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 60.8599 - mse: 60.8599 - mae: 5.7660 - val_loss: 534.2409 - val_mse: 534.2409 - val_mae: 12.8429\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.3693 - mse: 43.3693 - mae: 4.7994 - val_loss: 322.4732 - val_mse: 322.4732 - val_mae: 11.2345\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 77.2365 - mse: 77.2365 - mae: 5.5316 - val_loss: 756.2064 - val_mse: 756.2064 - val_mae: 13.3670\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114.1284 - mse: 114.1284 - mae: 7.5839 - val_loss: 707.6791 - val_mse: 707.6791 - val_mae: 19.1750\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 107.3067 - mse: 107.3067 - mae: 7.9132 - val_loss: 454.8444 - val_mse: 454.8444 - val_mae: 12.1466\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 95.6407 - mse: 95.6407 - mae: 6.7169 - val_loss: 432.5467 - val_mse: 432.5467 - val_mae: 11.7057\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52.1056 - mse: 52.1056 - mae: 5.2371 - val_loss: 756.5629 - val_mse: 756.5629 - val_mae: 14.2320\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60.8311 - mse: 60.8311 - mae: 5.6299 - val_loss: 344.6450 - val_mse: 344.6450 - val_mae: 10.0982\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.2627 - mse: 65.2627 - mae: 6.0870 - val_loss: 407.4167 - val_mse: 407.4167 - val_mae: 10.3658\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.2858 - mse: 48.2858 - mae: 5.1366 - val_loss: 658.5514 - val_mse: 658.5514 - val_mae: 14.5688\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.7443 - mse: 32.7443 - mae: 4.0132 - val_loss: 579.4966 - val_mse: 579.4966 - val_mae: 12.1762\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.7460 - mse: 18.7460 - mae: 3.0505 - val_loss: 434.0506 - val_mse: 434.0506 - val_mae: 10.5434\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7174 - mse: 13.7174 - mae: 2.7916 - val_loss: 508.6267 - val_mse: 508.6267 - val_mae: 12.5179\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9760 - mse: 11.9760 - mae: 2.4950 - val_loss: 606.9903 - val_mse: 606.9903 - val_mae: 12.3046\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.9687 - mse: 12.9687 - mae: 2.3317 - val_loss: 466.3987 - val_mse: 466.3987 - val_mae: 10.9867\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0101 - mse: 14.0101 - mae: 2.5384 - val_loss: 552.1635 - val_mse: 552.1635 - val_mae: 11.2001\n",
      "Epoch 573/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8481 - mse: 11.8481 - mae: 2.2778 - val_loss: 610.9889 - val_mse: 610.9889 - val_mae: 12.6505\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.4486 - mse: 8.4486 - mae: 1.9122 - val_loss: 539.5282 - val_mse: 539.5282 - val_mae: 11.8294\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6395 - mse: 5.6395 - mae: 1.7138 - val_loss: 492.7859 - val_mse: 492.7859 - val_mae: 11.5963\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.2140 - mse: 5.2140 - mae: 1.6274 - val_loss: 508.9197 - val_mse: 508.9197 - val_mae: 11.8087\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9143 - mse: 4.9143 - mae: 1.6031 - val_loss: 536.4436 - val_mse: 536.4436 - val_mae: 12.1103\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6078 - mse: 4.6078 - mae: 1.5220 - val_loss: 473.4138 - val_mse: 473.4138 - val_mae: 11.5572\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5648 - mse: 4.5648 - mae: 1.4884 - val_loss: 546.2155 - val_mse: 546.2155 - val_mae: 12.1635\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3498 - mse: 5.3498 - mae: 1.6001 - val_loss: 497.8364 - val_mse: 497.8364 - val_mae: 11.9735\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6091 - mse: 5.6091 - mae: 1.6502 - val_loss: 479.0958 - val_mse: 479.0958 - val_mae: 11.0599\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6279 - mse: 5.6279 - mae: 1.7004 - val_loss: 561.4301 - val_mse: 561.4301 - val_mae: 12.2808\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4068 - mse: 4.4068 - mae: 1.4611 - val_loss: 527.3735 - val_mse: 527.3735 - val_mae: 11.8102\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9108 - mse: 4.9108 - mae: 1.5835 - val_loss: 454.7628 - val_mse: 454.7628 - val_mae: 11.0378\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6900 - mse: 5.6900 - mae: 1.6505 - val_loss: 500.6932 - val_mse: 500.6932 - val_mae: 11.7316\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2404 - mse: 4.2404 - mae: 1.4307 - val_loss: 564.4109 - val_mse: 564.4109 - val_mae: 12.3595\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3857 - mse: 6.3857 - mae: 1.7412 - val_loss: 531.1538 - val_mse: 531.1538 - val_mae: 12.0572\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.1527 - mse: 6.1527 - mae: 1.7390 - val_loss: 419.1960 - val_mse: 419.1960 - val_mae: 10.5149\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.5125 - mse: 11.5125 - mae: 2.1500 - val_loss: 568.8723 - val_mse: 568.8723 - val_mae: 12.2206\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.7308 - mse: 5.7308 - mae: 1.6335 - val_loss: 531.8486 - val_mse: 531.8486 - val_mae: 11.9844\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9228 - mse: 4.9228 - mae: 1.5745 - val_loss: 467.1340 - val_mse: 467.1340 - val_mae: 11.1598\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5123 - mse: 4.5123 - mae: 1.5014 - val_loss: 502.5921 - val_mse: 502.5921 - val_mae: 11.6952\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9434 - mse: 3.9434 - mae: 1.3642 - val_loss: 517.1006 - val_mse: 517.1006 - val_mae: 12.0642\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7926 - mse: 4.7926 - mae: 1.4603 - val_loss: 515.3178 - val_mse: 515.3178 - val_mae: 11.9389\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2715 - mse: 4.2715 - mae: 1.5018 - val_loss: 473.3382 - val_mse: 473.3382 - val_mae: 11.1323\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5389 - mse: 5.5389 - mae: 1.6465 - val_loss: 515.1314 - val_mse: 515.1314 - val_mae: 12.2185\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1428 - mse: 4.1428 - mae: 1.4374 - val_loss: 529.3470 - val_mse: 529.3470 - val_mae: 11.7714\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2580 - mse: 4.2580 - mae: 1.5628 - val_loss: 531.2504 - val_mse: 531.2504 - val_mae: 12.3380\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2694 - mse: 4.2694 - mae: 1.5202 - val_loss: 497.5010 - val_mse: 497.5010 - val_mae: 11.5210\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3110 - mse: 4.3110 - mae: 1.5199 - val_loss: 486.7485 - val_mse: 486.7485 - val_mae: 11.5356\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6922 - mse: 3.6922 - mae: 1.4061 - val_loss: 517.3082 - val_mse: 517.3082 - val_mae: 11.8687\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6861 - mse: 3.6861 - mae: 1.3619 - val_loss: 526.1136 - val_mse: 526.1136 - val_mae: 12.0236\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6344 - mse: 3.6344 - mae: 1.2843 - val_loss: 491.3083 - val_mse: 491.3083 - val_mae: 11.8619\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0861 - mse: 3.0861 - mae: 1.2140 - val_loss: 480.1474 - val_mse: 480.1474 - val_mae: 11.5574\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1254 - mse: 3.1254 - mae: 1.2524 - val_loss: 518.8873 - val_mse: 518.8873 - val_mae: 12.1434\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2452 - mse: 3.2452 - mae: 1.2923 - val_loss: 510.9628 - val_mse: 510.9628 - val_mae: 11.8627\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9552 - mse: 2.9552 - mae: 1.1594 - val_loss: 510.9713 - val_mse: 510.9713 - val_mae: 11.9489\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6694 - mse: 2.6694 - mae: 1.0863 - val_loss: 469.4489 - val_mse: 469.4489 - val_mae: 11.3764\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2205 - mse: 3.2205 - mae: 1.2292 - val_loss: 487.5542 - val_mse: 487.5542 - val_mae: 11.7636\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6879 - mse: 3.6879 - mae: 1.3084 - val_loss: 530.6232 - val_mse: 530.6232 - val_mae: 11.9582\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2139 - mse: 3.2139 - mae: 1.2119 - val_loss: 513.8155 - val_mse: 513.8155 - val_mae: 11.8416\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5511 - mse: 2.5511 - mae: 1.1204 - val_loss: 510.9720 - val_mse: 510.9720 - val_mae: 11.8956\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5825 - mse: 2.5825 - mae: 1.1041 - val_loss: 490.4924 - val_mse: 490.4924 - val_mae: 11.7491\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5218 - mse: 2.5218 - mae: 1.0675 - val_loss: 486.8087 - val_mse: 486.8087 - val_mae: 11.6947\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4003 - mse: 2.4003 - mae: 1.0803 - val_loss: 517.5466 - val_mse: 517.5466 - val_mae: 11.9259\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6507 - mse: 2.6507 - mae: 1.1240 - val_loss: 508.2082 - val_mse: 508.2082 - val_mae: 12.0433\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4634 - mse: 2.4634 - mae: 1.1191 - val_loss: 471.2176 - val_mse: 471.2176 - val_mae: 11.3559\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6112 - mse: 2.6112 - mae: 1.1045 - val_loss: 509.5242 - val_mse: 509.5242 - val_mae: 12.0003\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5704 - mse: 2.5704 - mae: 1.1106 - val_loss: 479.5661 - val_mse: 479.5661 - val_mae: 11.5334\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3882 - mse: 2.3882 - mae: 1.0749 - val_loss: 474.9923 - val_mse: 474.9923 - val_mae: 11.6246\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 2.5389 - mse: 2.5389 - mae: 1.1002 - val_loss: 511.4794 - val_mse: 511.4794 - val_mae: 11.9263\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4378 - mse: 2.4378 - mae: 1.0539 - val_loss: 500.5969 - val_mse: 500.5969 - val_mae: 11.8865\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4374 - mse: 2.4374 - mae: 1.0825 - val_loss: 498.9503 - val_mse: 498.9503 - val_mae: 11.7143\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2812 - mse: 2.2812 - mae: 1.0494 - val_loss: 475.2093 - val_mse: 475.2093 - val_mae: 11.5812\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4481 - mse: 2.4481 - mae: 1.0920 - val_loss: 507.4216 - val_mse: 507.4216 - val_mae: 11.8258\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4447 - mse: 2.4447 - mae: 1.0553 - val_loss: 524.2355 - val_mse: 524.2355 - val_mae: 12.2139\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3955 - mse: 2.3955 - mae: 1.0803 - val_loss: 478.3277 - val_mse: 478.3277 - val_mae: 11.3893\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0796 - mse: 3.0796 - mae: 1.2197 - val_loss: 493.9944 - val_mse: 493.9944 - val_mae: 11.7531\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9456 - mse: 2.9456 - mae: 1.2199 - val_loss: 491.9352 - val_mse: 491.9352 - val_mae: 11.5718\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5906 - mse: 2.5906 - mae: 1.1193 - val_loss: 532.5336 - val_mse: 532.5336 - val_mae: 12.1210\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8802 - mse: 2.8802 - mae: 1.1760 - val_loss: 492.2413 - val_mse: 492.2413 - val_mae: 11.7796\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8313 - mse: 2.8313 - mae: 1.1726 - val_loss: 466.8036 - val_mse: 466.8036 - val_mae: 11.2196\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7536 - mse: 3.7536 - mae: 1.3329 - val_loss: 506.4348 - val_mse: 506.4348 - val_mae: 11.8690\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0870 - mse: 3.0870 - mae: 1.2136 - val_loss: 524.2512 - val_mse: 524.2512 - val_mae: 12.2046\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7206 - mse: 2.7206 - mae: 1.1496 - val_loss: 498.6810 - val_mse: 498.6810 - val_mae: 11.8045\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6294 - mse: 2.6294 - mae: 1.0732 - val_loss: 485.1657 - val_mse: 485.1657 - val_mae: 11.6351\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8452 - mse: 2.8452 - mae: 1.1104 - val_loss: 455.8091 - val_mse: 455.8091 - val_mae: 11.0776\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0808 - mse: 4.0808 - mae: 1.4244 - val_loss: 502.0004 - val_mse: 502.0004 - val_mae: 11.7719\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9705 - mse: 2.9705 - mae: 1.2621 - val_loss: 520.0610 - val_mse: 520.0610 - val_mae: 12.0231\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4221 - mse: 2.4221 - mae: 1.0339 - val_loss: 507.0700 - val_mse: 507.0700 - val_mae: 11.9874\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0103 - mse: 2.0103 - mae: 0.9445 - val_loss: 504.9562 - val_mse: 504.9562 - val_mae: 11.8427\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9326 - mse: 1.9326 - mae: 0.9969 - val_loss: 471.9334 - val_mse: 471.9334 - val_mae: 11.4947\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3210 - mse: 2.3210 - mae: 1.0498 - val_loss: 476.2280 - val_mse: 476.2280 - val_mae: 11.4925\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7094 - mse: 2.7094 - mae: 1.1147 - val_loss: 502.4791 - val_mse: 502.4791 - val_mae: 11.9755\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4724 - mse: 2.4724 - mae: 1.1018 - val_loss: 496.2262 - val_mse: 496.2262 - val_mae: 11.8193\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0101 - mse: 2.0101 - mae: 1.0040 - val_loss: 474.8975 - val_mse: 474.8975 - val_mae: 11.6030\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9483 - mse: 1.9483 - mae: 0.9315 - val_loss: 493.2595 - val_mse: 493.2595 - val_mae: 11.7380\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6313 - mse: 1.6313 - mae: 0.8558 - val_loss: 485.3878 - val_mse: 485.3878 - val_mae: 11.5923\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2178 - mse: 2.2178 - mae: 1.0493 - val_loss: 476.4335 - val_mse: 476.4335 - val_mae: 11.6413\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0710 - mse: 2.0710 - mae: 0.9607 - val_loss: 485.8036 - val_mse: 485.8036 - val_mae: 11.6259\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7437 - mse: 1.7437 - mae: 0.9263 - val_loss: 500.0812 - val_mse: 500.0812 - val_mae: 12.0329\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8687 - mse: 1.8687 - mae: 0.9677 - val_loss: 475.7997 - val_mse: 475.7997 - val_mae: 11.5683\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2.4406 - mse: 2.4406 - mae: 1.0243 - val_loss: 454.6922 - val_mse: 454.6922 - val_mae: 11.4260\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3415 - mse: 2.3415 - mae: 1.1139 - val_loss: 504.8228 - val_mse: 504.8228 - val_mae: 11.8215\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.2340 - mse: 2.2340 - mae: 1.0646 - val_loss: 508.0796 - val_mse: 508.0796 - val_mae: 12.0331\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2251 - mse: 2.2251 - mae: 1.0203 - val_loss: 466.1237 - val_mse: 466.1237 - val_mae: 11.3866\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8640 - mse: 2.8640 - mae: 1.1275 - val_loss: 451.6852 - val_mse: 451.6852 - val_mae: 11.1505\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5804 - mse: 5.5804 - mae: 1.4776 - val_loss: 447.5399 - val_mse: 447.5399 - val_mae: 11.3077\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.1660 - mse: 5.1660 - mae: 1.5440 - val_loss: 492.9951 - val_mse: 492.9951 - val_mae: 11.4298\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5847 - mse: 4.5847 - mae: 1.5920 - val_loss: 476.1338 - val_mse: 476.1338 - val_mae: 12.3839\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4509 - mse: 3.4509 - mae: 1.2845 - val_loss: 524.9832 - val_mse: 524.9832 - val_mae: 11.7093\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5376 - mse: 3.5376 - mae: 1.4249 - val_loss: 476.0058 - val_mse: 476.0058 - val_mae: 11.7475\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7134 - mse: 2.7134 - mae: 1.2280 - val_loss: 498.8287 - val_mse: 498.8287 - val_mae: 11.5978\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3280 - mse: 2.3280 - mae: 1.1494 - val_loss: 488.0357 - val_mse: 488.0357 - val_mae: 11.7634\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5537 - mse: 2.5537 - mae: 1.1237 - val_loss: 498.8569 - val_mse: 498.8569 - val_mae: 11.4546\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5834 - mse: 2.5834 - mae: 1.1526 - val_loss: 513.8965 - val_mse: 513.8965 - val_mae: 12.2154\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1624 - mse: 2.1624 - mae: 1.0558 - val_loss: 501.7580 - val_mse: 501.7580 - val_mae: 11.8194\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1765 - mse: 2.1765 - mae: 1.0401 - val_loss: 503.3587 - val_mse: 503.3587 - val_mae: 12.0461\n",
      "Epoch 669/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4822 - mse: 2.4822 - mae: 1.0700 - val_loss: 491.6838 - val_mse: 491.6838 - val_mae: 11.5977\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8960 - mse: 2.8960 - mae: 1.1114 - val_loss: 462.7086 - val_mse: 462.7086 - val_mae: 11.3941\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3789 - mse: 5.3789 - mae: 1.4253 - val_loss: 411.8782 - val_mse: 411.8782 - val_mae: 10.3967\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5854 - mse: 7.5854 - mae: 1.7987 - val_loss: 515.7597 - val_mse: 515.7597 - val_mae: 11.7238\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.3965 - mse: 6.3965 - mae: 1.7464 - val_loss: 516.8831 - val_mse: 516.8831 - val_mae: 12.5675\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.9190 - mse: 3.9190 - mae: 1.3737 - val_loss: 490.4170 - val_mse: 490.4170 - val_mae: 11.6662\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8156 - mse: 2.8156 - mae: 1.2733 - val_loss: 481.4992 - val_mse: 481.4992 - val_mae: 11.9133\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4540 - mse: 2.4540 - mae: 1.1544 - val_loss: 491.4710 - val_mse: 491.4710 - val_mae: 11.6377\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9665 - mse: 1.9665 - mae: 0.9243 - val_loss: 493.4137 - val_mse: 493.4137 - val_mae: 12.0021\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6768 - mse: 1.6768 - mae: 0.9334 - val_loss: 469.9701 - val_mse: 469.9701 - val_mae: 11.4135\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6985 - mse: 1.6985 - mae: 0.9620 - val_loss: 521.4180 - val_mse: 521.4180 - val_mae: 12.0409\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0351 - mse: 2.0351 - mae: 0.9937 - val_loss: 510.7756 - val_mse: 510.7756 - val_mae: 12.0070\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8770 - mse: 1.8770 - mae: 0.9329 - val_loss: 485.1683 - val_mse: 485.1683 - val_mae: 11.6532\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7777 - mse: 1.7777 - mae: 0.8740 - val_loss: 486.9789 - val_mse: 486.9789 - val_mae: 11.7491\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8089 - mse: 1.8089 - mae: 0.9225 - val_loss: 463.8550 - val_mse: 463.8550 - val_mae: 11.4197\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9437 - mse: 2.9437 - mae: 1.1514 - val_loss: 501.8398 - val_mse: 501.8398 - val_mae: 11.5562\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9272 - mse: 3.9272 - mae: 1.4866 - val_loss: 526.4335 - val_mse: 526.4335 - val_mae: 12.7462\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4835 - mse: 3.4835 - mae: 1.4061 - val_loss: 493.7268 - val_mse: 493.7268 - val_mae: 11.6702\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7731 - mse: 2.7731 - mae: 1.0984 - val_loss: 506.6946 - val_mse: 506.6946 - val_mae: 11.7967\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.3132 - mse: 5.3132 - mae: 1.3587 - val_loss: 423.0385 - val_mse: 423.0385 - val_mae: 10.5829\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5077 - mse: 7.5077 - mae: 1.8566 - val_loss: 453.7470 - val_mse: 453.7470 - val_mae: 11.2582\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9446 - mse: 9.9446 - mae: 2.0295 - val_loss: 494.1014 - val_mse: 494.1014 - val_mae: 11.3720\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.6332 - mse: 23.6332 - mae: 3.0623 - val_loss: 641.6061 - val_mse: 641.6061 - val_mae: 11.8114\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1746 - mse: 13.1746 - mae: 2.5161 - val_loss: 531.5284 - val_mse: 531.5284 - val_mae: 12.5598\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.9942 - mse: 22.9942 - mae: 3.2630 - val_loss: 704.3985 - val_mse: 704.3986 - val_mae: 12.9290\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.9374 - mse: 44.9374 - mae: 4.5910 - val_loss: 545.6500 - val_mse: 545.6500 - val_mae: 15.8505\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 69.4926 - mse: 69.4926 - mae: 5.6997 - val_loss: 694.4805 - val_mse: 694.4805 - val_mae: 12.3537\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 167.1265 - mse: 167.1265 - mae: 6.5415 - val_loss: 527.5999 - val_mse: 527.5999 - val_mae: 12.7585\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 180.4259 - mse: 180.4259 - mae: 9.9100 - val_loss: 986.3610 - val_mse: 986.3610 - val_mae: 13.9744\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114.2710 - mse: 114.2710 - mae: 8.4380 - val_loss: 940.8067 - val_mse: 940.8067 - val_mae: 16.3301\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 83.5899 - mse: 83.5899 - mae: 6.8237 - val_loss: 375.5558 - val_mse: 375.5558 - val_mae: 10.8397\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 89.5155 - mse: 89.5155 - mae: 5.3083 - val_loss: 406.0032 - val_mse: 406.0031 - val_mae: 10.5484\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57.4379 - mse: 57.4379 - mae: 5.1068 - val_loss: 760.9682 - val_mse: 760.9681 - val_mae: 16.5161\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.3808 - mse: 44.3808 - mae: 4.9980 - val_loss: 432.4224 - val_mse: 432.4224 - val_mae: 10.4115\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.9731 - mse: 31.9731 - mae: 4.0027 - val_loss: 533.9735 - val_mse: 533.9735 - val_mae: 11.2264\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7454 - mse: 13.7454 - mae: 2.6079 - val_loss: 622.9896 - val_mse: 622.9896 - val_mae: 12.2504\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.4831 - mse: 15.4831 - mae: 2.4509 - val_loss: 454.8798 - val_mse: 454.8798 - val_mae: 11.2746\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.6354 - mse: 7.6354 - mae: 1.9804 - val_loss: 457.4499 - val_mse: 457.4499 - val_mae: 10.8982\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7992 - mse: 5.7992 - mae: 1.7891 - val_loss: 501.4255 - val_mse: 501.4255 - val_mae: 11.8084\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8256 - mse: 4.8256 - mae: 1.5793 - val_loss: 568.9568 - val_mse: 568.9568 - val_mae: 12.1749\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8593 - mse: 4.8593 - mae: 1.5439 - val_loss: 490.0928 - val_mse: 490.0928 - val_mae: 11.4038\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8019 - mse: 4.8019 - mae: 1.6171 - val_loss: 485.9761 - val_mse: 485.9761 - val_mae: 11.1828\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9253 - mse: 3.9253 - mae: 1.3920 - val_loss: 521.0483 - val_mse: 521.0483 - val_mae: 12.0946\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5843 - mse: 4.5843 - mae: 1.4704 - val_loss: 533.8234 - val_mse: 533.8234 - val_mae: 11.9498\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6642 - mse: 3.6642 - mae: 1.3643 - val_loss: 463.0247 - val_mse: 463.0247 - val_mae: 11.1016\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4951 - mse: 3.4951 - mae: 1.3318 - val_loss: 541.9977 - val_mse: 541.9977 - val_mae: 11.8819\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5888 - mse: 3.5888 - mae: 1.3335 - val_loss: 486.3638 - val_mse: 486.3638 - val_mae: 11.2903\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1885 - mse: 3.1885 - mae: 1.2308 - val_loss: 500.0625 - val_mse: 500.0625 - val_mae: 11.7207\n",
      "Epoch 717/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2006 - mse: 3.2006 - mae: 1.2192 - val_loss: 542.9384 - val_mse: 542.9384 - val_mae: 11.7951\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0418 - mse: 3.0418 - mae: 1.2612 - val_loss: 537.2020 - val_mse: 537.2020 - val_mae: 12.1085\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2559 - mse: 3.2559 - mae: 1.3194 - val_loss: 478.3813 - val_mse: 478.3813 - val_mae: 11.3427\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4269 - mse: 2.4269 - mae: 1.0843 - val_loss: 478.8082 - val_mse: 478.8082 - val_mae: 11.5782\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3568 - mse: 2.3568 - mae: 1.0900 - val_loss: 506.9313 - val_mse: 506.9313 - val_mae: 11.6220\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4643 - mse: 2.4643 - mae: 1.1370 - val_loss: 505.9161 - val_mse: 505.9161 - val_mae: 11.6988\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3905 - mse: 2.3905 - mae: 1.1075 - val_loss: 536.9720 - val_mse: 536.9720 - val_mae: 12.1662\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7008 - mse: 2.7008 - mae: 1.1956 - val_loss: 468.8510 - val_mse: 468.8510 - val_mae: 11.3173\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8966 - mse: 2.8966 - mae: 1.1580 - val_loss: 478.7816 - val_mse: 478.7816 - val_mae: 11.4179\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2920 - mse: 2.2920 - mae: 1.0745 - val_loss: 519.8594 - val_mse: 519.8594 - val_mae: 11.7491\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.2497 - mse: 2.2497 - mae: 1.0595 - val_loss: 556.6148 - val_mse: 556.6148 - val_mae: 12.1674\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3864 - mse: 3.3864 - mae: 1.2376 - val_loss: 500.6653 - val_mse: 500.6653 - val_mae: 11.5187\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3411 - mse: 2.3411 - mae: 1.0607 - val_loss: 458.5574 - val_mse: 458.5574 - val_mae: 11.2434\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6348 - mse: 2.6348 - mae: 1.1750 - val_loss: 468.6411 - val_mse: 468.6411 - val_mae: 11.1787\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7425 - mse: 3.7425 - mae: 1.3987 - val_loss: 529.6685 - val_mse: 529.6685 - val_mae: 12.0765\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8706 - mse: 4.8706 - mae: 1.4429 - val_loss: 533.8301 - val_mse: 533.8301 - val_mae: 12.1906\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7873 - mse: 3.7873 - mae: 1.3936 - val_loss: 444.5039 - val_mse: 444.5039 - val_mae: 10.9099\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9289 - mse: 4.9289 - mae: 1.4805 - val_loss: 484.1242 - val_mse: 484.1242 - val_mae: 11.5176\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0693 - mse: 4.0693 - mae: 1.4080 - val_loss: 552.7681 - val_mse: 552.7681 - val_mae: 12.5190\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6304 - mse: 4.6304 - mae: 1.5168 - val_loss: 480.4902 - val_mse: 480.4902 - val_mae: 11.0678\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5994 - mse: 3.5994 - mae: 1.3763 - val_loss: 485.6537 - val_mse: 485.6537 - val_mae: 11.6347\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8083 - mse: 2.8083 - mae: 1.1737 - val_loss: 477.8385 - val_mse: 477.8385 - val_mae: 11.2332\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9188 - mse: 2.9188 - mae: 1.2351 - val_loss: 504.4743 - val_mse: 504.4743 - val_mae: 11.5864\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1431 - mse: 2.1431 - mae: 1.0204 - val_loss: 537.9128 - val_mse: 537.9128 - val_mae: 11.9253\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1656 - mse: 4.1656 - mae: 1.2010 - val_loss: 532.1014 - val_mse: 532.1014 - val_mae: 12.1737\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8426 - mse: 4.8426 - mae: 1.5057 - val_loss: 464.0938 - val_mse: 464.0938 - val_mae: 10.9288\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3321 - mse: 2.3321 - mae: 1.1706 - val_loss: 527.7883 - val_mse: 527.7883 - val_mae: 12.0273\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5245 - mse: 2.5245 - mae: 1.0871 - val_loss: 550.7469 - val_mse: 550.7469 - val_mae: 11.8999\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4451 - mse: 3.4451 - mae: 1.1758 - val_loss: 495.5908 - val_mse: 495.5908 - val_mae: 11.6941\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8790 - mse: 2.8790 - mae: 1.2469 - val_loss: 462.8794 - val_mse: 462.8794 - val_mae: 11.1922\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7395 - mse: 2.7395 - mae: 1.1599 - val_loss: 451.0897 - val_mse: 451.0897 - val_mae: 11.1246\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7607 - mse: 2.7607 - mae: 1.1759 - val_loss: 533.7134 - val_mse: 533.7134 - val_mae: 12.0296\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8955 - mse: 2.8955 - mae: 1.1820 - val_loss: 487.2575 - val_mse: 487.2575 - val_mae: 11.7178\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4877 - mse: 2.4877 - mae: 1.1972 - val_loss: 489.0342 - val_mse: 489.0342 - val_mae: 11.3819\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5624 - mse: 1.5624 - mae: 0.9510 - val_loss: 495.2385 - val_mse: 495.2385 - val_mae: 11.6803\n",
      "Epoch 752/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4986 - mse: 1.4986 - mae: 0.8516 - val_loss: 515.0287 - val_mse: 515.0287 - val_mae: 11.6772\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5777 - mse: 1.5777 - mae: 0.8712 - val_loss: 520.3824 - val_mse: 520.3824 - val_mae: 11.8498\n",
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8085 - mse: 1.8085 - mae: 0.8928 - val_loss: 486.8596 - val_mse: 486.8596 - val_mae: 11.5638\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5107 - mse: 1.5107 - mae: 0.8489 - val_loss: 469.8951 - val_mse: 469.8951 - val_mae: 11.2449\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.2155 - mse: 2.2155 - mae: 1.0054 - val_loss: 496.0739 - val_mse: 496.0739 - val_mae: 11.4375\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4318 - mse: 1.4318 - mae: 0.8477 - val_loss: 541.9716 - val_mse: 541.9716 - val_mae: 12.0437\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7134 - mse: 1.7134 - mae: 0.8776 - val_loss: 514.5007 - val_mse: 514.5007 - val_mae: 11.7049\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1202 - mse: 2.1202 - mae: 1.0107 - val_loss: 487.4477 - val_mse: 487.4477 - val_mae: 11.4899\n",
      "Epoch 760/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7408 - mse: 1.7408 - mae: 0.9217 - val_loss: 485.5352 - val_mse: 485.5352 - val_mae: 11.3266\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5193 - mse: 1.5193 - mae: 0.8761 - val_loss: 516.5692 - val_mse: 516.5692 - val_mae: 11.7450\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5437 - mse: 1.5437 - mae: 0.8797 - val_loss: 504.9922 - val_mse: 504.9922 - val_mae: 11.9049\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4954 - mse: 1.4954 - mae: 0.8455 - val_loss: 501.9245 - val_mse: 501.9245 - val_mae: 11.6048\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2140 - mse: 1.2140 - mae: 0.7375 - val_loss: 482.3238 - val_mse: 482.3238 - val_mae: 11.5169\n",
      "Epoch 765/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1992 - mse: 1.1992 - mae: 0.7482 - val_loss: 480.5483 - val_mse: 480.5483 - val_mae: 11.4200\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4664 - mse: 1.4664 - mae: 0.8074 - val_loss: 503.1829 - val_mse: 503.1829 - val_mae: 11.7378\n",
      "Epoch 767/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3109 - mse: 1.3109 - mae: 0.7957 - val_loss: 510.6625 - val_mse: 510.6625 - val_mae: 11.8040\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1423 - mse: 1.1423 - mae: 0.7296 - val_loss: 515.1804 - val_mse: 515.1804 - val_mae: 11.7400\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2059 - mse: 1.2059 - mae: 0.7626 - val_loss: 497.2190 - val_mse: 497.2190 - val_mae: 11.7790\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4684 - mse: 1.4684 - mae: 0.8373 - val_loss: 492.4042 - val_mse: 492.4042 - val_mae: 11.3840\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5403 - mse: 1.5403 - mae: 0.8700 - val_loss: 477.1853 - val_mse: 477.1853 - val_mae: 11.7630\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9562 - mse: 1.9562 - mae: 0.9593 - val_loss: 488.1880 - val_mse: 488.1880 - val_mae: 11.1322\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1369 - mse: 2.1369 - mae: 1.0172 - val_loss: 463.7285 - val_mse: 463.7285 - val_mae: 11.6191\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2628 - mse: 3.2628 - mae: 1.0982 - val_loss: 570.2269 - val_mse: 570.2269 - val_mae: 11.6319\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0643 - mse: 5.0643 - mae: 1.5583 - val_loss: 533.0040 - val_mse: 533.0040 - val_mae: 12.5866\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.1397 - mse: 4.1397 - mae: 1.6037 - val_loss: 509.2478 - val_mse: 509.2478 - val_mae: 11.2620\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4066 - mse: 3.4066 - mae: 1.3324 - val_loss: 458.2268 - val_mse: 458.2268 - val_mae: 11.5077\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6174 - mse: 4.6174 - mae: 1.2907 - val_loss: 541.0995 - val_mse: 541.0995 - val_mae: 11.2815\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3778 - mse: 3.3778 - mae: 1.3094 - val_loss: 517.8721 - val_mse: 517.8721 - val_mae: 12.1586\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6559 - mse: 4.6559 - mae: 1.4039 - val_loss: 475.0806 - val_mse: 475.0806 - val_mae: 10.8840\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.1004 - mse: 8.1004 - mae: 2.0519 - val_loss: 528.3226 - val_mse: 528.3226 - val_mae: 12.5405\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.5400 - mse: 5.5400 - mae: 1.6676 - val_loss: 538.0878 - val_mse: 538.0878 - val_mae: 12.2960\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.6021 - mse: 4.6021 - mae: 1.4473 - val_loss: 491.1691 - val_mse: 491.1691 - val_mae: 11.7989\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.7315 - mse: 6.7315 - mae: 1.5246 - val_loss: 459.7282 - val_mse: 459.7282 - val_mae: 11.6398\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.8407 - mse: 2.8407 - mae: 1.1732 - val_loss: 450.7118 - val_mse: 450.7118 - val_mae: 11.0994\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.8942 - mse: 2.8942 - mae: 1.1707 - val_loss: 439.2154 - val_mse: 439.2154 - val_mae: 11.1752\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.3769 - mse: 3.3769 - mae: 1.2672 - val_loss: 499.8707 - val_mse: 499.8707 - val_mae: 11.7298\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.4760 - mse: 2.4760 - mae: 1.0925 - val_loss: 486.6964 - val_mse: 486.6964 - val_mae: 11.6242\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.0444 - mse: 3.0444 - mae: 1.1415 - val_loss: 521.1130 - val_mse: 521.1130 - val_mae: 12.0812\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 8.9130 - mse: 8.9130 - mae: 1.8494 - val_loss: 477.1110 - val_mse: 477.1110 - val_mae: 12.0405\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.9220 - mse: 4.9220 - mae: 1.6025 - val_loss: 413.1820 - val_mse: 413.1820 - val_mae: 10.7300\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 3.6088 - mse: 3.6088 - mae: 1.3040 - val_loss: 452.1064 - val_mse: 452.1064 - val_mae: 11.2883\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.6412 - mse: 2.6412 - mae: 1.0923 - val_loss: 504.9517 - val_mse: 504.9517 - val_mae: 11.6193\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 2.0821 - mse: 2.0821 - mae: 1.0533 - val_loss: 494.8295 - val_mse: 494.8295 - val_mae: 11.9807\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.9900 - mse: 1.9900 - mae: 1.0179 - val_loss: 515.6074 - val_mse: 515.6074 - val_mae: 11.8190\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3.6756 - mse: 3.6756 - mae: 1.1518 - val_loss: 539.9724 - val_mse: 539.9724 - val_mae: 12.3834\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.0682 - mse: 5.0682 - mae: 1.5835 - val_loss: 480.5135 - val_mse: 480.5135 - val_mae: 11.2357\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2.9898 - mse: 2.9898 - mae: 1.1659 - val_loss: 429.0035 - val_mse: 429.0035 - val_mae: 10.9719\n",
      "Epoch 799/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9499 - mse: 3.9499 - mae: 1.3510 - val_loss: 452.0792 - val_mse: 452.0792 - val_mae: 11.0963\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4592 - mse: 2.4592 - mae: 1.1238 - val_loss: 485.0670 - val_mse: 485.0670 - val_mae: 11.6839\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2399 - mse: 1.2399 - mae: 0.7994 - val_loss: 497.4708 - val_mse: 497.4708 - val_mae: 11.3835\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4556 - mse: 1.4556 - mae: 0.8208 - val_loss: 494.1652 - val_mse: 494.1652 - val_mae: 11.8142\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5653 - mse: 1.5653 - mae: 0.8592 - val_loss: 490.9700 - val_mse: 490.9700 - val_mae: 11.2810\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3309 - mse: 1.3309 - mae: 0.8817 - val_loss: 483.8801 - val_mse: 483.8801 - val_mae: 11.6284\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1178 - mse: 1.1178 - mae: 0.7423 - val_loss: 490.5996 - val_mse: 490.5996 - val_mae: 11.5049\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9115 - mse: 0.9115 - mae: 0.6460 - val_loss: 487.0311 - val_mse: 487.0311 - val_mae: 11.6409\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9847 - mse: 0.9847 - mae: 0.6744 - val_loss: 484.1924 - val_mse: 484.1924 - val_mae: 11.5461\n",
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9315 - mse: 0.9315 - mae: 0.6459 - val_loss: 474.3674 - val_mse: 474.3674 - val_mae: 11.4787\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8250 - mse: 0.8250 - mae: 0.6230 - val_loss: 489.1466 - val_mse: 489.1466 - val_mae: 11.5992\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.5982 - val_loss: 472.1884 - val_mse: 472.1884 - val_mae: 11.3721\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8744 - mse: 0.8744 - mae: 0.6441 - val_loss: 507.0255 - val_mse: 507.0255 - val_mae: 11.7849\n",
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3649 - mse: 1.3649 - mae: 0.7586 - val_loss: 502.8644 - val_mse: 502.8644 - val_mae: 11.8435\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2267 - mse: 1.2267 - mae: 0.7193 - val_loss: 488.5298 - val_mse: 488.5298 - val_mae: 11.5618\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4431 - mse: 1.4431 - mae: 0.8320 - val_loss: 471.4306 - val_mse: 471.4306 - val_mae: 11.3027\n",
      "Epoch 815/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1134 - mse: 1.1134 - mae: 0.7518 - val_loss: 486.7296 - val_mse: 486.7296 - val_mae: 11.6133\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8251 - mse: 0.8251 - mae: 0.6365 - val_loss: 482.0062 - val_mse: 482.0062 - val_mae: 11.5046\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8705 - mse: 0.8705 - mae: 0.6673 - val_loss: 494.5780 - val_mse: 494.5780 - val_mae: 11.6592\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9757 - mse: 0.9757 - mae: 0.7133 - val_loss: 509.5064 - val_mse: 509.5064 - val_mae: 11.8988\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5090 - mse: 1.5090 - mae: 0.8349 - val_loss: 485.4733 - val_mse: 485.4733 - val_mae: 11.3335\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4378 - mse: 1.4378 - mae: 0.8148 - val_loss: 439.5239 - val_mse: 439.5239 - val_mae: 10.9448\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4872 - mse: 3.4872 - mae: 1.3768 - val_loss: 489.0590 - val_mse: 489.0590 - val_mae: 11.3605\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8592 - mse: 2.8592 - mae: 1.1903 - val_loss: 523.8622 - val_mse: 523.8622 - val_mae: 12.0950\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9601 - mse: 1.9601 - mae: 0.9808 - val_loss: 531.7766 - val_mse: 531.7766 - val_mae: 12.0787\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0954 - mse: 6.0954 - mae: 1.5945 - val_loss: 462.6161 - val_mse: 462.6161 - val_mae: 11.0836\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7186 - mse: 5.7186 - mae: 1.4507 - val_loss: 436.7362 - val_mse: 436.7362 - val_mae: 10.6164\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.7154 - mse: 11.7154 - mae: 2.1517 - val_loss: 446.5616 - val_mse: 446.5616 - val_mae: 10.6237\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3796 - mse: 5.3796 - mae: 1.5054 - val_loss: 559.6559 - val_mse: 559.6559 - val_mae: 12.5600\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3339 - mse: 5.3339 - mae: 1.7890 - val_loss: 495.6043 - val_mse: 495.6043 - val_mae: 11.8026\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5044 - mse: 5.5044 - mae: 1.6207 - val_loss: 549.7034 - val_mse: 549.7034 - val_mae: 11.7929\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6010 - mse: 5.6010 - mae: 1.6092 - val_loss: 546.6241 - val_mse: 546.6241 - val_mae: 12.6700\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2096 - mse: 4.2096 - mae: 1.5528 - val_loss: 508.9618 - val_mse: 508.9618 - val_mae: 10.9490\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4828 - mse: 5.4828 - mae: 1.4904 - val_loss: 401.4098 - val_mse: 401.4098 - val_mae: 10.8080\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9754 - mse: 5.9754 - mae: 1.6777 - val_loss: 515.7444 - val_mse: 515.7444 - val_mae: 11.6691\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9728 - mse: 3.9728 - mae: 1.5605 - val_loss: 552.7729 - val_mse: 552.7729 - val_mae: 12.5177\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4619 - mse: 3.4619 - mae: 1.4135 - val_loss: 505.9963 - val_mse: 505.9963 - val_mae: 11.2779\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3876 - mse: 1.3876 - mae: 0.8305 - val_loss: 459.9609 - val_mse: 459.9609 - val_mae: 11.1025\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6421 - mse: 1.6421 - mae: 0.9052 - val_loss: 476.3874 - val_mse: 476.3874 - val_mae: 11.7283\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4358 - mse: 1.4358 - mae: 0.8490 - val_loss: 463.8871 - val_mse: 463.8871 - val_mae: 10.9521\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3200 - mse: 1.3200 - mae: 0.8521 - val_loss: 489.2152 - val_mse: 489.2152 - val_mae: 11.5744\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1741 - mse: 1.1741 - mae: 0.7711 - val_loss: 502.4779 - val_mse: 502.4779 - val_mae: 11.6414\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9508 - mse: 0.9508 - mae: 0.6817 - val_loss: 497.7672 - val_mse: 497.7672 - val_mae: 11.6043\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9094 - mse: 0.9094 - mae: 0.6804 - val_loss: 470.4879 - val_mse: 470.4879 - val_mae: 11.2192\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9032 - mse: 0.9032 - mae: 0.6927 - val_loss: 483.9398 - val_mse: 483.9398 - val_mae: 11.5421\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0799 - mse: 1.0799 - mae: 0.7582 - val_loss: 503.9023 - val_mse: 503.9023 - val_mae: 11.6111\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0881 - mse: 1.0881 - mae: 0.7512 - val_loss: 505.0685 - val_mse: 505.0685 - val_mae: 11.8401\n",
      "Epoch 846/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1071 - mse: 1.1071 - mae: 0.7321 - val_loss: 499.3349 - val_mse: 499.3349 - val_mae: 11.4531\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7521 - mse: 0.7521 - mae: 0.6157 - val_loss: 480.0521 - val_mse: 480.0521 - val_mae: 11.3798\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8420 - mse: 0.8420 - mae: 0.6453 - val_loss: 490.5251 - val_mse: 490.5251 - val_mae: 11.4863\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9067 - mse: 0.9067 - mae: 0.6792 - val_loss: 498.1465 - val_mse: 498.1465 - val_mae: 11.7238\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7223 - mse: 0.7223 - mae: 0.5823 - val_loss: 479.6825 - val_mse: 479.6825 - val_mae: 11.3840\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7499 - mse: 0.7499 - mae: 0.6117 - val_loss: 467.2705 - val_mse: 467.2705 - val_mae: 11.2821\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0603 - mse: 1.0603 - mae: 0.6812 - val_loss: 447.9510 - val_mse: 447.9510 - val_mae: 11.1045\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5961 - mse: 1.5961 - mae: 0.8401 - val_loss: 469.7983 - val_mse: 469.7983 - val_mae: 11.1013\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5214 - mse: 1.5214 - mae: 0.8738 - val_loss: 475.1938 - val_mse: 475.1938 - val_mae: 11.5477\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8943 - mse: 0.8943 - mae: 0.6549 - val_loss: 465.2610 - val_mse: 465.2610 - val_mae: 11.1807\n",
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8445 - mse: 0.8445 - mae: 0.6543 - val_loss: 488.1095 - val_mse: 488.1095 - val_mae: 11.5366\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7444 - mse: 0.7444 - mae: 0.6084 - val_loss: 496.9176 - val_mse: 496.9176 - val_mae: 11.5621\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1914 - mse: 1.1914 - mae: 0.7192 - val_loss: 489.0871 - val_mse: 489.0871 - val_mae: 11.8561\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9552 - mse: 1.9552 - mae: 1.0179 - val_loss: 467.1261 - val_mse: 467.1261 - val_mae: 11.1621\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9762 - mse: 1.9762 - mae: 0.9657 - val_loss: 440.4892 - val_mse: 440.4892 - val_mae: 10.7315\n",
      "Epoch 861/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8377 - mse: 2.8377 - mae: 1.0687 - val_loss: 488.3831 - val_mse: 488.3831 - val_mae: 11.1215\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1202 - mse: 2.1202 - mae: 0.9827 - val_loss: 422.7941 - val_mse: 422.7941 - val_mae: 10.9308\n",
      "Epoch 863/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.9561 - mse: 6.9561 - mae: 1.6254 - val_loss: 523.8983 - val_mse: 523.8983 - val_mae: 10.7740\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9361 - mse: 4.9361 - mae: 1.4832 - val_loss: 436.1856 - val_mse: 436.1856 - val_mae: 11.3515\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0556 - mse: 10.0556 - mae: 2.2000 - val_loss: 497.0413 - val_mse: 497.0413 - val_mae: 10.7470\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5293 - mse: 6.5293 - mae: 1.8606 - val_loss: 461.5316 - val_mse: 461.5316 - val_mae: 11.8873\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4245 - mse: 4.4245 - mae: 1.4303 - val_loss: 554.8040 - val_mse: 554.8040 - val_mae: 11.6041\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2722 - mse: 3.2722 - mae: 1.4199 - val_loss: 475.6845 - val_mse: 475.6845 - val_mae: 11.7973\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0547 - mse: 3.0547 - mae: 1.2979 - val_loss: 544.7200 - val_mse: 544.7200 - val_mae: 11.3314\n",
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9617 - mse: 2.9617 - mae: 1.3261 - val_loss: 489.0117 - val_mse: 489.0117 - val_mae: 11.9818\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4412 - mse: 2.4412 - mae: 1.2217 - val_loss: 519.1962 - val_mse: 519.1962 - val_mae: 11.3900\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0645 - mse: 2.0645 - mae: 1.0862 - val_loss: 486.7438 - val_mse: 486.7438 - val_mae: 11.6645\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7769 - mse: 1.7769 - mae: 0.9858 - val_loss: 512.2963 - val_mse: 512.2963 - val_mae: 11.1918\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0894 - mse: 2.0894 - mae: 1.0232 - val_loss: 493.3360 - val_mse: 493.3360 - val_mae: 11.9938\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1523 - mse: 2.1523 - mae: 1.1423 - val_loss: 533.3453 - val_mse: 533.3453 - val_mae: 11.9148\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7137 - mse: 5.7137 - mae: 1.4766 - val_loss: 454.6465 - val_mse: 454.6465 - val_mae: 11.2321\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3654 - mse: 5.3654 - mae: 1.5192 - val_loss: 439.1936 - val_mse: 439.1936 - val_mae: 10.5846\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0540 - mse: 4.0540 - mae: 1.3463 - val_loss: 396.0178 - val_mse: 396.0178 - val_mae: 10.8044\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5985 - mse: 6.5985 - mae: 1.7260 - val_loss: 445.1612 - val_mse: 445.1612 - val_mae: 10.7874\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0889 - mse: 6.0889 - mae: 1.6914 - val_loss: 597.1822 - val_mse: 597.1822 - val_mae: 11.9620\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.4718 - mse: 20.4718 - mae: 2.6879 - val_loss: 679.2751 - val_mse: 679.2751 - val_mae: 15.5975\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.1026 - mse: 30.1026 - mae: 4.2848 - val_loss: 549.3887 - val_mse: 549.3887 - val_mae: 11.0461\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.8044 - mse: 10.8044 - mae: 2.5474 - val_loss: 521.5596 - val_mse: 521.5596 - val_mae: 11.0347\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0046 - mse: 12.0046 - mae: 2.0564 - val_loss: 439.6046 - val_mse: 439.6046 - val_mae: 10.7649\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 20.8929 - mse: 20.8929 - mae: 2.8826 - val_loss: 392.7759 - val_mse: 392.7759 - val_mae: 10.6097\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.8614 - mse: 30.8614 - mae: 3.9908 - val_loss: 522.2621 - val_mse: 522.2621 - val_mae: 10.4091\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.8112 - mse: 22.8112 - mae: 2.9712 - val_loss: 622.5856 - val_mse: 622.5856 - val_mae: 12.8757\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.5810 - mse: 25.5810 - mae: 3.0802 - val_loss: 547.6177 - val_mse: 547.6177 - val_mae: 13.1115\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.6085 - mse: 33.6085 - mae: 3.5625 - val_loss: 627.8927 - val_mse: 627.8927 - val_mae: 13.3992\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 55.9595 - mse: 55.9595 - mae: 4.7933 - val_loss: 421.7308 - val_mse: 421.7308 - val_mae: 11.9859\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 69.3718 - mse: 69.3718 - mae: 5.7054 - val_loss: 358.4229 - val_mse: 358.4229 - val_mae: 12.3886\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 94.8998 - mse: 94.8998 - mae: 6.3128 - val_loss: 517.1479 - val_mse: 517.1479 - val_mae: 13.7259\n",
      "Epoch 893/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.7487 - mse: 64.7487 - mae: 5.2047 - val_loss: 902.5520 - val_mse: 902.5520 - val_mae: 14.8051\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 75.0881 - mse: 75.0881 - mae: 4.6921 - val_loss: 751.0204 - val_mse: 751.0204 - val_mae: 15.0870\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 53.1724 - mse: 53.1724 - mae: 5.3191 - val_loss: 497.1960 - val_mse: 497.1960 - val_mae: 12.9158\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 71.8480 - mse: 71.8480 - mae: 6.1885 - val_loss: 764.8918 - val_mse: 764.8917 - val_mae: 16.7399\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.7777 - mse: 39.7777 - mae: 4.6804 - val_loss: 823.8302 - val_mse: 823.8302 - val_mae: 13.0790\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 61.6419 - mse: 61.6419 - mae: 4.3541 - val_loss: 486.6514 - val_mse: 486.6514 - val_mae: 11.5351\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.4079 - mse: 48.4079 - mae: 4.4202 - val_loss: 475.4086 - val_mse: 475.4086 - val_mae: 10.8926\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.3271 - mse: 44.3271 - mae: 4.9739 - val_loss: 742.9920 - val_mse: 742.9920 - val_mae: 17.9848\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 84.6790 - mse: 84.6790 - mae: 5.6106 - val_loss: 972.3035 - val_mse: 972.3035 - val_mae: 14.8127\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 172.0749 - mse: 172.0749 - mae: 7.4184 - val_loss: 363.7119 - val_mse: 363.7119 - val_mae: 11.2495\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 100.0217 - mse: 100.0217 - mae: 7.1098 - val_loss: 331.4009 - val_mse: 331.4009 - val_mae: 10.8195\n",
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66.9875 - mse: 66.9875 - mae: 5.2347 - val_loss: 647.2302 - val_mse: 647.2302 - val_mae: 13.3289\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.5271 - mse: 36.5271 - mae: 3.8146 - val_loss: 341.5213 - val_mse: 341.5213 - val_mae: 11.2478\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 59.7623 - mse: 59.7623 - mae: 4.8299 - val_loss: 447.1320 - val_mse: 447.1320 - val_mae: 10.7229\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.6384 - mse: 33.6384 - mae: 4.3348 - val_loss: 626.7630 - val_mse: 626.7630 - val_mae: 14.9125\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.3175 - mse: 28.3175 - mae: 3.8333 - val_loss: 440.2776 - val_mse: 440.2776 - val_mae: 11.2645\n",
      "Epoch 909/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 9.8653 - mse: 9.8653 - mae: 2.2207 - val_loss: 545.3167 - val_mse: 545.3167 - val_mae: 11.4454\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.7206 - mse: 7.7206 - mae: 1.9854 - val_loss: 563.0458 - val_mse: 563.0458 - val_mae: 12.5599\n",
      "Epoch 911/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7240 - mse: 9.7240 - mae: 2.3028 - val_loss: 419.7339 - val_mse: 419.7339 - val_mae: 10.3882\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.4531 - mse: 7.4531 - mae: 1.7856 - val_loss: 526.3167 - val_mse: 526.3167 - val_mae: 12.1301\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6435 - mse: 5.6435 - mae: 1.6940 - val_loss: 413.2839 - val_mse: 413.2839 - val_mae: 10.7303\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1427 - mse: 6.1427 - mae: 1.5062 - val_loss: 475.8867 - val_mse: 475.8867 - val_mae: 11.3157\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0311 - mse: 3.0311 - mae: 1.2292 - val_loss: 502.7876 - val_mse: 502.7876 - val_mae: 11.2331\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7191 - mse: 2.7191 - mae: 1.2205 - val_loss: 530.2834 - val_mse: 530.2834 - val_mae: 11.9533\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2528 - mse: 2.2528 - mae: 1.1413 - val_loss: 498.3104 - val_mse: 498.3104 - val_mae: 11.3301\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1859 - mse: 2.1859 - mae: 1.1030 - val_loss: 495.0702 - val_mse: 495.0702 - val_mae: 11.3283\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1617 - mse: 2.1617 - mae: 1.0713 - val_loss: 543.7342 - val_mse: 543.7342 - val_mae: 11.7624\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3282 - mse: 2.3282 - mae: 1.0904 - val_loss: 505.1466 - val_mse: 505.1466 - val_mae: 11.4395\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7342 - mse: 1.7342 - mae: 0.9394 - val_loss: 491.4410 - val_mse: 491.4410 - val_mae: 11.2983\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6910 - mse: 1.6910 - mae: 0.9504 - val_loss: 517.4772 - val_mse: 517.4772 - val_mae: 11.7178\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6472 - mse: 1.6472 - mae: 0.9132 - val_loss: 499.2295 - val_mse: 499.2295 - val_mae: 11.6180\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4821 - mse: 1.4821 - mae: 0.8802 - val_loss: 485.0814 - val_mse: 485.0814 - val_mae: 11.1205\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3546 - mse: 1.3546 - mae: 0.8601 - val_loss: 519.1061 - val_mse: 519.1061 - val_mae: 11.8554\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4273 - mse: 1.4273 - mae: 0.9116 - val_loss: 483.5410 - val_mse: 483.5410 - val_mae: 11.1501\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5095 - mse: 1.5095 - mae: 0.8775 - val_loss: 488.1690 - val_mse: 488.1690 - val_mae: 11.5040\n",
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2901 - mse: 1.2901 - mae: 0.8395 - val_loss: 508.7899 - val_mse: 508.7899 - val_mae: 11.4719\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1844 - mse: 1.1844 - mae: 0.7677 - val_loss: 504.6622 - val_mse: 504.6622 - val_mae: 11.3985\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9770 - mse: 0.9770 - mae: 0.6848 - val_loss: 516.6718 - val_mse: 516.6718 - val_mae: 11.6039\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1116 - mse: 1.1116 - mae: 0.7494 - val_loss: 508.4447 - val_mse: 508.4447 - val_mae: 11.4610\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9491 - mse: 0.9491 - mae: 0.6600 - val_loss: 494.4556 - val_mse: 494.4556 - val_mae: 11.3597\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0337 - mse: 1.0337 - mae: 0.7137 - val_loss: 511.8217 - val_mse: 511.8217 - val_mae: 11.5606\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9372 - mse: 0.9372 - mae: 0.6994 - val_loss: 498.0161 - val_mse: 498.0161 - val_mae: 11.4073\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0176 - mse: 1.0176 - mae: 0.6909 - val_loss: 497.7798 - val_mse: 497.7798 - val_mae: 11.3030\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9284 - mse: 0.9284 - mae: 0.6787 - val_loss: 507.6424 - val_mse: 507.6424 - val_mae: 11.4747\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9901 - mse: 0.9901 - mae: 0.6843 - val_loss: 496.3401 - val_mse: 496.3401 - val_mae: 11.4579\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0056 - mse: 1.0056 - mae: 0.6788 - val_loss: 492.7312 - val_mse: 492.7312 - val_mae: 11.2856\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8809 - mse: 0.8809 - mae: 0.6519 - val_loss: 508.6175 - val_mse: 508.6175 - val_mae: 11.5344\n",
      "Epoch 940/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8996 - mse: 0.8996 - mae: 0.6510 - val_loss: 513.7014 - val_mse: 513.7014 - val_mae: 11.5223\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9780 - mse: 0.9780 - mae: 0.7050 - val_loss: 494.2997 - val_mse: 494.2997 - val_mae: 11.2911\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9254 - mse: 0.9254 - mae: 0.6719 - val_loss: 486.6815 - val_mse: 486.6815 - val_mae: 11.3122\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0378 - mse: 1.0378 - mae: 0.7018 - val_loss: 504.2375 - val_mse: 504.2375 - val_mae: 11.4695\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8939 - mse: 0.8939 - mae: 0.6459 - val_loss: 497.7725 - val_mse: 497.7725 - val_mae: 11.4678\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9432 - mse: 0.9432 - mae: 0.6853 - val_loss: 488.3270 - val_mse: 488.3270 - val_mae: 11.2219\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8320 - mse: 0.8320 - mae: 0.6252 - val_loss: 500.4239 - val_mse: 500.4239 - val_mae: 11.4576\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7392 - mse: 0.7392 - mae: 0.5725 - val_loss: 504.3393 - val_mse: 504.3393 - val_mae: 11.4874\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8462 - mse: 0.8462 - mae: 0.6448 - val_loss: 494.6297 - val_mse: 494.6297 - val_mae: 11.3712\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7836 - mse: 0.7836 - mae: 0.5912 - val_loss: 505.1516 - val_mse: 505.1516 - val_mae: 11.4839\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6946 - mse: 0.6946 - mae: 0.5673 - val_loss: 497.5291 - val_mse: 497.5291 - val_mae: 11.3847\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7272 - mse: 0.7272 - mae: 0.5782 - val_loss: 499.8755 - val_mse: 499.8755 - val_mae: 11.4232\n",
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7206 - mse: 0.7206 - mae: 0.5632 - val_loss: 504.7924 - val_mse: 504.7924 - val_mae: 11.4387\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7584 - mse: 0.7584 - mae: 0.5973 - val_loss: 493.1510 - val_mse: 493.1510 - val_mae: 11.4098\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7733 - mse: 0.7733 - mae: 0.5990 - val_loss: 518.8817 - val_mse: 518.8817 - val_mae: 11.5360\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1195 - mse: 1.1195 - mae: 0.7091 - val_loss: 505.9792 - val_mse: 505.9792 - val_mae: 11.6340\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9495 - mse: 0.9495 - mae: 0.7188 - val_loss: 485.6535 - val_mse: 485.6535 - val_mae: 11.2255\n",
      "Epoch 957/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8943 - mse: 0.8943 - mae: 0.6429 - val_loss: 505.4066 - val_mse: 505.4066 - val_mae: 11.4743\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8832 - mse: 0.8832 - mae: 0.6619 - val_loss: 525.9522 - val_mse: 525.9522 - val_mae: 11.8230\n",
      "Epoch 959/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9312 - mse: 0.9312 - mae: 0.6709 - val_loss: 474.3940 - val_mse: 474.3940 - val_mae: 11.0057\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4276 - mse: 1.4276 - mae: 0.8713 - val_loss: 489.8248 - val_mse: 489.8248 - val_mae: 11.2762\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0871 - mse: 1.0871 - mae: 0.7227 - val_loss: 513.8655 - val_mse: 513.8655 - val_mae: 11.5834\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2881 - mse: 1.2881 - mae: 0.7451 - val_loss: 530.1398 - val_mse: 530.1398 - val_mae: 11.8383\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3353 - mse: 1.3353 - mae: 0.7741 - val_loss: 501.8456 - val_mse: 501.8456 - val_mae: 11.4379\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7864 - mse: 0.7864 - mae: 0.6460 - val_loss: 493.6469 - val_mse: 493.6469 - val_mae: 11.2994\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8992 - mse: 0.8992 - mae: 0.6752 - val_loss: 510.0744 - val_mse: 510.0744 - val_mae: 11.5542\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6532 - mse: 0.6532 - mae: 0.5364 - val_loss: 512.8121 - val_mse: 512.8121 - val_mae: 11.5894\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6636 - mse: 0.6636 - mae: 0.5603 - val_loss: 492.0948 - val_mse: 492.0948 - val_mae: 11.2440\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7008 - mse: 0.7008 - mae: 0.5966 - val_loss: 513.3889 - val_mse: 513.3889 - val_mae: 11.5172\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7004 - mse: 0.7004 - mae: 0.5681 - val_loss: 508.2388 - val_mse: 508.2388 - val_mae: 11.4659\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6516 - mse: 0.6516 - mae: 0.5442 - val_loss: 500.9642 - val_mse: 500.9642 - val_mae: 11.5016\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6185 - mse: 0.6185 - mae: 0.5242 - val_loss: 503.5558 - val_mse: 503.5558 - val_mae: 11.4129\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6044 - mse: 0.6044 - mae: 0.5250 - val_loss: 502.5868 - val_mse: 502.5868 - val_mae: 11.5473\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7658 - mse: 0.7658 - mae: 0.6154 - val_loss: 484.3937 - val_mse: 484.3937 - val_mae: 11.2028\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7591 - mse: 0.7591 - mae: 0.5968 - val_loss: 495.0993 - val_mse: 495.0993 - val_mae: 11.4277\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8560 - mse: 0.8560 - mae: 0.6065 - val_loss: 512.7531 - val_mse: 512.7531 - val_mae: 11.4647\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6995 - mse: 0.6995 - mae: 0.5822 - val_loss: 514.4453 - val_mse: 514.4453 - val_mae: 11.6082\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6928 - mse: 0.6928 - mae: 0.5664 - val_loss: 509.8777 - val_mse: 509.8777 - val_mae: 11.4335\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8164 - mse: 0.8164 - mae: 0.6067 - val_loss: 495.2672 - val_mse: 495.2672 - val_mae: 11.3742\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7860 - mse: 0.7860 - mae: 0.6107 - val_loss: 499.9427 - val_mse: 499.9427 - val_mae: 11.3527\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6024 - mse: 0.6024 - mae: 0.5282 - val_loss: 493.6854 - val_mse: 493.6854 - val_mae: 11.4496\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5932 - mse: 0.5932 - mae: 0.5214 - val_loss: 493.7487 - val_mse: 493.7487 - val_mae: 11.3006\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5441 - mse: 0.5441 - mae: 0.4931 - val_loss: 493.5937 - val_mse: 493.5937 - val_mae: 11.3423\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5393 - mse: 0.5393 - mae: 0.4981 - val_loss: 501.7847 - val_mse: 501.7847 - val_mae: 11.4065\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5893 - mse: 0.5893 - mae: 0.4951 - val_loss: 493.9417 - val_mse: 493.9417 - val_mae: 11.2907\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5775 - mse: 0.5775 - mae: 0.5254 - val_loss: 517.8774 - val_mse: 517.8774 - val_mae: 11.6137\n",
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6995 - mse: 0.6995 - mae: 0.5871 - val_loss: 505.1089 - val_mse: 505.1089 - val_mae: 11.3918\n",
      "Epoch 987/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7614 - mse: 0.7614 - mae: 0.6279 - val_loss: 469.7487 - val_mse: 469.7487 - val_mae: 11.0831\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3491 - mse: 1.3491 - mae: 0.7655 - val_loss: 499.3348 - val_mse: 499.3348 - val_mae: 11.3073\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4496 - mse: 1.4496 - mae: 0.8588 - val_loss: 519.9449 - val_mse: 519.9449 - val_mae: 11.8117\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9740 - mse: 0.9740 - mae: 0.6986 - val_loss: 508.1638 - val_mse: 508.1638 - val_mae: 11.3791\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6362 - mse: 0.6362 - mae: 0.5498 - val_loss: 502.6108 - val_mse: 502.6108 - val_mae: 11.5209\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6510 - mse: 0.6510 - mae: 0.5451 - val_loss: 504.1534 - val_mse: 504.1534 - val_mae: 11.3244\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7404 - mse: 0.7404 - mae: 0.5606 - val_loss: 479.0247 - val_mse: 479.0247 - val_mae: 11.1725\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8419 - mse: 0.8419 - mae: 0.6021 - val_loss: 496.8713 - val_mse: 496.8713 - val_mae: 11.3733\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8441 - mse: 0.8441 - mae: 0.6207 - val_loss: 517.8904 - val_mse: 517.8904 - val_mae: 11.6389\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1376 - mse: 1.1376 - mae: 0.7263 - val_loss: 505.6480 - val_mse: 505.6480 - val_mae: 11.6910\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9698 - mse: 0.9698 - mae: 0.6951 - val_loss: 503.1162 - val_mse: 503.1162 - val_mae: 11.3843\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2243 - mse: 1.2243 - mae: 0.7055 - val_loss: 454.1516 - val_mse: 454.1516 - val_mae: 11.0829\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4409 - mse: 2.4409 - mae: 1.0161 - val_loss: 523.2290 - val_mse: 523.2290 - val_mae: 11.3241\n",
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3369 - mse: 2.3369 - mae: 1.0895 - val_loss: 516.8870 - val_mse: 516.8870 - val_mae: 11.5479\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=32, epochs=epochs, validation_data =(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "CRUU1ZEfaS-r",
    "outputId": "14c3e54b-2f90-4ba4-f4b2-f71a57a18200"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3c984f09d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('model loss of Train and Validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(hist.history['loss'], marker = 'o', ms = 2, label='train_loss')\n",
    "plt.plot(hist.history['val_loss'], marker = 'o', ms = 2, label='validation_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "yhnj8MKNawcc",
    "outputId": "cb646a02-0991-4325-e562-974b1b376646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cdcf366e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('model MAE of Train and Validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mae')\n",
    "plt.plot(hist.history['mae'], marker = 'o', ms = 2, label='train_mae')\n",
    "plt.plot(hist.history['val_mae'], marker = 'o', ms = 2, label='val_mae')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_DWk0S5awWu",
    "outputId": "bad459c6-a289-49db-99ba-384626e85ffb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step\n",
      "예측값 :  [162.84047]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "MCply5YZawNY",
    "outputId": "fd880b41-5cc4-42a7-d3d2-ba3999150d8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cdcff9bd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i for i in range(len(preds))]\n",
    "plt.plot(x_axis,preds,label='preds')\n",
    "plt.plot(x_axis,Y_val,label='labels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHdieDXVdmkE",
    "outputId": "2e2b7f42-af51-46b7-9ad7-90a12f0307fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712209833710711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWB-PADSBFD7"
   },
   "source": [
    "### 1.3 EarlyStopping 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSKFacWib7Wf"
   },
   "source": [
    "Early stopping을 걸어주어 설정한 epoch동안 모델이 개선되지 않을시 학습을 조기 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4ksoZzobb7Ia"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "\n",
    "################# 자유롭게 MLP 모델을 구현합니다.\n",
    "model2.add(tf.keras.Input(shape = 3))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(200))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(300))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(200))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(100))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlOdQuizcwDH",
    "outputId": "617da91f-0928-4773-f32c-14f72ae00258"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "loss = MeanSquaredError()\n",
    "epochs = 1000\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 100)               400       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 300)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K6RisKwtb7Fi"
   },
   "outputs": [],
   "source": [
    "# Early stopping : 성능 개선이 없으면 종료\n",
    "earlystopping = EarlyStopping(monitor='val_loss',  # 모니터 기준 설정 (val loss) \n",
    "                patience=10,                       # 10회 Epoch동안 개선되지 않는다면 종료\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dZxfxVyBchvp"
   },
   "outputs": [],
   "source": [
    "# 체크포인트: 중간에 모델을 저장하고 싶을 때 저장하는 기능\n",
    "ckpt_name = 'checkpoint-epoch-{}-batch-{}-trial-001.h5'.format(epochs, batch_size)\n",
    "checkpoint = ModelCheckpoint(ckpt_name,             # file명을 지정합니다\n",
    "                            monitor='val_loss',     # val_loss 값이 개선되었을때 호출됩니다\n",
    "                            verbose=1,              # 로그를 출력합니다\n",
    "                            save_best_only=True,    # 가장 best 값만 저장합니다\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7X5W_ppb7Cb",
    "outputId": "e387def9-21ce-4342-8dbb-41207463d848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 100)               400       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 300)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzOgOiLsb64A",
    "outputId": "48c318bb-50fb-4bba-9a53-22ce52f7ee1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/6 [====>.........................] - ETA: 2s - loss: 20495.3477 - mse: 20495.3477 - mae: 93.5654\n",
      "Epoch 1: val_loss improved from inf to 34027.22266, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 1s 55ms/step - loss: 23043.7285 - mse: 23043.7285 - mae: 102.0430 - val_loss: 34027.2227 - val_mse: 34027.2227 - val_mae: 126.8121\n",
      "Epoch 2/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15284.4463 - mse: 15284.4463 - mae: 87.6809\n",
      "Epoch 2: val_loss improved from 34027.22266 to 33708.11328, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 22874.4824 - mse: 22874.4824 - mae: 101.2730 - val_loss: 33708.1133 - val_mse: 33708.1133 - val_mae: 125.6326\n",
      "Epoch 3/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22057.8008 - mse: 22057.8008 - mae: 96.8879\n",
      "Epoch 3: val_loss improved from 33708.11328 to 32977.06250, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 22539.4082 - mse: 22539.4082 - mae: 99.5655 - val_loss: 32977.0625 - val_mse: 32977.0625 - val_mae: 122.8699\n",
      "Epoch 4/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24986.0527 - mse: 24986.0527 - mae: 88.5832\n",
      "Epoch 4: val_loss improved from 32977.06250 to 31321.84180, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 21780.3828 - mse: 21780.3848 - mae: 95.8034 - val_loss: 31321.8418 - val_mse: 31321.8418 - val_mae: 116.9621\n",
      "Epoch 5/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14471.3271 - mse: 14471.3271 - mae: 72.9847\n",
      "Epoch 5: val_loss improved from 31321.84180 to 27917.11914, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 19950.1367 - mse: 19950.1367 - mae: 88.8302 - val_loss: 27917.1191 - val_mse: 27917.1191 - val_mae: 105.7815\n",
      "Epoch 6/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18294.1875 - mse: 18294.1875 - mae: 90.1923\n",
      "Epoch 6: val_loss improved from 27917.11914 to 21966.85352, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 17126.0957 - mse: 17126.0957 - mae: 78.2763 - val_loss: 21966.8535 - val_mse: 21966.8535 - val_mae: 85.5756\n",
      "Epoch 7/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21721.7891 - mse: 21721.7891 - mae: 79.7025\n",
      "Epoch 7: val_loss improved from 21966.85352 to 14752.99609, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 12775.5176 - mse: 12775.5176 - mae: 69.6564 - val_loss: 14752.9961 - val_mse: 14752.9961 - val_mae: 69.1717\n",
      "Epoch 8/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4671.2300 - mse: 4671.2300 - mae: 52.8733\n",
      "Epoch 8: val_loss improved from 14752.99609 to 10243.96094, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 8790.5713 - mse: 8790.5713 - mae: 67.2167 - val_loss: 10243.9609 - val_mse: 10243.9609 - val_mae: 68.6172\n",
      "Epoch 9/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7368.8433 - mse: 7368.8433 - mae: 66.2019\n",
      "Epoch 9: val_loss improved from 10243.96094 to 7764.36133, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 6838.4180 - mse: 6838.4180 - mae: 65.4406 - val_loss: 7764.3613 - val_mse: 7764.3613 - val_mae: 58.6868\n",
      "Epoch 10/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3652.8936 - mse: 3652.8936 - mae: 52.4440\n",
      "Epoch 10: val_loss improved from 7764.36133 to 6312.40674, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 4651.1025 - mse: 4651.1025 - mae: 50.1110 - val_loss: 6312.4067 - val_mse: 6312.4067 - val_mae: 45.9980\n",
      "Epoch 11/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4802.0273 - mse: 4802.0273 - mae: 46.4620\n",
      "Epoch 11: val_loss improved from 6312.40674 to 5186.98926, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3291.7275 - mse: 3291.7275 - mae: 37.6581 - val_loss: 5186.9893 - val_mse: 5186.9893 - val_mae: 38.2607\n",
      "Epoch 12/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2999.0513 - mse: 2999.0513 - mae: 34.8467\n",
      "Epoch 12: val_loss improved from 5186.98926 to 4237.23389, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2513.7893 - mse: 2513.7893 - mae: 31.3423 - val_loss: 4237.2339 - val_mse: 4237.2339 - val_mae: 35.1476\n",
      "Epoch 13/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2258.2646 - mse: 2258.2646 - mae: 28.4211\n",
      "Epoch 13: val_loss improved from 4237.23389 to 3676.76172, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2017.7577 - mse: 2017.7577 - mae: 28.9026 - val_loss: 3676.7617 - val_mse: 3676.7617 - val_mae: 34.2385\n",
      "Epoch 14/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4691.0947 - mse: 4691.0947 - mae: 41.0699\n",
      "Epoch 14: val_loss improved from 3676.76172 to 3424.40137, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1726.3165 - mse: 1726.3165 - mae: 26.6978 - val_loss: 3424.4014 - val_mse: 3424.4014 - val_mae: 29.7791\n",
      "Epoch 15/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 966.4091 - mse: 966.4091 - mae: 24.5512\n",
      "Epoch 15: val_loss improved from 3424.40137 to 3423.51978, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1628.0785 - mse: 1628.0785 - mae: 23.6314 - val_loss: 3423.5198 - val_mse: 3423.5198 - val_mae: 27.4003\n",
      "Epoch 16/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1210.5110 - mse: 1210.5110 - mae: 19.0383\n",
      "Epoch 16: val_loss improved from 3423.51978 to 2819.04590, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1459.6813 - mse: 1459.6813 - mae: 23.5724 - val_loss: 2819.0459 - val_mse: 2819.0459 - val_mae: 32.1930\n",
      "Epoch 17/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 727.4651 - mse: 727.4651 - mae: 20.9927\n",
      "Epoch 17: val_loss improved from 2819.04590 to 2723.61914, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1353.8903 - mse: 1353.8903 - mae: 25.0498 - val_loss: 2723.6191 - val_mse: 2723.6191 - val_mae: 29.5529\n",
      "Epoch 18/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3170.0762 - mse: 3170.0762 - mae: 30.0603\n",
      "Epoch 18: val_loss did not improve from 2723.61914\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1248.2371 - mse: 1248.2371 - mae: 23.4100 - val_loss: 2799.6985 - val_mse: 2799.6985 - val_mae: 25.9237\n",
      "Epoch 19/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2212.7461 - mse: 2212.7461 - mae: 26.6697\n",
      "Epoch 19: val_loss did not improve from 2723.61914\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1235.5745 - mse: 1235.5743 - mae: 21.7127 - val_loss: 2897.2366 - val_mse: 2897.2366 - val_mae: 25.4950\n",
      "Epoch 20/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 550.0756 - mse: 550.0756 - mae: 17.8530\n",
      "Epoch 20: val_loss improved from 2723.61914 to 2612.89990, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1173.0037 - mse: 1173.0037 - mae: 20.8805 - val_loss: 2612.8999 - val_mse: 2612.8999 - val_mae: 25.1622\n",
      "Epoch 21/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 510.5507 - mse: 510.5507 - mae: 16.6118\n",
      "Epoch 21: val_loss improved from 2612.89990 to 2320.83154, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1088.3820 - mse: 1088.3820 - mae: 21.6920 - val_loss: 2320.8315 - val_mse: 2320.8315 - val_mae: 27.4246\n",
      "Epoch 22/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 660.5071 - mse: 660.5071 - mae: 16.7955\n",
      "Epoch 22: val_loss improved from 2320.83154 to 2319.48462, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1094.6311 - mse: 1094.6311 - mae: 22.0286 - val_loss: 2319.4846 - val_mse: 2319.4846 - val_mae: 25.2228\n",
      "Epoch 23/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 915.8230 - mse: 915.8230 - mae: 22.4222\n",
      "Epoch 23: val_loss did not improve from 2319.48462\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1034.7747 - mse: 1034.7747 - mae: 19.9682 - val_loss: 2485.5259 - val_mse: 2485.5259 - val_mae: 23.7695\n",
      "Epoch 24/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1649.5785 - mse: 1649.5785 - mae: 21.3337\n",
      "Epoch 24: val_loss did not improve from 2319.48462\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1000.6251 - mse: 1000.6251 - mae: 19.1070 - val_loss: 2368.7173 - val_mse: 2368.7173 - val_mae: 23.5035\n",
      "Epoch 25/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2022.8976 - mse: 2022.8976 - mae: 27.2037\n",
      "Epoch 25: val_loss improved from 2319.48462 to 2082.01392, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 52ms/step - loss: 998.3177 - mse: 998.3178 - mae: 19.8596 - val_loss: 2082.0139 - val_mse: 2082.0139 - val_mae: 24.5169\n",
      "Epoch 26/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1463.7925 - mse: 1463.7925 - mae: 24.3417\n",
      "Epoch 26: val_loss did not improve from 2082.01392\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 943.7997 - mse: 943.7997 - mae: 19.4750 - val_loss: 2183.2192 - val_mse: 2183.2192 - val_mae: 22.9490\n",
      "Epoch 27/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1041.4469 - mse: 1041.4469 - mae: 17.7095\n",
      "Epoch 27: val_loss did not improve from 2082.01392\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 899.6646 - mse: 899.6646 - mae: 18.2012 - val_loss: 2275.8948 - val_mse: 2275.8948 - val_mae: 22.7140\n",
      "Epoch 28/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 348.0560 - mse: 348.0560 - mae: 15.2431\n",
      "Epoch 28: val_loss improved from 2082.01392 to 2028.44812, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 866.0206 - mse: 866.0206 - mae: 17.8997 - val_loss: 2028.4481 - val_mse: 2028.4481 - val_mae: 22.7185\n",
      "Epoch 29/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 592.2015 - mse: 592.2015 - mae: 14.3988\n",
      "Epoch 29: val_loss did not improve from 2028.44812\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 863.3010 - mse: 863.3010 - mae: 18.0149 - val_loss: 2073.6396 - val_mse: 2073.6396 - val_mae: 22.2896\n",
      "Epoch 30/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1536.3278 - mse: 1536.3278 - mae: 22.0681\n",
      "Epoch 30: val_loss improved from 2028.44812 to 1959.66895, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 893.6445 - mse: 893.6445 - mae: 18.5699 - val_loss: 1959.6689 - val_mse: 1959.6689 - val_mae: 22.2387\n",
      "Epoch 31/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 979.4598 - mse: 979.4598 - mae: 17.5101\n",
      "Epoch 31: val_loss did not improve from 1959.66895\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 792.9910 - mse: 792.9910 - mae: 17.3150 - val_loss: 2207.3188 - val_mse: 2207.3188 - val_mae: 22.3129\n",
      "Epoch 32/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 874.9514 - mse: 874.9514 - mae: 20.1130\n",
      "Epoch 32: val_loss did not improve from 1959.66895\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 850.5576 - mse: 850.5576 - mae: 17.0592 - val_loss: 2167.5557 - val_mse: 2167.5557 - val_mae: 21.5702\n",
      "Epoch 33/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 981.7435 - mse: 981.7435 - mae: 17.6332\n",
      "Epoch 33: val_loss improved from 1959.66895 to 1797.03467, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 809.0659 - mse: 809.0659 - mae: 17.4681 - val_loss: 1797.0347 - val_mse: 1797.0347 - val_mae: 21.8608\n",
      "Epoch 34/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 800.0721 - mse: 800.0721 - mae: 19.5026\n",
      "Epoch 34: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 805.1754 - mse: 805.1754 - mae: 17.6015 - val_loss: 1994.1646 - val_mse: 1994.1646 - val_mae: 21.2507\n",
      "Epoch 35/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 915.1993 - mse: 915.1993 - mae: 19.8944\n",
      "Epoch 35: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 779.7338 - mse: 779.7338 - mae: 16.7302 - val_loss: 1887.0690 - val_mse: 1887.0690 - val_mae: 21.2597\n",
      "Epoch 36/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 661.3594 - mse: 661.3594 - mae: 16.1321\n",
      "Epoch 36: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 762.4416 - mse: 762.4416 - mae: 16.7694 - val_loss: 1867.0858 - val_mse: 1867.0858 - val_mae: 21.1842\n",
      "Epoch 37/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1424.0607 - mse: 1424.0607 - mae: 24.0375\n",
      "Epoch 37: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 744.2551 - mse: 744.2551 - mae: 16.7237 - val_loss: 1847.9696 - val_mse: 1847.9696 - val_mae: 20.9076\n",
      "Epoch 38/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 792.5924 - mse: 792.5924 - mae: 17.1502\n",
      "Epoch 38: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 745.5126 - mse: 745.5126 - mae: 16.4439 - val_loss: 1883.4464 - val_mse: 1883.4464 - val_mae: 20.8027\n",
      "Epoch 39/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 218.4975 - mse: 218.4975 - mae: 11.8880\n",
      "Epoch 39: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 719.8563 - mse: 719.8563 - mae: 16.2705 - val_loss: 1799.3342 - val_mse: 1799.3342 - val_mae: 20.5867\n",
      "Epoch 40/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 368.9381 - mse: 368.9381 - mae: 13.3519\n",
      "Epoch 40: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 713.2551 - mse: 713.2552 - mae: 16.2745 - val_loss: 1811.8923 - val_mse: 1811.8923 - val_mae: 20.5710\n",
      "Epoch 41/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 358.1895 - mse: 358.1895 - mae: 12.8726\n",
      "Epoch 41: val_loss did not improve from 1797.03467\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 775.2592 - mse: 775.2592 - mae: 16.5923 - val_loss: 1866.9387 - val_mse: 1866.9387 - val_mae: 20.4720\n",
      "Epoch 42/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 242.1712 - mse: 242.1712 - mae: 9.5293\n",
      "Epoch 42: val_loss improved from 1797.03467 to 1636.52295, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 722.8068 - mse: 722.8068 - mae: 16.4027 - val_loss: 1636.5229 - val_mse: 1636.5229 - val_mae: 20.2836\n",
      "Epoch 43/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1668.6389 - mse: 1668.6389 - mae: 26.0431\n",
      "Epoch 43: val_loss did not improve from 1636.52295\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 691.8501 - mse: 691.8501 - mae: 16.0595 - val_loss: 1756.7212 - val_mse: 1756.7212 - val_mae: 20.3454\n",
      "Epoch 44/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1050.1704 - mse: 1050.1704 - mae: 19.6622\n",
      "Epoch 44: val_loss did not improve from 1636.52295\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 667.3894 - mse: 667.3894 - mae: 15.5005 - val_loss: 1923.5518 - val_mse: 1923.5518 - val_mae: 20.4818\n",
      "Epoch 45/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 272.1850 - mse: 272.1850 - mae: 12.8464\n",
      "Epoch 45: val_loss did not improve from 1636.52295\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 712.8007 - mse: 712.8007 - mae: 15.8078 - val_loss: 1702.4512 - val_mse: 1702.4512 - val_mae: 19.6646\n",
      "Epoch 46/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 462.1071 - mse: 462.1071 - mae: 15.2890\n",
      "Epoch 46: val_loss did not improve from 1636.52295\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 671.2571 - mse: 671.2571 - mae: 15.5592 - val_loss: 1713.4725 - val_mse: 1713.4725 - val_mae: 19.5201\n",
      "Epoch 47/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 471.9550 - mse: 471.9550 - mae: 14.5194\n",
      "Epoch 47: val_loss did not improve from 1636.52295\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 686.3755 - mse: 686.3755 - mae: 15.4902 - val_loss: 1746.8610 - val_mse: 1746.8610 - val_mae: 19.4902\n",
      "Epoch 48/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 344.6705 - mse: 344.6705 - mae: 12.2777\n",
      "Epoch 48: val_loss improved from 1636.52295 to 1490.26733, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 721.4088 - mse: 721.4088 - mae: 16.1488 - val_loss: 1490.2673 - val_mse: 1490.2673 - val_mae: 19.5046\n",
      "Epoch 49/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 416.5301 - mse: 416.5301 - mae: 15.2204\n",
      "Epoch 49: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 655.7000 - mse: 655.7000 - mae: 15.1320 - val_loss: 1927.7114 - val_mse: 1927.7114 - val_mae: 20.7180\n",
      "Epoch 50/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 818.9663 - mse: 818.9663 - mae: 16.5091\n",
      "Epoch 50: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 669.1582 - mse: 669.1582 - mae: 15.1186 - val_loss: 1712.9767 - val_mse: 1712.9767 - val_mae: 19.0899\n",
      "Epoch 51/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 605.7289 - mse: 605.7289 - mae: 15.7358\n",
      "Epoch 51: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 637.6901 - mse: 637.6901 - mae: 15.1251 - val_loss: 1569.3652 - val_mse: 1569.3655 - val_mae: 19.2734\n",
      "Epoch 52/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 194.1227 - mse: 194.1227 - mae: 10.8082\n",
      "Epoch 52: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 652.6163 - mse: 652.6163 - mae: 15.0735 - val_loss: 1735.2290 - val_mse: 1735.2290 - val_mae: 19.0918\n",
      "Epoch 53/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 532.0014 - mse: 532.0014 - mae: 14.1896\n",
      "Epoch 53: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 630.4236 - mse: 630.4236 - mae: 14.4985 - val_loss: 1556.3158 - val_mse: 1556.3158 - val_mae: 18.6389\n",
      "Epoch 54/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 382.7939 - mse: 382.7939 - mae: 14.3568\n",
      "Epoch 54: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 604.2338 - mse: 604.2338 - mae: 14.3663 - val_loss: 1624.3898 - val_mse: 1624.3899 - val_mae: 18.5479\n",
      "Epoch 55/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 133.3733 - mse: 133.3733 - mae: 8.7846\n",
      "Epoch 55: val_loss did not improve from 1490.26733\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 595.4948 - mse: 595.4948 - mae: 14.1367 - val_loss: 1647.6064 - val_mse: 1647.6064 - val_mae: 18.4866\n",
      "Epoch 56/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 395.9970 - mse: 395.9970 - mae: 14.1273\n",
      "Epoch 56: val_loss improved from 1490.26733 to 1437.43347, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 645.7905 - mse: 645.7905 - mae: 14.6198 - val_loss: 1437.4335 - val_mse: 1437.4335 - val_mae: 18.0728\n",
      "Epoch 57/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 622.4327 - mse: 622.4327 - mae: 15.7686\n",
      "Epoch 57: val_loss did not improve from 1437.43347\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 563.8421 - mse: 563.8421 - mae: 13.9045 - val_loss: 1661.3661 - val_mse: 1661.3661 - val_mae: 19.1738\n",
      "Epoch 58/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1639.8955 - mse: 1639.8955 - mae: 21.3129\n",
      "Epoch 58: val_loss did not improve from 1437.43347\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 602.6923 - mse: 602.6923 - mae: 14.1361 - val_loss: 1704.6068 - val_mse: 1704.6068 - val_mae: 18.5135\n",
      "Epoch 59/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 662.0142 - mse: 662.0142 - mae: 16.1182\n",
      "Epoch 59: val_loss did not improve from 1437.43347\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 580.4938 - mse: 580.4938 - mae: 14.0063 - val_loss: 1475.8292 - val_mse: 1475.8293 - val_mae: 18.3358\n",
      "Epoch 60/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 862.8302 - mse: 862.8302 - mae: 18.1550\n",
      "Epoch 60: val_loss did not improve from 1437.43347\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 579.0742 - mse: 579.0742 - mae: 13.6699 - val_loss: 1495.5723 - val_mse: 1495.5723 - val_mae: 17.7035\n",
      "Epoch 61/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 165.8680 - mse: 165.8680 - mae: 8.8048\n",
      "Epoch 61: val_loss improved from 1437.43347 to 1433.20923, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 558.1502 - mse: 558.1502 - mae: 13.2363 - val_loss: 1433.2092 - val_mse: 1433.2092 - val_mae: 17.7516\n",
      "Epoch 62/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 439.0149 - mse: 439.0149 - mae: 10.9575\n",
      "Epoch 62: val_loss did not improve from 1433.20923\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 552.0140 - mse: 552.0140 - mae: 13.1805 - val_loss: 1506.7838 - val_mse: 1506.7838 - val_mae: 17.6027\n",
      "Epoch 63/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 660.4188 - mse: 660.4188 - mae: 14.3906\n",
      "Epoch 63: val_loss did not improve from 1433.20923\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 568.7149 - mse: 568.7149 - mae: 13.2772 - val_loss: 1546.6282 - val_mse: 1546.6283 - val_mae: 17.5761\n",
      "Epoch 64/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 178.0907 - mse: 178.0907 - mae: 10.2560\n",
      "Epoch 64: val_loss improved from 1433.20923 to 1331.91150, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 528.0972 - mse: 528.0972 - mae: 12.8850 - val_loss: 1331.9115 - val_mse: 1331.9115 - val_mae: 16.6935\n",
      "Epoch 65/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 254.2302 - mse: 254.2302 - mae: 9.5218\n",
      "Epoch 65: val_loss did not improve from 1331.91150\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 529.0615 - mse: 529.0615 - mae: 12.5826 - val_loss: 1437.7368 - val_mse: 1437.7367 - val_mae: 17.0992\n",
      "Epoch 66/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 203.1702 - mse: 203.1702 - mae: 8.1080\n",
      "Epoch 66: val_loss did not improve from 1331.91150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 544.6303 - mse: 544.6303 - mae: 12.4228 - val_loss: 1435.0967 - val_mse: 1435.0967 - val_mae: 16.8448\n",
      "Epoch 67/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 220.2372 - mse: 220.2372 - mae: 10.4914\n",
      "Epoch 67: val_loss did not improve from 1331.91150\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 510.1911 - mse: 510.1911 - mae: 12.5971 - val_loss: 1380.0569 - val_mse: 1380.0569 - val_mae: 16.4982\n",
      "Epoch 68/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 308.7636 - mse: 308.7636 - mae: 10.7682\n",
      "Epoch 68: val_loss improved from 1331.91150 to 1330.20813, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 503.7136 - mse: 503.7136 - mae: 12.0153 - val_loss: 1330.2081 - val_mse: 1330.2081 - val_mae: 16.5164\n",
      "Epoch 69/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 356.3204 - mse: 356.3204 - mae: 8.8230\n",
      "Epoch 69: val_loss did not improve from 1330.20813\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 537.6771 - mse: 537.6771 - mae: 12.1648 - val_loss: 1371.5554 - val_mse: 1371.5555 - val_mae: 16.3626\n",
      "Epoch 70/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 300.1082 - mse: 300.1082 - mae: 9.7300\n",
      "Epoch 70: val_loss improved from 1330.20813 to 1329.51062, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 16ms/step - loss: 494.8518 - mse: 494.8518 - mae: 12.2456 - val_loss: 1329.5106 - val_mse: 1329.5106 - val_mae: 16.0832\n",
      "Epoch 71/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 133.9000 - mse: 133.9000 - mae: 8.1225\n",
      "Epoch 71: val_loss did not improve from 1329.51062\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 492.9708 - mse: 492.9708 - mae: 11.7095 - val_loss: 1410.8907 - val_mse: 1410.8907 - val_mae: 16.8661\n",
      "Epoch 72/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 959.2388 - mse: 959.2388 - mae: 17.2709\n",
      "Epoch 72: val_loss did not improve from 1329.51062\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 486.6683 - mse: 486.6683 - mae: 11.8036 - val_loss: 1414.2947 - val_mse: 1414.2947 - val_mae: 16.2772\n",
      "Epoch 73/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 657.9038 - mse: 657.9038 - mae: 12.5700\n",
      "Epoch 73: val_loss improved from 1329.51062 to 1243.30188, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 509.4797 - mse: 509.4797 - mae: 12.1499 - val_loss: 1243.3019 - val_mse: 1243.3019 - val_mae: 15.7378\n",
      "Epoch 74/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 514.0917 - mse: 514.0917 - mae: 11.1861\n",
      "Epoch 74: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 501.3572 - mse: 501.3572 - mae: 12.1518 - val_loss: 1440.0380 - val_mse: 1440.0380 - val_mae: 16.6057\n",
      "Epoch 75/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 365.8836 - mse: 365.8836 - mae: 9.0791\n",
      "Epoch 75: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 474.2523 - mse: 474.2523 - mae: 11.5579 - val_loss: 1346.6942 - val_mse: 1346.6942 - val_mae: 16.3252\n",
      "Epoch 76/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 89.8041 - mse: 89.8041 - mae: 7.9129\n",
      "Epoch 76: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 481.1179 - mse: 481.1179 - mae: 12.1002 - val_loss: 1249.3475 - val_mse: 1249.3475 - val_mae: 16.3976\n",
      "Epoch 77/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 390.8318 - mse: 390.8318 - mae: 10.7091\n",
      "Epoch 77: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 465.7115 - mse: 465.7115 - mae: 11.9474 - val_loss: 1392.7847 - val_mse: 1392.7847 - val_mae: 16.0985\n",
      "Epoch 78/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 680.2350 - mse: 680.2350 - mae: 12.9941\n",
      "Epoch 78: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 449.4062 - mse: 449.4062 - mae: 11.5266 - val_loss: 1320.6812 - val_mse: 1320.6812 - val_mae: 15.8864\n",
      "Epoch 79/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 159.5389 - mse: 159.5389 - mae: 9.6553\n",
      "Epoch 79: val_loss did not improve from 1243.30188\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 440.4483 - mse: 440.4483 - mae: 10.8547 - val_loss: 1313.3611 - val_mse: 1313.3611 - val_mae: 15.6939\n",
      "Epoch 80/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 369.1130 - mse: 369.1130 - mae: 8.6656\n",
      "Epoch 80: val_loss improved from 1243.30188 to 1212.00452, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 441.7375 - mse: 441.7375 - mae: 10.6294 - val_loss: 1212.0045 - val_mse: 1212.0045 - val_mae: 15.2570\n",
      "Epoch 81/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 317.4342 - mse: 317.4342 - mae: 11.0086\n",
      "Epoch 81: val_loss did not improve from 1212.00452\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 439.2601 - mse: 439.2601 - mae: 10.6926 - val_loss: 1322.1849 - val_mse: 1322.1849 - val_mae: 15.1582\n",
      "Epoch 82/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 354.5759 - mse: 354.5759 - mae: 9.5324\n",
      "Epoch 82: val_loss did not improve from 1212.00452\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 421.3765 - mse: 421.3765 - mae: 10.6309 - val_loss: 1288.9364 - val_mse: 1288.9364 - val_mae: 15.2526\n",
      "Epoch 83/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 224.3661 - mse: 224.3661 - mae: 9.0055\n",
      "Epoch 83: val_loss improved from 1212.00452 to 1160.37402, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 412.2805 - mse: 412.2805 - mae: 10.3159 - val_loss: 1160.3740 - val_mse: 1160.3740 - val_mae: 14.8198\n",
      "Epoch 84/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 239.4548 - mse: 239.4548 - mae: 8.3835\n",
      "Epoch 84: val_loss did not improve from 1160.37402\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 396.4001 - mse: 396.4001 - mae: 10.1791 - val_loss: 1216.7607 - val_mse: 1216.7607 - val_mae: 15.2822\n",
      "Epoch 85/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 530.8904 - mse: 530.8904 - mae: 10.7519\n",
      "Epoch 85: val_loss did not improve from 1160.37402\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 398.3225 - mse: 398.3225 - mae: 9.9190 - val_loss: 1271.2874 - val_mse: 1271.2874 - val_mae: 14.9891\n",
      "Epoch 86/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 322.4353 - mse: 322.4353 - mae: 9.7227\n",
      "Epoch 86: val_loss did not improve from 1160.37402\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 392.3065 - mse: 392.3065 - mae: 10.1429 - val_loss: 1201.4451 - val_mse: 1201.4451 - val_mae: 14.6866\n",
      "Epoch 87/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 566.0374 - mse: 566.0374 - mae: 11.1981\n",
      "Epoch 87: val_loss did not improve from 1160.37402\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 397.9595 - mse: 397.9595 - mae: 9.9709 - val_loss: 1205.1423 - val_mse: 1205.1423 - val_mae: 15.3835\n",
      "Epoch 88/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 602.3800 - mse: 602.3800 - mae: 12.9307\n",
      "Epoch 88: val_loss did not improve from 1160.37402\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 393.8238 - mse: 393.8238 - mae: 10.0595 - val_loss: 1249.1433 - val_mse: 1249.1433 - val_mae: 14.7144\n",
      "Epoch 89/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 189.3669 - mse: 189.3669 - mae: 7.8505\n",
      "Epoch 89: val_loss improved from 1160.37402 to 1124.54138, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 365.1761 - mse: 365.1761 - mae: 9.8540 - val_loss: 1124.5414 - val_mse: 1124.5414 - val_mae: 14.4855\n",
      "Epoch 90/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 179.7906 - mse: 179.7906 - mae: 8.7400\n",
      "Epoch 90: val_loss improved from 1124.54138 to 1121.82495, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 378.4485 - mse: 378.4485 - mae: 9.9306 - val_loss: 1121.8250 - val_mse: 1121.8250 - val_mae: 14.8908\n",
      "Epoch 91/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 222.1688 - mse: 222.1688 - mae: 8.3249\n",
      "Epoch 91: val_loss did not improve from 1121.82495\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 363.0981 - mse: 363.0981 - mae: 9.6686 - val_loss: 1162.0012 - val_mse: 1162.0012 - val_mae: 14.4437\n",
      "Epoch 92/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 711.7904 - mse: 711.7904 - mae: 13.4524\n",
      "Epoch 92: val_loss did not improve from 1121.82495\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 360.7064 - mse: 360.7064 - mae: 9.6042 - val_loss: 1275.7111 - val_mse: 1275.7111 - val_mae: 14.8875\n",
      "Epoch 93/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 345.3137 - mse: 345.3137 - mae: 9.2996\n",
      "Epoch 93: val_loss improved from 1121.82495 to 1113.87671, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 375.6729 - mse: 375.6729 - mae: 9.4856 - val_loss: 1113.8767 - val_mse: 1113.8767 - val_mae: 14.0872\n",
      "Epoch 94/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 41.4345 - mse: 41.4345 - mae: 4.3250\n",
      "Epoch 94: val_loss did not improve from 1113.87671\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 363.4546 - mse: 363.4546 - mae: 9.5932 - val_loss: 1171.3346 - val_mse: 1171.3346 - val_mae: 14.2671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 87.4559 - mse: 87.4559 - mae: 6.6965\n",
      "Epoch 95: val_loss improved from 1113.87671 to 1003.12024, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 343.8885 - mse: 343.8885 - mae: 9.1959 - val_loss: 1003.1202 - val_mse: 1003.1202 - val_mae: 13.6505\n",
      "Epoch 96/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 615.1545 - mse: 615.1545 - mae: 10.9736\n",
      "Epoch 96: val_loss did not improve from 1003.12024\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 349.7524 - mse: 349.7524 - mae: 9.3868 - val_loss: 1128.3928 - val_mse: 1128.3928 - val_mae: 14.0687\n",
      "Epoch 97/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 504.3402 - mse: 504.3402 - mae: 13.5327\n",
      "Epoch 97: val_loss improved from 1003.12024 to 968.36798, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 43ms/step - loss: 323.8324 - mse: 323.8324 - mae: 9.0627 - val_loss: 968.3680 - val_mse: 968.3680 - val_mae: 13.5534\n",
      "Epoch 98/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.2408 - mse: 19.2408 - mae: 3.2692\n",
      "Epoch 98: val_loss did not improve from 968.36798\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 330.6651 - mse: 330.6651 - mae: 9.1608 - val_loss: 1094.6146 - val_mse: 1094.6146 - val_mae: 14.4163\n",
      "Epoch 99/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 268.4702 - mse: 268.4702 - mae: 9.8340\n",
      "Epoch 99: val_loss did not improve from 968.36798\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 357.4506 - mse: 357.4506 - mae: 9.5342 - val_loss: 1206.8953 - val_mse: 1206.8953 - val_mae: 14.2863\n",
      "Epoch 100/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 566.3483 - mse: 566.3483 - mae: 13.1097\n",
      "Epoch 100: val_loss improved from 968.36798 to 946.31451, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 356.7271 - mse: 356.7271 - mae: 9.8929 - val_loss: 946.3145 - val_mse: 946.3145 - val_mae: 13.7099\n",
      "Epoch 101/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 598.1646 - mse: 598.1646 - mae: 13.9632\n",
      "Epoch 101: val_loss did not improve from 946.31451\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 332.1512 - mse: 332.1512 - mae: 9.7894 - val_loss: 1152.5063 - val_mse: 1152.5063 - val_mae: 14.0211\n",
      "Epoch 102/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 673.5649 - mse: 673.5649 - mae: 15.4396\n",
      "Epoch 102: val_loss did not improve from 946.31451\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 320.2748 - mse: 320.2748 - mae: 9.2504 - val_loss: 1004.8195 - val_mse: 1004.8195 - val_mae: 13.4214\n",
      "Epoch 103/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 258.8748 - mse: 258.8748 - mae: 8.7716\n",
      "Epoch 103: val_loss improved from 946.31451 to 812.34314, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 350.6085 - mse: 350.6085 - mae: 9.3758 - val_loss: 812.3431 - val_mse: 812.3431 - val_mae: 14.0323\n",
      "Epoch 104/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 537.5329 - mse: 537.5329 - mae: 12.1071\n",
      "Epoch 104: val_loss did not improve from 812.34314\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 340.2197 - mse: 340.2197 - mae: 10.1854 - val_loss: 1069.1061 - val_mse: 1069.1061 - val_mae: 13.5103\n",
      "Epoch 105/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 324.1586 - mse: 324.1586 - mae: 8.0145\n",
      "Epoch 105: val_loss did not improve from 812.34314\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 315.8122 - mse: 315.8122 - mae: 9.6413 - val_loss: 954.1246 - val_mse: 954.1246 - val_mae: 13.0957\n",
      "Epoch 106/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 135.2505 - mse: 135.2505 - mae: 7.6753\n",
      "Epoch 106: val_loss did not improve from 812.34314\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 300.6127 - mse: 300.6127 - mae: 8.8685 - val_loss: 1011.2352 - val_mse: 1011.2352 - val_mae: 13.9383\n",
      "Epoch 107/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 151.7521 - mse: 151.7521 - mae: 7.9699\n",
      "Epoch 107: val_loss did not improve from 812.34314\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 318.2688 - mse: 318.2688 - mae: 9.2165 - val_loss: 1114.0812 - val_mse: 1114.0812 - val_mae: 13.4964\n",
      "Epoch 108/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 124.3096 - mse: 124.3096 - mae: 6.7956\n",
      "Epoch 108: val_loss improved from 812.34314 to 794.70306, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 371.3261 - mse: 371.3261 - mae: 9.9413 - val_loss: 794.7031 - val_mse: 794.7031 - val_mae: 12.5685\n",
      "Epoch 109/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 607.7905 - mse: 607.7905 - mae: 10.6481\n",
      "Epoch 109: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 337.8716 - mse: 337.8716 - mae: 9.7037 - val_loss: 1036.8041 - val_mse: 1036.8041 - val_mae: 13.2983\n",
      "Epoch 110/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 62.3886 - mse: 62.3886 - mae: 4.7575\n",
      "Epoch 110: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 269.0183 - mse: 269.0183 - mae: 8.7722 - val_loss: 836.1143 - val_mse: 836.1143 - val_mae: 12.3880\n",
      "Epoch 111/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 168.0595 - mse: 168.0595 - mae: 8.1290\n",
      "Epoch 111: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.0692 - mse: 271.0692 - mae: 8.6066 - val_loss: 945.0690 - val_mse: 945.0690 - val_mae: 13.8747\n",
      "Epoch 112/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 87.4313 - mse: 87.4313 - mae: 5.9384\n",
      "Epoch 112: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.7025 - mse: 264.7025 - mae: 8.6031 - val_loss: 994.3012 - val_mse: 994.3012 - val_mae: 12.8401\n",
      "Epoch 113/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 459.7669 - mse: 459.7669 - mae: 11.1219\n",
      "Epoch 113: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.5791 - mse: 261.5791 - mae: 8.5758 - val_loss: 858.3616 - val_mse: 858.3616 - val_mae: 12.2563\n",
      "Epoch 114/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 202.0859 - mse: 202.0859 - mae: 6.9140\n",
      "Epoch 114: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 247.0087 - mse: 247.0087 - mae: 8.1796 - val_loss: 898.0797 - val_mse: 898.0797 - val_mae: 12.6480\n",
      "Epoch 115/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 445.9108 - mse: 445.9108 - mae: 11.1202\n",
      "Epoch 115: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.3852 - mse: 239.3852 - mae: 8.0052 - val_loss: 859.3226 - val_mse: 859.3226 - val_mae: 12.0886\n",
      "Epoch 116/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 415.0151 - mse: 415.0151 - mae: 11.2770\n",
      "Epoch 116: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.3344 - mse: 237.3344 - mae: 8.0609 - val_loss: 881.8814 - val_mse: 881.8814 - val_mae: 12.4343\n",
      "Epoch 117/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 83.5434 - mse: 83.5434 - mae: 5.6276\n",
      "Epoch 117: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.7473 - mse: 239.7473 - mae: 7.9691 - val_loss: 823.0018 - val_mse: 823.0018 - val_mae: 12.1723\n",
      "Epoch 118/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 214.3974 - mse: 214.3974 - mae: 8.2732\n",
      "Epoch 118: val_loss did not improve from 794.70306\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.0197 - mse: 230.0197 - mae: 7.9219 - val_loss: 987.3374 - val_mse: 987.3374 - val_mae: 12.8404\n"
     ]
    }
   ],
   "source": [
    "# callbacks=[checkpoint,earlystopping] 추가\n",
    "hist2 = model2.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val), callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNGsAhH7cStm",
    "outputId": "eee1a4ba-8815-4cee-b2ee-d75f4b00a065"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n",
      "예측값 :  [154.712]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "model2.load_weights(ckpt_name)\n",
    "preds = model2.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "kziU8_kscSml",
    "outputId": "704b1441-8d54-4629-fd3c-3911ad55364b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cdcef9b10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i for i in range(len(preds))]\n",
    "plt.plot(x_axis,preds,label='preds')\n",
    "plt.plot(x_axis,Y_val,label='labels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1az2GSnMdyK2"
   },
   "source": [
    "회귀 모델의 성능 평가를 위해 r2 score를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ClN-60-NdU3s",
    "outputId": "90a9c3c8-9ed7-41c5-9e3a-ffb05f39b2bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9557528534403033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능이 더 나빠진 것을 확인할 수 있습니다. Patience가 너무 낮아서, 학습이 충분히 되기 전에 종료 된 것 같습니다.\\\n",
    "Patience를 바꿔서 다시 실험해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 100)               400       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 300)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#model2와 동일\n",
    "model3 = tf.keras.Sequential()\n",
    "\n",
    "################# 자유롭게 MLP 모델을 구현합니다.\n",
    "model3.add(tf.keras.Input(shape = 3))\n",
    "model3.add(Dense(100))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(200))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(300))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(200))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(100))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dense(1))\n",
    "\n",
    "learning_rate = 5e-4\n",
    "optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "loss = MeanSquaredError()\n",
    "epochs = 1000\n",
    "batch_size=32\n",
    "# Early stopping : 성능 개선이 없으면 종료\n",
    "earlystopping = EarlyStopping(monitor='val_loss',  # 모니터 기준 설정 (val loss) \n",
    "                patience=100,                      # 100회 Epoch동안 개선되지 않는다면 종료\n",
    "                )\n",
    "# 체크포인트: 중간에 모델을 저장하고 싶을 때 저장하는 기능\n",
    "\n",
    "ckpt_name = 'checkpoint-epoch-{}-batch-{}-trial-001.h5'.format(epochs, batch_size)\n",
    "checkpoint = ModelCheckpoint(ckpt_name,             # file명을 지정합니다\n",
    "                            monitor='val_loss',     # val_loss 값이 개선되었을때 호출됩니다\n",
    "                            verbose=1,              # 로그를 출력합니다\n",
    "                            save_best_only=True,    # 가장 best 값만 저장합니다\n",
    "                            )\n",
    "model3.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/6 [====>.........................] - ETA: 3s - loss: 18985.0938 - mse: 18985.0938 - mae: 85.3574\n",
      "Epoch 1: val_loss improved from inf to 34012.53516, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 1s 54ms/step - loss: 23030.1133 - mse: 23030.1133 - mae: 102.0162 - val_loss: 34012.5352 - val_mse: 34012.5352 - val_mae: 126.8320\n",
      "Epoch 2/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13090.1768 - mse: 13090.1768 - mae: 90.4291\n",
      "Epoch 2: val_loss improved from 34012.53516 to 33687.69922, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 22874.4727 - mse: 22874.4727 - mae: 101.3421 - val_loss: 33687.6992 - val_mse: 33687.6992 - val_mae: 125.7770\n",
      "Epoch 3/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20428.3730 - mse: 20428.3730 - mae: 101.2233\n",
      "Epoch 3: val_loss improved from 33687.69922 to 32907.05078, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 22514.8105 - mse: 22514.8105 - mae: 99.8014 - val_loss: 32907.0508 - val_mse: 32907.0508 - val_mae: 123.1865\n",
      "Epoch 4/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34527.5781 - mse: 34527.5781 - mae: 121.2640\n",
      "Epoch 4: val_loss improved from 32907.05078 to 31099.00586, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 21652.8145 - mse: 21652.8145 - mae: 96.2628 - val_loss: 31099.0059 - val_mse: 31099.0059 - val_mae: 117.2070\n",
      "Epoch 5/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9922.3027 - mse: 9922.3027 - mae: 75.7594\n",
      "Epoch 5: val_loss improved from 31099.00586 to 27235.77734, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 19714.7383 - mse: 19714.7383 - mae: 88.4310 - val_loss: 27235.7773 - val_mse: 27235.7773 - val_mae: 105.0200\n",
      "Epoch 6/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15942.8232 - mse: 15942.8232 - mae: 60.9924\n",
      "Epoch 6: val_loss improved from 27235.77734 to 20380.72656, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 15906.2588 - mse: 15906.2588 - mae: 74.6561 - val_loss: 20380.7266 - val_mse: 20380.7266 - val_mae: 81.3941\n",
      "Epoch 7/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23942.8262 - mse: 23942.8262 - mae: 91.7987\n",
      "Epoch 7: val_loss improved from 20380.72656 to 12280.16699, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 11305.8008 - mse: 11305.8008 - mae: 63.5153 - val_loss: 12280.1670 - val_mse: 12280.1670 - val_mae: 61.4341\n",
      "Epoch 8/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8176.9863 - mse: 8176.9863 - mae: 60.1893\n",
      "Epoch 8: val_loss improved from 12280.16699 to 8241.16016, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 6908.6577 - mse: 6908.6577 - mae: 59.6747 - val_loss: 8241.1602 - val_mse: 8241.1602 - val_mae: 60.2540\n",
      "Epoch 9/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7537.1978 - mse: 7537.1978 - mae: 63.4037\n",
      "Epoch 9: val_loss improved from 8241.16016 to 6489.64600, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 5983.6597 - mse: 5983.6597 - mae: 62.5241 - val_loss: 6489.6460 - val_mse: 6489.6460 - val_mae: 54.4200\n",
      "Epoch 10/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3772.4473 - mse: 3772.4473 - mae: 54.5320\n",
      "Epoch 10: val_loss improved from 6489.64600 to 5707.40771, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3963.4670 - mse: 3963.4670 - mae: 46.4922 - val_loss: 5707.4077 - val_mse: 5707.4077 - val_mae: 39.9771\n",
      "Epoch 11/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3795.6226 - mse: 3795.6226 - mae: 40.0675\n",
      "Epoch 11: val_loss improved from 5707.40771 to 4916.66064, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2879.2349 - mse: 2879.2349 - mae: 33.1852 - val_loss: 4916.6606 - val_mse: 4916.6606 - val_mae: 35.0213\n",
      "Epoch 12/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4596.1143 - mse: 4596.1143 - mae: 39.9690\n",
      "Epoch 12: val_loss improved from 4916.66064 to 3917.90454, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2335.4158 - mse: 2335.4158 - mae: 29.8996 - val_loss: 3917.9045 - val_mse: 3917.9045 - val_mae: 34.3706\n",
      "Epoch 13/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3072.0825 - mse: 3072.0825 - mae: 31.1618\n",
      "Epoch 13: val_loss improved from 3917.90454 to 3504.85449, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1884.1447 - mse: 1884.1447 - mae: 28.7952 - val_loss: 3504.8545 - val_mse: 3504.8550 - val_mae: 35.1749\n",
      "Epoch 14/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1295.2612 - mse: 1295.2612 - mae: 27.8056\n",
      "Epoch 14: val_loss improved from 3504.85449 to 3386.17993, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1638.8628 - mse: 1638.8628 - mae: 25.8349 - val_loss: 3386.1799 - val_mse: 3386.1799 - val_mae: 30.2862\n",
      "Epoch 15/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1471.4072 - mse: 1471.4072 - mae: 23.3652\n",
      "Epoch 15: val_loss improved from 3386.17993 to 3213.16553, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1514.5118 - mse: 1514.5118 - mae: 23.6804 - val_loss: 3213.1655 - val_mse: 3213.1653 - val_mae: 28.7492\n",
      "Epoch 16/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2308.5051 - mse: 2308.5051 - mae: 24.7643\n",
      "Epoch 16: val_loss improved from 3213.16553 to 2906.66162, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1423.9979 - mse: 1423.9979 - mae: 23.7352 - val_loss: 2906.6616 - val_mse: 2906.6616 - val_mae: 30.4500\n",
      "Epoch 17/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3876.3059 - mse: 3876.3059 - mae: 38.5637\n",
      "Epoch 17: val_loss improved from 2906.66162 to 2804.09497, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1376.1685 - mse: 1376.1685 - mae: 24.2836 - val_loss: 2804.0950 - val_mse: 2804.0950 - val_mae: 29.2886\n",
      "Epoch 18/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 544.9358 - mse: 544.9358 - mae: 17.6706\n",
      "Epoch 18: val_loss did not improve from 2804.09497\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1295.2361 - mse: 1295.2361 - mae: 22.2893 - val_loss: 2940.3074 - val_mse: 2940.3069 - val_mae: 26.0761\n",
      "Epoch 19/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1028.8331 - mse: 1028.8331 - mae: 20.8839\n",
      "Epoch 19: val_loss improved from 2804.09497 to 2709.63965, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1256.5863 - mse: 1256.5863 - mae: 21.8495 - val_loss: 2709.6396 - val_mse: 2709.6396 - val_mae: 27.2159\n",
      "Epoch 20/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2038.1971 - mse: 2038.1971 - mae: 25.3714\n",
      "Epoch 20: val_loss improved from 2709.63965 to 2494.06152, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1236.2209 - mse: 1236.2209 - mae: 23.0955 - val_loss: 2494.0615 - val_mse: 2494.0615 - val_mae: 28.8948\n",
      "Epoch 21/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 999.2991 - mse: 999.2991 - mae: 23.6506\n",
      "Epoch 21: val_loss did not improve from 2494.06152\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1251.0745 - mse: 1251.0745 - mae: 22.9299 - val_loss: 2617.1428 - val_mse: 2617.1428 - val_mae: 25.8654\n",
      "Epoch 22/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 2360.7793 - mse: 2360.7793 - mae: 27.3492\n",
      "Epoch 22: val_loss improved from 2494.06152 to 2348.40894, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1226.0514 - mse: 1226.0514 - mae: 22.9171 - val_loss: 2348.4089 - val_mse: 2348.4089 - val_mae: 27.9002\n",
      "Epoch 23/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1599.3701 - mse: 1599.3701 - mae: 30.4190\n",
      "Epoch 23: val_loss did not improve from 2348.40894\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1104.2113 - mse: 1104.2113 - mae: 22.1001 - val_loss: 2433.0112 - val_mse: 2433.0112 - val_mae: 25.4044\n",
      "Epoch 24/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 379.4289 - mse: 379.4289 - mae: 12.7353\n",
      "Epoch 24: val_loss did not improve from 2348.40894\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1101.1060 - mse: 1101.1060 - mae: 20.7445 - val_loss: 2459.9246 - val_mse: 2459.9246 - val_mae: 24.5024\n",
      "Epoch 25/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 473.4708 - mse: 473.4708 - mae: 16.1435\n",
      "Epoch 25: val_loss did not improve from 2348.40894\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1107.0275 - mse: 1107.0275 - mae: 20.0366 - val_loss: 2503.3579 - val_mse: 2503.3579 - val_mae: 23.8672\n",
      "Epoch 26/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1015.1819 - mse: 1015.1819 - mae: 20.2422\n",
      "Epoch 26: val_loss improved from 2348.40894 to 2127.30054, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1005.7720 - mse: 1005.7720 - mae: 20.3984 - val_loss: 2127.3005 - val_mse: 2127.3005 - val_mae: 26.7798\n",
      "Epoch 27/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 850.6741 - mse: 850.6741 - mae: 20.8859\n",
      "Epoch 27: val_loss improved from 2127.30054 to 2106.63867, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1059.8081 - mse: 1059.8081 - mae: 22.0688 - val_loss: 2106.6387 - val_mse: 2106.6387 - val_mae: 25.3615\n",
      "Epoch 28/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1024.1814 - mse: 1024.1814 - mae: 18.8253\n",
      "Epoch 28: val_loss did not improve from 2106.63867\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 956.3472 - mse: 956.3472 - mae: 19.8460 - val_loss: 2372.1042 - val_mse: 2372.1042 - val_mae: 23.3373\n",
      "Epoch 29/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1526.0573 - mse: 1526.0573 - mae: 22.6077\n",
      "Epoch 29: val_loss did not improve from 2106.63867\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 966.7122 - mse: 966.7122 - mae: 18.7551 - val_loss: 2227.9265 - val_mse: 2227.9265 - val_mae: 23.1773\n",
      "Epoch 30/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 412.3121 - mse: 412.3121 - mae: 14.7812\n",
      "Epoch 30: val_loss improved from 2106.63867 to 2082.28613, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 39ms/step - loss: 912.5463 - mse: 912.5463 - mae: 18.8441 - val_loss: 2082.2861 - val_mse: 2082.2861 - val_mae: 23.3721\n",
      "Epoch 31/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1800.1957 - mse: 1800.1957 - mae: 26.6852\n",
      "Epoch 31: val_loss improved from 2082.28613 to 2059.64014, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 899.6505 - mse: 899.6505 - mae: 19.0025 - val_loss: 2059.6401 - val_mse: 2059.6401 - val_mae: 22.9256\n",
      "Epoch 32/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1282.2788 - mse: 1282.2788 - mae: 19.8229\n",
      "Epoch 32: val_loss did not improve from 2059.64014\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 878.0859 - mse: 878.0859 - mae: 18.6552 - val_loss: 2065.3564 - val_mse: 2065.3564 - val_mae: 22.5345\n",
      "Epoch 33/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 597.9950 - mse: 597.9950 - mae: 15.2772\n",
      "Epoch 33: val_loss improved from 2059.64014 to 1985.93298, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 870.2684 - mse: 870.2684 - mae: 18.4787 - val_loss: 1985.9330 - val_mse: 1985.9330 - val_mae: 22.5066\n",
      "Epoch 34/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 648.6689 - mse: 648.6689 - mae: 18.2245\n",
      "Epoch 34: val_loss did not improve from 1985.93298\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 868.8148 - mse: 868.8148 - mae: 17.8525 - val_loss: 2118.9487 - val_mse: 2118.9487 - val_mae: 21.8791\n",
      "Epoch 35/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1482.4338 - mse: 1482.4338 - mae: 19.4645\n",
      "Epoch 35: val_loss improved from 1985.93298 to 1883.51819, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 829.1017 - mse: 829.1017 - mae: 17.8916 - val_loss: 1883.5182 - val_mse: 1883.5182 - val_mae: 21.9490\n",
      "Epoch 36/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 664.5043 - mse: 664.5043 - mae: 19.3326\n",
      "Epoch 36: val_loss did not improve from 1883.51819\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 811.7419 - mse: 811.7419 - mae: 17.8890 - val_loss: 1886.6777 - val_mse: 1886.6777 - val_mae: 21.6789\n",
      "Epoch 37/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1229.6552 - mse: 1229.6552 - mae: 21.9203\n",
      "Epoch 37: val_loss did not improve from 1883.51819\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 794.1935 - mse: 794.1935 - mae: 17.4061 - val_loss: 1949.6292 - val_mse: 1949.6292 - val_mae: 21.3396\n",
      "Epoch 38/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 695.3850 - mse: 695.3850 - mae: 16.7922\n",
      "Epoch 38: val_loss improved from 1883.51819 to 1874.06494, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 771.3260 - mse: 771.3260 - mae: 17.0025 - val_loss: 1874.0649 - val_mse: 1874.0649 - val_mae: 21.0235\n",
      "Epoch 39/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 906.2502 - mse: 906.2502 - mae: 16.6518\n",
      "Epoch 39: val_loss improved from 1874.06494 to 1812.29407, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 782.7537 - mse: 782.7537 - mae: 17.2624 - val_loss: 1812.2941 - val_mse: 1812.2941 - val_mae: 20.8070\n",
      "Epoch 40/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1178.1692 - mse: 1178.1692 - mae: 19.3515\n",
      "Epoch 40: val_loss did not improve from 1812.29407\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 736.8411 - mse: 736.8411 - mae: 16.5206 - val_loss: 1998.8027 - val_mse: 1998.8027 - val_mae: 20.5506\n",
      "Epoch 41/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 638.4523 - mse: 638.4523 - mae: 16.7837\n",
      "Epoch 41: val_loss improved from 1812.29407 to 1769.69189, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 760.9525 - mse: 760.9525 - mae: 16.6402 - val_loss: 1769.6919 - val_mse: 1769.6919 - val_mae: 20.3871\n",
      "Epoch 42/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 464.4701 - mse: 464.4701 - mae: 16.2649\n",
      "Epoch 42: val_loss did not improve from 1769.69189\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 718.4929 - mse: 718.4929 - mae: 16.4736 - val_loss: 1789.2146 - val_mse: 1789.2146 - val_mae: 20.2141\n",
      "Epoch 43/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1029.4739 - mse: 1029.4739 - mae: 18.2819\n",
      "Epoch 43: val_loss did not improve from 1769.69189\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 711.8830 - mse: 711.8830 - mae: 15.9000 - val_loss: 1920.9659 - val_mse: 1920.9659 - val_mae: 20.2298\n",
      "Epoch 44/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 434.7393 - mse: 434.7393 - mae: 13.6927\n",
      "Epoch 44: val_loss did not improve from 1769.69189\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 704.6566 - mse: 704.6566 - mae: 15.6599 - val_loss: 1793.0132 - val_mse: 1793.0132 - val_mae: 19.7015\n",
      "Epoch 45/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 259.4907 - mse: 259.4907 - mae: 11.6789\n",
      "Epoch 45: val_loss improved from 1769.69189 to 1569.59009, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 704.5167 - mse: 704.5167 - mae: 16.1743 - val_loss: 1569.5901 - val_mse: 1569.5901 - val_mae: 19.8492\n",
      "Epoch 46/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 683.9653 - mse: 683.9653 - mae: 16.3832\n",
      "Epoch 46: val_loss did not improve from 1569.59009\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 686.8894 - mse: 686.8894 - mae: 15.7918 - val_loss: 1665.4058 - val_mse: 1665.4058 - val_mae: 19.4738\n",
      "Epoch 47/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1001.2663 - mse: 1001.2663 - mae: 15.9375\n",
      "Epoch 47: val_loss did not improve from 1569.59009\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 691.4296 - mse: 691.4296 - mae: 15.5302 - val_loss: 1693.7297 - val_mse: 1693.7297 - val_mae: 19.4359\n",
      "Epoch 48/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 561.0377 - mse: 561.0377 - mae: 14.3009\n",
      "Epoch 48: val_loss did not improve from 1569.59009\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 657.0726 - mse: 657.0726 - mae: 14.8074 - val_loss: 1763.5530 - val_mse: 1763.5530 - val_mae: 19.3994\n",
      "Epoch 49/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 461.6199 - mse: 461.6199 - mae: 13.3564\n",
      "Epoch 49: val_loss did not improve from 1569.59009\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 646.7805 - mse: 646.7805 - mae: 14.7942 - val_loss: 1677.8409 - val_mse: 1677.8409 - val_mae: 19.1449\n",
      "Epoch 50/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 437.6687 - mse: 437.6687 - mae: 15.3366\n",
      "Epoch 50: val_loss improved from 1569.59009 to 1545.92480, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 648.3863 - mse: 648.3863 - mae: 15.1319 - val_loss: 1545.9248 - val_mse: 1545.9248 - val_mae: 18.7128\n",
      "Epoch 51/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1220.6831 - mse: 1220.6831 - mae: 19.9224\n",
      "Epoch 51: val_loss did not improve from 1545.92480\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 627.5046 - mse: 627.5046 - mae: 14.5411 - val_loss: 1600.0283 - val_mse: 1600.0283 - val_mae: 18.6730\n",
      "Epoch 52/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 376.5657 - mse: 376.5657 - mae: 12.7606\n",
      "Epoch 52: val_loss did not improve from 1545.92480\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 635.1810 - mse: 635.1810 - mae: 14.3474 - val_loss: 1595.1274 - val_mse: 1595.1272 - val_mae: 18.4555\n",
      "Epoch 53/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 742.1454 - mse: 742.1454 - mae: 12.1519\n",
      "Epoch 53: val_loss did not improve from 1545.92480\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 617.8816 - mse: 617.8816 - mae: 14.2761 - val_loss: 1606.0839 - val_mse: 1606.0839 - val_mae: 18.4982\n",
      "Epoch 54/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 450.0911 - mse: 450.0911 - mae: 11.5906\n",
      "Epoch 54: val_loss did not improve from 1545.92480\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 630.7161 - mse: 630.7161 - mae: 13.7775 - val_loss: 1778.1213 - val_mse: 1778.1213 - val_mae: 18.7485\n",
      "Epoch 55/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 264.1313 - mse: 264.1313 - mae: 10.9489\n",
      "Epoch 55: val_loss improved from 1545.92480 to 1478.39099, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 598.0471 - mse: 598.0471 - mae: 13.3727 - val_loss: 1478.3910 - val_mse: 1478.3910 - val_mae: 17.6941\n",
      "Epoch 56/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1081.3610 - mse: 1081.3610 - mae: 16.3745\n",
      "Epoch 56: val_loss improved from 1478.39099 to 1459.25562, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 588.5743 - mse: 588.5743 - mae: 13.5200 - val_loss: 1459.2556 - val_mse: 1459.2554 - val_mae: 17.3647\n",
      "Epoch 57/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 969.1042 - mse: 969.1042 - mae: 15.6697\n",
      "Epoch 57: val_loss did not improve from 1459.25562\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 575.3176 - mse: 575.3176 - mae: 13.0901 - val_loss: 1584.2000 - val_mse: 1584.2000 - val_mae: 17.7909\n",
      "Epoch 58/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 390.9126 - mse: 390.9126 - mae: 14.1219\n",
      "Epoch 58: val_loss did not improve from 1459.25562\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 575.5907 - mse: 575.5907 - mae: 13.0460 - val_loss: 1526.2015 - val_mse: 1526.2018 - val_mae: 17.6091\n",
      "Epoch 59/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 240.4141 - mse: 240.4141 - mae: 8.9797\n",
      "Epoch 59: val_loss did not improve from 1459.25562\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 572.9803 - mse: 572.9803 - mae: 12.9126 - val_loss: 1573.2413 - val_mse: 1573.2413 - val_mae: 17.3525\n",
      "Epoch 60/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 256.5307 - mse: 256.5307 - mae: 9.9977\n",
      "Epoch 60: val_loss improved from 1459.25562 to 1308.15808, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 563.0043 - mse: 563.0043 - mae: 12.8186 - val_loss: 1308.1581 - val_mse: 1308.1581 - val_mae: 16.3540\n",
      "Epoch 61/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 403.0475 - mse: 403.0475 - mae: 11.0348\n",
      "Epoch 61: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 550.9969 - mse: 550.9969 - mae: 12.4039 - val_loss: 1572.9010 - val_mse: 1572.9010 - val_mae: 17.5928\n",
      "Epoch 62/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 574.8984 - mse: 574.8984 - mae: 11.7387\n",
      "Epoch 62: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 554.8384 - mse: 554.8384 - mae: 12.3464 - val_loss: 1440.3623 - val_mse: 1440.3623 - val_mae: 16.9321\n",
      "Epoch 63/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 621.7482 - mse: 621.7482 - mae: 11.8064\n",
      "Epoch 63: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 536.6523 - mse: 536.6523 - mae: 12.4530 - val_loss: 1549.2339 - val_mse: 1549.2339 - val_mae: 16.8837\n",
      "Epoch 64/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 308.7512 - mse: 308.7512 - mae: 10.9597\n",
      "Epoch 64: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 535.1068 - mse: 535.1068 - mae: 11.8479 - val_loss: 1473.0809 - val_mse: 1473.0809 - val_mae: 16.4330\n",
      "Epoch 65/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 488.7001 - mse: 488.7001 - mae: 9.7730\n",
      "Epoch 65: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 580.8167 - mse: 580.8167 - mae: 12.5400 - val_loss: 1349.5056 - val_mse: 1349.5056 - val_mae: 16.2234\n",
      "Epoch 66/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 463.7302 - mse: 463.7302 - mae: 12.7140\n",
      "Epoch 66: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 516.1017 - mse: 516.1017 - mae: 11.8068 - val_loss: 1524.2792 - val_mse: 1524.2791 - val_mae: 16.9356\n",
      "Epoch 67/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 453.9013 - mse: 453.9013 - mae: 12.6928\n",
      "Epoch 67: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 512.0062 - mse: 512.0062 - mae: 12.0743 - val_loss: 1510.9486 - val_mse: 1510.9485 - val_mae: 16.8316\n",
      "Epoch 68/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 505.9305 - mse: 505.9305 - mae: 12.1566\n",
      "Epoch 68: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 503.2741 - mse: 503.2741 - mae: 11.5814 - val_loss: 1371.9388 - val_mse: 1371.9388 - val_mae: 16.1168\n",
      "Epoch 69/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 457.7504 - mse: 457.7504 - mae: 10.7268\n",
      "Epoch 69: val_loss did not improve from 1308.15808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 510.8330 - mse: 510.8330 - mae: 11.7749 - val_loss: 1310.7866 - val_mse: 1310.7866 - val_mae: 15.9623\n",
      "Epoch 70/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 304.4734 - mse: 304.4734 - mae: 8.9513\n",
      "Epoch 70: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 526.7774 - mse: 526.7774 - mae: 11.4444 - val_loss: 1525.0016 - val_mse: 1525.0016 - val_mae: 16.6035\n",
      "Epoch 71/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 243.8982 - mse: 243.8982 - mae: 7.9101\n",
      "Epoch 71: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 491.1684 - mse: 491.1684 - mae: 11.7237 - val_loss: 1321.1995 - val_mse: 1321.1995 - val_mae: 15.8007\n",
      "Epoch 72/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 183.4498 - mse: 183.4498 - mae: 8.5447\n",
      "Epoch 72: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 470.4928 - mse: 470.4928 - mae: 10.9039 - val_loss: 1379.3707 - val_mse: 1379.3707 - val_mae: 16.2695\n",
      "Epoch 73/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 498.1380 - mse: 498.1380 - mae: 10.8726\n",
      "Epoch 73: val_loss did not improve from 1308.15808\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 472.6460 - mse: 472.6460 - mae: 10.8662 - val_loss: 1342.0242 - val_mse: 1342.0242 - val_mae: 15.6750\n",
      "Epoch 74/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 109.7779 - mse: 109.7779 - mae: 6.4928\n",
      "Epoch 74: val_loss improved from 1308.15808 to 1210.28564, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 558.7490 - mse: 558.7490 - mae: 12.2633 - val_loss: 1210.2856 - val_mse: 1210.2856 - val_mae: 15.4661\n",
      "Epoch 75/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1116.0615 - mse: 1116.0615 - mae: 17.4084\n",
      "Epoch 75: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 559.6816 - mse: 559.6816 - mae: 12.0633 - val_loss: 1628.6498 - val_mse: 1628.6498 - val_mae: 17.1859\n",
      "Epoch 76/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 554.8933 - mse: 554.8933 - mae: 12.7060\n",
      "Epoch 76: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 501.1702 - mse: 501.1702 - mae: 11.6645 - val_loss: 1224.1470 - val_mse: 1224.1470 - val_mae: 16.3094\n",
      "Epoch 77/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 153.1979 - mse: 153.1979 - mae: 8.5849\n",
      "Epoch 77: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 458.7834 - mse: 458.7834 - mae: 10.7688 - val_loss: 1435.4388 - val_mse: 1435.4387 - val_mae: 16.8162\n",
      "Epoch 78/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 498.9597 - mse: 498.9597 - mae: 11.2205\n",
      "Epoch 78: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 457.8239 - mse: 457.8239 - mae: 11.1485 - val_loss: 1352.0192 - val_mse: 1352.0192 - val_mae: 15.8268\n",
      "Epoch 79/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 154.9692 - mse: 154.9692 - mae: 6.9478\n",
      "Epoch 79: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 443.0714 - mse: 443.0714 - mae: 10.7361 - val_loss: 1224.2123 - val_mse: 1224.2123 - val_mae: 15.1953\n",
      "Epoch 80/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 159.9768 - mse: 159.9768 - mae: 8.4123\n",
      "Epoch 80: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 441.5649 - mse: 441.5649 - mae: 10.6353 - val_loss: 1390.8118 - val_mse: 1390.8118 - val_mae: 15.7258\n",
      "Epoch 81/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 44.5924 - mse: 44.5924 - mae: 4.6702\n",
      "Epoch 81: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 441.0177 - mse: 441.0177 - mae: 10.3962 - val_loss: 1345.6290 - val_mse: 1345.6290 - val_mae: 15.2470\n",
      "Epoch 82/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 591.3680 - mse: 591.3680 - mae: 10.7101\n",
      "Epoch 82: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 428.3488 - mse: 428.3488 - mae: 10.4157 - val_loss: 1237.5505 - val_mse: 1237.5505 - val_mae: 15.0440\n",
      "Epoch 83/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 450.9719 - mse: 450.9719 - mae: 8.6912\n",
      "Epoch 83: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 415.0773 - mse: 415.0773 - mae: 10.2043 - val_loss: 1288.7972 - val_mse: 1288.7972 - val_mae: 15.5059\n",
      "Epoch 84/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 352.1870 - mse: 352.1870 - mae: 11.8242\n",
      "Epoch 84: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 434.5698 - mse: 434.5698 - mae: 10.1378 - val_loss: 1369.6375 - val_mse: 1369.6373 - val_mae: 15.4092\n",
      "Epoch 85/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 43.5420 - mse: 43.5420 - mae: 4.9024\n",
      "Epoch 85: val_loss did not improve from 1210.28564\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 427.4095 - mse: 427.4095 - mae: 10.1938 - val_loss: 1310.3655 - val_mse: 1310.3655 - val_mae: 15.1728\n",
      "Epoch 86/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 280.5388 - mse: 280.5388 - mae: 9.8421\n",
      "Epoch 86: val_loss improved from 1210.28564 to 1082.16931, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 421.2468 - mse: 421.2468 - mae: 10.2257 - val_loss: 1082.1693 - val_mse: 1082.1693 - val_mae: 14.7251\n",
      "Epoch 87/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 520.5740 - mse: 520.5740 - mae: 12.9913\n",
      "Epoch 87: val_loss did not improve from 1082.16931\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 439.2947 - mse: 439.2947 - mae: 10.7463 - val_loss: 1286.7900 - val_mse: 1286.7900 - val_mae: 15.5730\n",
      "Epoch 88/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 208.8943 - mse: 208.8943 - mae: 8.3736\n",
      "Epoch 88: val_loss did not improve from 1082.16931\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 394.3112 - mse: 394.3112 - mae: 9.9137 - val_loss: 1180.8755 - val_mse: 1180.8755 - val_mae: 15.0053\n",
      "Epoch 89/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 141.8863 - mse: 141.8863 - mae: 7.9043\n",
      "Epoch 89: val_loss improved from 1082.16931 to 1073.43054, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 398.7150 - mse: 398.7150 - mae: 10.2753 - val_loss: 1073.4305 - val_mse: 1073.4305 - val_mae: 14.7306\n",
      "Epoch 90/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 564.3725 - mse: 564.3725 - mae: 13.4152\n",
      "Epoch 90: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 391.5450 - mse: 391.5450 - mae: 10.3812 - val_loss: 1303.7495 - val_mse: 1303.7496 - val_mae: 15.7546\n",
      "Epoch 91/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 632.5968 - mse: 632.5968 - mae: 12.1082\n",
      "Epoch 91: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 385.5859 - mse: 385.5859 - mae: 9.5922 - val_loss: 1256.4033 - val_mse: 1256.4033 - val_mae: 15.0870\n",
      "Epoch 92/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1071.1003 - mse: 1071.1003 - mae: 16.5945\n",
      "Epoch 92: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 412.9138 - mse: 412.9138 - mae: 10.4006 - val_loss: 1092.1527 - val_mse: 1092.1527 - val_mae: 14.6373\n",
      "Epoch 93/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 741.6085 - mse: 741.6085 - mae: 14.7866\n",
      "Epoch 93: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 414.3293 - mse: 414.3293 - mae: 10.5587 - val_loss: 1249.5692 - val_mse: 1249.5692 - val_mae: 15.1590\n",
      "Epoch 94/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 149.0097 - mse: 149.0097 - mae: 7.7310\n",
      "Epoch 94: val_loss did not improve from 1073.43054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 372.5665 - mse: 372.5665 - mae: 9.7697 - val_loss: 1074.8452 - val_mse: 1074.8452 - val_mae: 14.5780\n",
      "Epoch 95/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 186.6282 - mse: 186.6282 - mae: 8.8305\n",
      "Epoch 95: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 365.4764 - mse: 365.4764 - mae: 9.5728 - val_loss: 1248.8547 - val_mse: 1248.8547 - val_mae: 15.9315\n",
      "Epoch 96/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 97.5016 - mse: 97.5016 - mae: 6.4531\n",
      "Epoch 96: val_loss did not improve from 1073.43054\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 359.0335 - mse: 359.0335 - mae: 9.8058 - val_loss: 1124.2285 - val_mse: 1124.2285 - val_mae: 14.5030\n",
      "Epoch 97/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 370.1589 - mse: 370.1589 - mae: 8.5689\n",
      "Epoch 97: val_loss improved from 1073.43054 to 1066.32019, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 348.1636 - mse: 348.1636 - mae: 9.3879 - val_loss: 1066.3202 - val_mse: 1066.3202 - val_mae: 14.3405\n",
      "Epoch 98/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 104.3301 - mse: 104.3301 - mae: 5.9690\n",
      "Epoch 98: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 350.2651 - mse: 350.2651 - mae: 9.2980 - val_loss: 1179.1143 - val_mse: 1179.1143 - val_mae: 15.2403\n",
      "Epoch 99/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 368.0329 - mse: 368.0329 - mae: 9.1547\n",
      "Epoch 99: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 342.7280 - mse: 342.7280 - mae: 9.2593 - val_loss: 1151.7336 - val_mse: 1151.7336 - val_mae: 14.4777\n",
      "Epoch 100/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 229.2460 - mse: 229.2460 - mae: 8.1795\n",
      "Epoch 100: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 344.4873 - mse: 344.4873 - mae: 9.1617 - val_loss: 1179.7479 - val_mse: 1179.7479 - val_mae: 14.4117\n",
      "Epoch 101/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 402.7331 - mse: 402.7331 - mae: 7.9983\n",
      "Epoch 101: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 337.6020 - mse: 337.6020 - mae: 9.1557 - val_loss: 1098.6780 - val_mse: 1098.6780 - val_mae: 14.3788\n",
      "Epoch 102/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 418.6356 - mse: 418.6356 - mae: 9.9316\n",
      "Epoch 102: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 355.5133 - mse: 355.5133 - mae: 9.1761 - val_loss: 1072.6874 - val_mse: 1072.6874 - val_mae: 14.1645\n",
      "Epoch 103/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 365.5204 - mse: 365.5204 - mae: 8.8173\n",
      "Epoch 103: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.6655 - mse: 310.6655 - mae: 8.6908 - val_loss: 1177.3705 - val_mse: 1177.3705 - val_mae: 14.4911\n",
      "Epoch 104/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 600.1287 - mse: 600.1287 - mae: 11.2483\n",
      "Epoch 104: val_loss did not improve from 1066.32019\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 319.0070 - mse: 319.0070 - mae: 8.7907 - val_loss: 1139.9253 - val_mse: 1139.9253 - val_mae: 14.2367\n",
      "Epoch 105/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 70.4124 - mse: 70.4124 - mae: 5.3783\n",
      "Epoch 105: val_loss improved from 1066.32019 to 977.51276, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 313.2194 - mse: 313.2194 - mae: 8.8020 - val_loss: 977.5128 - val_mse: 977.5128 - val_mae: 13.8038\n",
      "Epoch 106/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 406.5565 - mse: 406.5565 - mae: 8.6798\n",
      "Epoch 106: val_loss did not improve from 977.51276\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 325.3652 - mse: 325.3652 - mae: 9.4862 - val_loss: 1143.1289 - val_mse: 1143.1289 - val_mae: 14.2669\n",
      "Epoch 107/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 281.0659 - mse: 281.0659 - mae: 7.8046\n",
      "Epoch 107: val_loss did not improve from 977.51276\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 304.8919 - mse: 304.8919 - mae: 9.0091 - val_loss: 1073.8164 - val_mse: 1073.8164 - val_mae: 13.9433\n",
      "Epoch 108/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 403.3115 - mse: 403.3115 - mae: 9.2388\n",
      "Epoch 108: val_loss did not improve from 977.51276\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 297.2159 - mse: 297.2159 - mae: 8.9716 - val_loss: 1009.2446 - val_mse: 1009.2446 - val_mae: 14.1676\n",
      "Epoch 109/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 435.9317 - mse: 435.9317 - mae: 11.1297\n",
      "Epoch 109: val_loss improved from 977.51276 to 946.42438, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 289.7811 - mse: 289.7811 - mae: 8.9198 - val_loss: 946.4244 - val_mse: 946.4244 - val_mae: 13.3491\n",
      "Epoch 110/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 610.0827 - mse: 610.0827 - mae: 13.6668\n",
      "Epoch 110: val_loss did not improve from 946.42438\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 290.4432 - mse: 290.4432 - mae: 8.6873 - val_loss: 1025.9611 - val_mse: 1025.9611 - val_mae: 13.7751\n",
      "Epoch 111/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 485.2670 - mse: 485.2670 - mae: 11.0230\n",
      "Epoch 111: val_loss did not improve from 946.42438\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 332.3067 - mse: 332.3067 - mae: 8.6010 - val_loss: 1123.5229 - val_mse: 1123.5229 - val_mae: 14.1512\n",
      "Epoch 112/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 176.6480 - mse: 176.6480 - mae: 7.8238\n",
      "Epoch 112: val_loss improved from 946.42438 to 830.61078, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 311.9697 - mse: 311.9697 - mae: 9.2084 - val_loss: 830.6108 - val_mse: 830.6108 - val_mae: 12.9062\n",
      "Epoch 113/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 347.8486 - mse: 347.8486 - mae: 9.2075\n",
      "Epoch 113: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 282.9174 - mse: 282.9174 - mae: 8.8997 - val_loss: 1055.1433 - val_mse: 1055.1433 - val_mae: 14.3857\n",
      "Epoch 114/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 155.9654 - mse: 155.9654 - mae: 7.8059\n",
      "Epoch 114: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 281.9609 - mse: 281.9609 - mae: 8.5565 - val_loss: 958.7018 - val_mse: 958.7018 - val_mae: 13.2261\n",
      "Epoch 115/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 287.6875 - mse: 287.6875 - mae: 7.8101\n",
      "Epoch 115: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.9282 - mse: 271.9282 - mae: 8.4550 - val_loss: 1025.2598 - val_mse: 1025.2598 - val_mae: 13.7165\n",
      "Epoch 116/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 157.5674 - mse: 157.5674 - mae: 7.6809\n",
      "Epoch 116: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 278.7019 - mse: 278.7019 - mae: 8.4589 - val_loss: 876.6417 - val_mse: 876.6417 - val_mae: 12.7912\n",
      "Epoch 117/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 231.7098 - mse: 231.7098 - mae: 8.4809\n",
      "Epoch 117: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 302.2654 - mse: 302.2654 - mae: 8.5313 - val_loss: 1139.6228 - val_mse: 1139.6228 - val_mae: 13.9165\n",
      "Epoch 118/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 64.6376 - mse: 64.6376 - mae: 4.6725\n",
      "Epoch 118: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 253.3832 - mse: 253.3832 - mae: 9.0086 - val_loss: 836.7506 - val_mse: 836.7506 - val_mae: 12.6183\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 356.4194 - mse: 356.4194 - mae: 13.4199\n",
      "Epoch 119: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.7943 - mse: 261.7943 - mae: 8.4041 - val_loss: 969.7925 - val_mse: 969.7925 - val_mae: 14.4181\n",
      "Epoch 120/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 537.4166 - mse: 537.4166 - mae: 12.5366\n",
      "Epoch 120: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 247.5750 - mse: 247.5750 - mae: 8.7473 - val_loss: 869.0988 - val_mse: 869.0988 - val_mae: 12.6088\n",
      "Epoch 121/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 248.6272 - mse: 248.6272 - mae: 8.7864\n",
      "Epoch 121: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.2860 - mse: 229.2860 - mae: 8.1114 - val_loss: 983.7218 - val_mse: 983.7218 - val_mae: 13.2146\n",
      "Epoch 122/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 349.0435 - mse: 349.0435 - mae: 9.0185\n",
      "Epoch 122: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.0596 - mse: 231.0596 - mae: 7.8466 - val_loss: 933.1931 - val_mse: 933.1931 - val_mae: 12.7013\n",
      "Epoch 123/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 502.0836 - mse: 502.0836 - mae: 12.4909\n",
      "Epoch 123: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.4727 - mse: 248.4727 - mae: 7.9306 - val_loss: 966.6663 - val_mse: 966.6663 - val_mae: 12.6575\n",
      "Epoch 124/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 274.8022 - mse: 274.8022 - mae: 9.9207\n",
      "Epoch 124: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 256.6787 - mse: 256.6787 - mae: 8.4552 - val_loss: 835.4893 - val_mse: 835.4893 - val_mae: 12.9254\n",
      "Epoch 125/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 454.9855 - mse: 454.9855 - mae: 11.5659\n",
      "Epoch 125: val_loss did not improve from 830.61078\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 272.4446 - mse: 272.4446 - mae: 8.7499 - val_loss: 1024.8743 - val_mse: 1024.8743 - val_mae: 13.1676\n",
      "Epoch 126/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 427.2027 - mse: 427.2027 - mae: 9.5044\n",
      "Epoch 126: val_loss improved from 830.61078 to 711.27069, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 241.6297 - mse: 241.6297 - mae: 9.2565 - val_loss: 711.2707 - val_mse: 711.2707 - val_mae: 11.8913\n",
      "Epoch 127/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 278.5373 - mse: 278.5373 - mae: 9.8673\n",
      "Epoch 127: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 236.6798 - mse: 236.6798 - mae: 8.5829 - val_loss: 1003.8037 - val_mse: 1003.8037 - val_mae: 14.5124\n",
      "Epoch 128/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 47.3891 - mse: 47.3891 - mae: 4.8633\n",
      "Epoch 128: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 199.6175 - mse: 199.6175 - mae: 7.7631 - val_loss: 806.9518 - val_mse: 806.9519 - val_mae: 12.0553\n",
      "Epoch 129/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 211.1767 - mse: 211.1767 - mae: 8.3366\n",
      "Epoch 129: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 213.1106 - mse: 213.1106 - mae: 8.0827 - val_loss: 861.8304 - val_mse: 861.8304 - val_mae: 12.2972\n",
      "Epoch 130/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 298.3761 - mse: 298.3761 - mae: 9.1478\n",
      "Epoch 130: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 201.2892 - mse: 201.2892 - mae: 7.8341 - val_loss: 808.7502 - val_mse: 808.7502 - val_mae: 11.8723\n",
      "Epoch 131/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 307.7085 - mse: 307.7085 - mae: 9.0656\n",
      "Epoch 131: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 191.6534 - mse: 191.6534 - mae: 7.6011 - val_loss: 812.7084 - val_mse: 812.7084 - val_mae: 11.7488\n",
      "Epoch 132/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 287.4439 - mse: 287.4439 - mae: 9.0425\n",
      "Epoch 132: val_loss did not improve from 711.27069\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7322 - mse: 221.7322 - mae: 7.4162 - val_loss: 1008.9460 - val_mse: 1008.9460 - val_mae: 13.2140\n",
      "Epoch 133/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 282.0800 - mse: 282.0800 - mae: 6.8963\n",
      "Epoch 133: val_loss improved from 711.27069 to 706.88422, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 243.6169 - mse: 243.6169 - mae: 8.1295 - val_loss: 706.8842 - val_mse: 706.8842 - val_mae: 11.4574\n",
      "Epoch 134/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 86.0175 - mse: 86.0175 - mae: 5.6135\n",
      "Epoch 134: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 181.8642 - mse: 181.8642 - mae: 7.3647 - val_loss: 1001.0519 - val_mse: 1001.0519 - val_mae: 13.4424\n",
      "Epoch 135/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 186.2005 - mse: 186.2005 - mae: 6.9970\n",
      "Epoch 135: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.3788 - mse: 168.3788 - mae: 6.9272 - val_loss: 777.6055 - val_mse: 777.6055 - val_mae: 11.4200\n",
      "Epoch 136/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 346.7552 - mse: 346.7552 - mae: 9.2923\n",
      "Epoch 136: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 186.1609 - mse: 186.1609 - mae: 7.7072 - val_loss: 747.7532 - val_mse: 747.7532 - val_mae: 11.9423\n",
      "Epoch 137/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 56.4717 - mse: 56.4717 - mae: 5.6270\n",
      "Epoch 137: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 178.9909 - mse: 178.9909 - mae: 7.6385 - val_loss: 903.2989 - val_mse: 903.2989 - val_mae: 12.4822\n",
      "Epoch 138/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 69.8298 - mse: 69.8298 - mae: 5.6953\n",
      "Epoch 138: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 169.5558 - mse: 169.5558 - mae: 7.6046 - val_loss: 715.7589 - val_mse: 715.7589 - val_mae: 11.0167\n",
      "Epoch 139/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.1422 - mse: 29.1422 - mae: 3.7296\n",
      "Epoch 139: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 154.7802 - mse: 154.7802 - mae: 6.8579 - val_loss: 801.5162 - val_mse: 801.5162 - val_mae: 12.1799\n",
      "Epoch 140/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 230.9134 - mse: 230.9134 - mae: 8.2926\n",
      "Epoch 140: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 154.5738 - mse: 154.5738 - mae: 6.7903 - val_loss: 843.4387 - val_mse: 843.4387 - val_mae: 11.8653\n",
      "Epoch 141/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 206.4398 - mse: 206.4398 - mae: 7.2531\n",
      "Epoch 141: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 163.5014 - mse: 163.5014 - mae: 7.2386 - val_loss: 762.2751 - val_mse: 762.2751 - val_mae: 11.1703\n",
      "Epoch 142/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 182.2379 - mse: 182.2379 - mae: 7.4700\n",
      "Epoch 142: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 152.1200 - mse: 152.1200 - mae: 6.9824 - val_loss: 809.1553 - val_mse: 809.1553 - val_mae: 12.0534\n",
      "Epoch 143/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 145.0868 - mse: 145.0868 - mae: 7.5028\n",
      "Epoch 143: val_loss did not improve from 706.88422\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 148.8865 - mse: 148.8865 - mae: 6.7227 - val_loss: 754.5419 - val_mse: 754.5419 - val_mae: 10.8488\n",
      "Epoch 144/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 271.8142 - mse: 271.8142 - mae: 9.4580\n",
      "Epoch 144: val_loss improved from 706.88422 to 664.89618, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 18ms/step - loss: 182.1971 - mse: 182.1971 - mae: 7.7093 - val_loss: 664.8962 - val_mse: 664.8962 - val_mae: 11.9093\n",
      "Epoch 145/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 75.4755 - mse: 75.4755 - mae: 5.6566\n",
      "Epoch 145: val_loss did not improve from 664.89618\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 168.6356 - mse: 168.6356 - mae: 7.6199 - val_loss: 837.3649 - val_mse: 837.3649 - val_mae: 12.0414\n",
      "Epoch 146/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 158.4234 - mse: 158.4234 - mae: 7.1701\n",
      "Epoch 146: val_loss improved from 664.89618 to 639.63025, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 36ms/step - loss: 162.1461 - mse: 162.1461 - mae: 8.1662 - val_loss: 639.6302 - val_mse: 639.6302 - val_mae: 10.7770\n",
      "Epoch 147/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 143.4110 - mse: 143.4110 - mae: 5.7652\n",
      "Epoch 147: val_loss did not improve from 639.63025\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 139.9184 - mse: 139.9184 - mae: 6.7998 - val_loss: 801.3861 - val_mse: 801.3862 - val_mae: 12.6316\n",
      "Epoch 148/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 167.4753 - mse: 167.4753 - mae: 6.8598\n",
      "Epoch 148: val_loss did not improve from 639.63025\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 155.3552 - mse: 155.3552 - mae: 6.9870 - val_loss: 667.2114 - val_mse: 667.2114 - val_mae: 10.5953\n",
      "Epoch 149/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 107.7854 - mse: 107.7854 - mae: 6.8049\n",
      "Epoch 149: val_loss did not improve from 639.63025\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 199.5384 - mse: 199.5384 - mae: 7.5619 - val_loss: 908.1094 - val_mse: 908.1094 - val_mae: 12.9550\n",
      "Epoch 150/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 202.6010 - mse: 202.6010 - mae: 7.9404\n",
      "Epoch 150: val_loss improved from 639.63025 to 569.51056, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 121.0273 - mse: 121.0273 - mae: 6.5777 - val_loss: 569.5106 - val_mse: 569.5106 - val_mae: 10.7071\n",
      "Epoch 151/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 103.8056 - mse: 103.8056 - mae: 6.8510\n",
      "Epoch 151: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 152.5514 - mse: 152.5514 - mae: 7.1008 - val_loss: 857.6406 - val_mse: 857.6406 - val_mae: 13.4515\n",
      "Epoch 152/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 244.1931 - mse: 244.1931 - mae: 8.8800\n",
      "Epoch 152: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 122.6048 - mse: 122.6048 - mae: 6.2209 - val_loss: 672.0179 - val_mse: 672.0179 - val_mae: 10.4300\n",
      "Epoch 153/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 126.3041 - mse: 126.3041 - mae: 7.6937\n",
      "Epoch 153: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.6316 - mse: 120.6315 - mae: 6.5231 - val_loss: 734.1445 - val_mse: 734.1445 - val_mae: 11.6782\n",
      "Epoch 154/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 50.3987 - mse: 50.3987 - mae: 5.1471\n",
      "Epoch 154: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 113.5780 - mse: 113.5780 - mae: 6.1740 - val_loss: 703.9208 - val_mse: 703.9208 - val_mae: 10.9064\n",
      "Epoch 155/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 61.8994 - mse: 61.8994 - mae: 4.0846\n",
      "Epoch 155: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104.6143 - mse: 104.6143 - mae: 5.7922 - val_loss: 691.7502 - val_mse: 691.7502 - val_mae: 10.6355\n",
      "Epoch 156/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 133.8394 - mse: 133.8394 - mae: 6.3351\n",
      "Epoch 156: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 112.3412 - mse: 112.3412 - mae: 5.9487 - val_loss: 712.1282 - val_mse: 712.1282 - val_mae: 11.3489\n",
      "Epoch 157/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 114.6210 - mse: 114.6210 - mae: 5.7834\n",
      "Epoch 157: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 100.0770 - mse: 100.0770 - mae: 5.7790 - val_loss: 704.5792 - val_mse: 704.5792 - val_mae: 10.6807\n",
      "Epoch 158/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 94.3842 - mse: 94.3842 - mae: 6.3370\n",
      "Epoch 158: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 95.8223 - mse: 95.8223 - mae: 5.5812 - val_loss: 727.0654 - val_mse: 727.0654 - val_mae: 11.0517\n",
      "Epoch 159/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 119.8594 - mse: 119.8594 - mae: 5.6237\n",
      "Epoch 159: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 96.1828 - mse: 96.1828 - mae: 5.4739 - val_loss: 691.6906 - val_mse: 691.6906 - val_mae: 10.9149\n",
      "Epoch 160/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 60.5196 - mse: 60.5196 - mae: 5.1161\n",
      "Epoch 160: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 94.8357 - mse: 94.8357 - mae: 5.5601 - val_loss: 709.1805 - val_mse: 709.1805 - val_mae: 11.0841\n",
      "Epoch 161/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 107.4743 - mse: 107.4743 - mae: 4.7652\n",
      "Epoch 161: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.9748 - mse: 98.9748 - mae: 5.5717 - val_loss: 685.1013 - val_mse: 685.1013 - val_mae: 10.5501\n",
      "Epoch 162/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 46.2995 - mse: 46.2995 - mae: 4.5246\n",
      "Epoch 162: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 90.3577 - mse: 90.3577 - mae: 5.6564 - val_loss: 645.1913 - val_mse: 645.1913 - val_mae: 10.6904\n",
      "Epoch 163/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 100.7192 - mse: 100.7192 - mae: 5.8437\n",
      "Epoch 163: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 90.6626 - mse: 90.6626 - mae: 5.9622 - val_loss: 694.8351 - val_mse: 694.8351 - val_mae: 11.0641\n",
      "Epoch 164/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 88.3813 - mse: 88.3813 - mae: 6.2327\n",
      "Epoch 164: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 86.2965 - mse: 86.2965 - mae: 5.4060 - val_loss: 764.7155 - val_mse: 764.7155 - val_mae: 11.4916\n",
      "Epoch 165/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 96.4872 - mse: 96.4872 - mae: 6.0215\n",
      "Epoch 165: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 88.0420 - mse: 88.0420 - mae: 5.4409 - val_loss: 653.8285 - val_mse: 653.8285 - val_mae: 10.6958\n",
      "Epoch 166/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 102.5630 - mse: 102.5630 - mae: 6.0095\n",
      "Epoch 166: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 82.9912 - mse: 82.9912 - mae: 5.4692 - val_loss: 633.0066 - val_mse: 633.0066 - val_mae: 10.8152\n",
      "Epoch 167/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 121.5509 - mse: 121.5509 - mae: 5.9439\n",
      "Epoch 167: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 79.9071 - mse: 79.9071 - mae: 5.2529 - val_loss: 661.7202 - val_mse: 661.7202 - val_mae: 10.8626\n",
      "Epoch 168/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 140.4883 - mse: 140.4883 - mae: 6.6327\n",
      "Epoch 168: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 83.2536 - mse: 83.2536 - mae: 5.2533 - val_loss: 652.4940 - val_mse: 652.4940 - val_mae: 11.5939\n",
      "Epoch 169/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 159.0816 - mse: 159.0816 - mae: 7.6219\n",
      "Epoch 169: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 91.0087 - mse: 91.0087 - mae: 5.3779 - val_loss: 707.6240 - val_mse: 707.6240 - val_mae: 10.8189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 50.3678 - mse: 50.3678 - mae: 5.0399\n",
      "Epoch 170: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 76.4032 - mse: 76.4032 - mae: 5.4262 - val_loss: 620.0561 - val_mse: 620.0561 - val_mae: 10.6944\n",
      "Epoch 171/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 82.4930 - mse: 82.4930 - mae: 5.2704\n",
      "Epoch 171: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 96.0607 - mse: 96.0607 - mae: 6.0018 - val_loss: 613.5937 - val_mse: 613.5937 - val_mae: 11.7436\n",
      "Epoch 172/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.9531 - mse: 32.9531 - mae: 3.9979\n",
      "Epoch 172: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 80.5040 - mse: 80.5040 - mae: 5.5432 - val_loss: 693.9489 - val_mse: 693.9489 - val_mae: 11.5185\n",
      "Epoch 173/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 79.4543 - mse: 79.4543 - mae: 6.3787\n",
      "Epoch 173: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 75.3467 - mse: 75.3467 - mae: 5.5236 - val_loss: 586.7703 - val_mse: 586.7703 - val_mae: 10.7990\n",
      "Epoch 174/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.2191 - mse: 35.2191 - mae: 4.3094\n",
      "Epoch 174: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 70.8483 - mse: 70.8483 - mae: 5.4615 - val_loss: 633.0167 - val_mse: 633.0167 - val_mae: 10.6761\n",
      "Epoch 175/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 105.4883 - mse: 105.4883 - mae: 6.2894\n",
      "Epoch 175: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72.7575 - mse: 72.7575 - mae: 5.0955 - val_loss: 629.0899 - val_mse: 629.0899 - val_mae: 10.4697\n",
      "Epoch 176/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 42.3411 - mse: 42.3411 - mae: 4.4830\n",
      "Epoch 176: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 64.3707 - mse: 64.3707 - mae: 4.9963 - val_loss: 603.4425 - val_mse: 603.4425 - val_mae: 11.5725\n",
      "Epoch 177/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 50.2624 - mse: 50.2624 - mae: 5.4425\n",
      "Epoch 177: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 67.6688 - mse: 67.6688 - mae: 5.1208 - val_loss: 626.7157 - val_mse: 626.7157 - val_mae: 11.0143\n",
      "Epoch 178/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 67.6179 - mse: 67.6179 - mae: 6.1231\n",
      "Epoch 178: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 78.6894 - mse: 78.6894 - mae: 5.5802 - val_loss: 584.1761 - val_mse: 584.1761 - val_mae: 11.5427\n",
      "Epoch 179/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 58.2702 - mse: 58.2702 - mae: 5.3056\n",
      "Epoch 179: val_loss did not improve from 569.51056\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 72.3218 - mse: 72.3218 - mae: 5.3705 - val_loss: 610.0150 - val_mse: 610.0150 - val_mae: 10.7434\n",
      "Epoch 180/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 77.0155 - mse: 77.0155 - mae: 5.5470\n",
      "Epoch 180: val_loss improved from 569.51056 to 547.27448, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 65.4959 - mse: 65.4959 - mae: 5.1454 - val_loss: 547.2745 - val_mse: 547.2745 - val_mae: 10.5166\n",
      "Epoch 181/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 68.4267 - mse: 68.4267 - mae: 5.3060\n",
      "Epoch 181: val_loss did not improve from 547.27448\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 58.6058 - mse: 58.6058 - mae: 4.9471 - val_loss: 614.1523 - val_mse: 614.1523 - val_mae: 11.3184\n",
      "Epoch 182/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 63.9217 - mse: 63.9217 - mae: 4.9066\n",
      "Epoch 182: val_loss did not improve from 547.27448\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 56.5770 - mse: 56.5770 - mae: 4.6171 - val_loss: 583.0032 - val_mse: 583.0032 - val_mae: 10.9823\n",
      "Epoch 183/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 41.6653 - mse: 41.6653 - mae: 4.5179\n",
      "Epoch 183: val_loss did not improve from 547.27448\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58.3565 - mse: 58.3565 - mae: 4.5144 - val_loss: 585.1981 - val_mse: 585.1981 - val_mae: 10.9465\n",
      "Epoch 184/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 53.9789 - mse: 53.9789 - mae: 4.5079\n",
      "Epoch 184: val_loss improved from 547.27448 to 534.68518, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 62.3756 - mse: 62.3756 - mae: 4.6974 - val_loss: 534.6852 - val_mse: 534.6852 - val_mae: 10.2782\n",
      "Epoch 185/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 39.2719 - mse: 39.2719 - mae: 4.0625\n",
      "Epoch 185: val_loss did not improve from 534.68518\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 58.9304 - mse: 58.9304 - mae: 4.5841 - val_loss: 619.5226 - val_mse: 619.5226 - val_mae: 10.6621\n",
      "Epoch 186/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.1896 - mse: 30.1896 - mae: 3.7594\n",
      "Epoch 186: val_loss improved from 534.68518 to 508.01724, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 49.0874 - mse: 49.0874 - mae: 4.4509 - val_loss: 508.0172 - val_mse: 508.0172 - val_mae: 10.5214\n",
      "Epoch 187/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 98.4832 - mse: 98.4832 - mae: 6.2648\n",
      "Epoch 187: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 54.6593 - mse: 54.6593 - mae: 4.6354 - val_loss: 530.7516 - val_mse: 530.7516 - val_mae: 10.8157\n",
      "Epoch 188/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 75.0733 - mse: 75.0733 - mae: 4.8447\n",
      "Epoch 188: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 54.5908 - mse: 54.5908 - mae: 4.6165 - val_loss: 571.8474 - val_mse: 571.8474 - val_mae: 10.7379\n",
      "Epoch 189/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 51.4560 - mse: 51.4560 - mae: 4.0830\n",
      "Epoch 189: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 52.4334 - mse: 52.4334 - mae: 4.5219 - val_loss: 537.3889 - val_mse: 537.3889 - val_mae: 11.2577\n",
      "Epoch 190/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 71.7968 - mse: 71.7968 - mae: 5.1361\n",
      "Epoch 190: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 45.5524 - mse: 45.5524 - mae: 4.1986 - val_loss: 590.2955 - val_mse: 590.2955 - val_mae: 10.9270\n",
      "Epoch 191/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.3441 - mse: 37.3441 - mae: 3.5875\n",
      "Epoch 191: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 48.1021 - mse: 48.1021 - mae: 4.5544 - val_loss: 569.2241 - val_mse: 569.2241 - val_mae: 11.3441\n",
      "Epoch 192/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 30.3900 - mse: 30.3900 - mae: 3.6664\n",
      "Epoch 192: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 47.2667 - mse: 47.2667 - mae: 4.3305 - val_loss: 527.8005 - val_mse: 527.8005 - val_mae: 10.6977\n",
      "Epoch 193/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 63.7093 - mse: 63.7093 - mae: 4.5779\n",
      "Epoch 193: val_loss did not improve from 508.01724\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 44.6995 - mse: 44.6995 - mae: 3.9971 - val_loss: 561.2120 - val_mse: 561.2120 - val_mae: 10.6550\n",
      "Epoch 194/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 71.4565 - mse: 71.4565 - mae: 5.0881\n",
      "Epoch 194: val_loss improved from 508.01724 to 463.30121, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 42.2153 - mse: 42.2153 - mae: 4.0809 - val_loss: 463.3012 - val_mse: 463.3012 - val_mae: 10.5994\n",
      "Epoch 195/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 24.8265 - mse: 24.8265 - mae: 3.7106\n",
      "Epoch 195: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 45.4923 - mse: 45.4923 - mae: 4.2073 - val_loss: 575.1966 - val_mse: 575.1966 - val_mae: 11.2288\n",
      "Epoch 196/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 52.1348 - mse: 52.1348 - mae: 3.7892\n",
      "Epoch 196: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 42.0433 - mse: 42.0433 - mae: 4.0569 - val_loss: 532.7292 - val_mse: 532.7292 - val_mae: 10.5458\n",
      "Epoch 197/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 42.9120 - mse: 42.9120 - mae: 4.1887\n",
      "Epoch 197: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 40.7314 - mse: 40.7314 - mae: 4.0732 - val_loss: 528.3069 - val_mse: 528.3069 - val_mae: 10.6281\n",
      "Epoch 198/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.3642 - mse: 24.3642 - mae: 3.3320\n",
      "Epoch 198: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 38.0574 - mse: 38.0574 - mae: 3.9550 - val_loss: 514.7210 - val_mse: 514.7210 - val_mae: 10.6182\n",
      "Epoch 199/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 58.9009 - mse: 58.9009 - mae: 4.6823\n",
      "Epoch 199: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 37.9422 - mse: 37.9422 - mae: 4.0624 - val_loss: 542.1910 - val_mse: 542.1910 - val_mae: 10.9897\n",
      "Epoch 200/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 38.9027 - mse: 38.9027 - mae: 3.7018\n",
      "Epoch 200: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.8185 - mse: 37.8185 - mae: 3.7977 - val_loss: 522.2900 - val_mse: 522.2900 - val_mae: 10.6037\n",
      "Epoch 201/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21.4079 - mse: 21.4079 - mae: 2.8787\n",
      "Epoch 201: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 35.3541 - mse: 35.3541 - mae: 3.7153 - val_loss: 518.2745 - val_mse: 518.2745 - val_mae: 10.9227\n",
      "Epoch 202/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 55.6527 - mse: 55.6527 - mae: 4.1248\n",
      "Epoch 202: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 35.2230 - mse: 35.2230 - mae: 3.8836 - val_loss: 482.5917 - val_mse: 482.5917 - val_mae: 10.6671\n",
      "Epoch 203/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.2277 - mse: 29.2277 - mae: 3.5309\n",
      "Epoch 203: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.0146 - mse: 35.0146 - mae: 3.7779 - val_loss: 504.0316 - val_mse: 504.0316 - val_mae: 10.6960\n",
      "Epoch 204/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.9852 - mse: 35.9852 - mae: 3.6310\n",
      "Epoch 204: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.3014 - mse: 34.3014 - mae: 3.8378 - val_loss: 537.7263 - val_mse: 537.7263 - val_mae: 11.0173\n",
      "Epoch 205/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.3533 - mse: 12.3533 - mae: 2.5891\n",
      "Epoch 205: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.9142 - mse: 32.9142 - mae: 3.5812 - val_loss: 477.4643 - val_mse: 477.4643 - val_mae: 10.2248\n",
      "Epoch 206/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.9699 - mse: 17.9699 - mae: 3.0066\n",
      "Epoch 206: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.7568 - mse: 36.7568 - mae: 3.8746 - val_loss: 531.7427 - val_mse: 531.7427 - val_mae: 11.4129\n",
      "Epoch 207/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 43.1621 - mse: 43.1621 - mae: 4.3740\n",
      "Epoch 207: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 36.9420 - mse: 36.9420 - mae: 3.8391 - val_loss: 484.8762 - val_mse: 484.8762 - val_mae: 10.3552\n",
      "Epoch 208/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.8362 - mse: 23.8362 - mae: 3.2180\n",
      "Epoch 208: val_loss did not improve from 463.30121\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33.5097 - mse: 33.5097 - mae: 3.8581 - val_loss: 530.7529 - val_mse: 530.7529 - val_mae: 11.9284\n",
      "Epoch 209/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 43.2578 - mse: 43.2578 - mae: 4.6748\n",
      "Epoch 209: val_loss improved from 463.30121 to 462.29688, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 35.8190 - mse: 35.8190 - mae: 4.0145 - val_loss: 462.2969 - val_mse: 462.2969 - val_mae: 10.3814\n",
      "Epoch 210/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 34.6911 - mse: 34.6911 - mae: 4.8759\n",
      "Epoch 210: val_loss did not improve from 462.29688\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 39.9707 - mse: 39.9707 - mae: 4.3062 - val_loss: 591.0779 - val_mse: 591.0779 - val_mae: 12.7971\n",
      "Epoch 211/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.6548 - mse: 31.6548 - mae: 4.2225\n",
      "Epoch 211: val_loss improved from 462.29688 to 425.63013, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 42.6817 - mse: 42.6817 - mae: 4.3134 - val_loss: 425.6301 - val_mse: 425.6301 - val_mae: 9.7447\n",
      "Epoch 212/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.1954 - mse: 20.1954 - mae: 3.4488\n",
      "Epoch 212: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 33.5604 - mse: 33.5604 - mae: 4.0107 - val_loss: 587.5759 - val_mse: 587.5759 - val_mae: 12.1726\n",
      "Epoch 213/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 66.0383 - mse: 66.0383 - mae: 5.1555\n",
      "Epoch 213: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 36.2565 - mse: 36.2565 - mae: 4.0186 - val_loss: 460.7315 - val_mse: 460.7315 - val_mae: 10.2832\n",
      "Epoch 214/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.4140 - mse: 23.4140 - mae: 3.6526\n",
      "Epoch 214: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 31.9715 - mse: 31.9715 - mae: 3.7023 - val_loss: 506.1825 - val_mse: 506.1825 - val_mae: 11.4736\n",
      "Epoch 215/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.2384 - mse: 25.2384 - mae: 3.3112\n",
      "Epoch 215: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.7640 - mse: 34.7640 - mae: 3.8715 - val_loss: 498.2595 - val_mse: 498.2595 - val_mae: 10.6402\n",
      "Epoch 216/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 23.4507 - mse: 23.4507 - mae: 3.3654\n",
      "Epoch 216: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.0754 - mse: 30.0754 - mae: 3.4690 - val_loss: 471.0530 - val_mse: 471.0530 - val_mae: 10.5385\n",
      "Epoch 217/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 40.5544 - mse: 40.5544 - mae: 3.9710\n",
      "Epoch 217: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.9471 - mse: 26.9471 - mae: 3.3935 - val_loss: 448.3282 - val_mse: 448.3282 - val_mae: 10.6427\n",
      "Epoch 218/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.4380 - mse: 20.4380 - mae: 3.3347\n",
      "Epoch 218: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.6354 - mse: 26.6354 - mae: 3.3489 - val_loss: 493.0036 - val_mse: 493.0036 - val_mae: 10.9631\n",
      "Epoch 219/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8458 - mse: 9.8458 - mae: 2.3564\n",
      "Epoch 219: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.6060 - mse: 27.6060 - mae: 3.2891 - val_loss: 451.4057 - val_mse: 451.4057 - val_mae: 10.2038\n",
      "Epoch 220/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.2200 - mse: 35.2200 - mae: 3.8332\n",
      "Epoch 220: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27.2030 - mse: 27.2030 - mae: 3.4407 - val_loss: 453.7840 - val_mse: 453.7840 - val_mae: 10.9353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1571 - mse: 10.1571 - mae: 2.0335\n",
      "Epoch 221: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27.6173 - mse: 27.6173 - mae: 3.3820 - val_loss: 466.5221 - val_mse: 466.5221 - val_mae: 10.7589\n",
      "Epoch 222/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 46.3531 - mse: 46.3531 - mae: 3.7963\n",
      "Epoch 222: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.8979 - mse: 26.8979 - mae: 3.3701 - val_loss: 451.2670 - val_mse: 451.2670 - val_mae: 10.5803\n",
      "Epoch 223/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.0951 - mse: 15.0951 - mae: 3.1811\n",
      "Epoch 223: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.3197 - mse: 29.3197 - mae: 3.4996 - val_loss: 455.8804 - val_mse: 455.8804 - val_mae: 11.1299\n",
      "Epoch 224/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.0510 - mse: 32.0510 - mae: 3.6645\n",
      "Epoch 224: val_loss did not improve from 425.63013\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.6598 - mse: 28.6598 - mae: 3.5152 - val_loss: 529.7692 - val_mse: 529.7692 - val_mae: 11.4672\n",
      "Epoch 225/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9805 - mse: 9.9805 - mae: 2.1713\n",
      "Epoch 225: val_loss improved from 425.63013 to 389.92395, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 38.4449 - mse: 38.4449 - mae: 3.8710 - val_loss: 389.9240 - val_mse: 389.9240 - val_mae: 9.8552\n",
      "Epoch 226/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 56.9949 - mse: 56.9949 - mae: 5.4796\n",
      "Epoch 226: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 40.4473 - mse: 40.4473 - mae: 4.2084 - val_loss: 498.5123 - val_mse: 498.5123 - val_mae: 11.9388\n",
      "Epoch 227/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.5489 - mse: 24.5489 - mae: 3.5049\n",
      "Epoch 227: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.6575 - mse: 25.6575 - mae: 3.4640 - val_loss: 410.1075 - val_mse: 410.1075 - val_mae: 9.9552\n",
      "Epoch 228/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 50.1830 - mse: 50.1830 - mae: 5.1760\n",
      "Epoch 228: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.0370 - mse: 31.0370 - mae: 3.8496 - val_loss: 493.3604 - val_mse: 493.3604 - val_mae: 11.8551\n",
      "Epoch 229/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.4470 - mse: 20.4470 - mae: 3.0398\n",
      "Epoch 229: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.8297 - mse: 29.8297 - mae: 3.6700 - val_loss: 457.0359 - val_mse: 457.0359 - val_mae: 10.6501\n",
      "Epoch 230/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.1834 - mse: 28.1834 - mae: 3.3079\n",
      "Epoch 230: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27.6475 - mse: 27.6475 - mae: 3.4386 - val_loss: 426.7369 - val_mse: 426.7369 - val_mae: 10.3264\n",
      "Epoch 231/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.6738 - mse: 12.6738 - mae: 2.5984\n",
      "Epoch 231: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.6902 - mse: 27.6902 - mae: 3.4657 - val_loss: 473.6559 - val_mse: 473.6559 - val_mae: 11.1666\n",
      "Epoch 232/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.5820 - mse: 12.5820 - mae: 2.3142\n",
      "Epoch 232: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.7802 - mse: 24.7802 - mae: 3.2648 - val_loss: 425.6268 - val_mse: 425.6268 - val_mae: 10.3326\n",
      "Epoch 233/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.2823 - mse: 18.2823 - mae: 3.1932\n",
      "Epoch 233: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.2825 - mse: 22.2825 - mae: 3.0985 - val_loss: 446.1924 - val_mse: 446.1924 - val_mae: 11.3709\n",
      "Epoch 234/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.3880 - mse: 24.3880 - mae: 3.3841\n",
      "Epoch 234: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.6764 - mse: 27.6764 - mae: 3.3830 - val_loss: 403.5481 - val_mse: 403.5480 - val_mae: 10.3014\n",
      "Epoch 235/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 37.4543 - mse: 37.4543 - mae: 4.6898\n",
      "Epoch 235: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.4783 - mse: 29.4783 - mae: 3.8637 - val_loss: 475.6360 - val_mse: 475.6360 - val_mae: 11.8954\n",
      "Epoch 236/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.9235 - mse: 25.9235 - mae: 4.2305\n",
      "Epoch 236: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.2809 - mse: 26.2809 - mae: 3.5811 - val_loss: 412.5570 - val_mse: 412.5570 - val_mae: 10.1257\n",
      "Epoch 237/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.0296 - mse: 19.0296 - mae: 2.8107\n",
      "Epoch 237: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.4278 - mse: 24.4278 - mae: 3.3331 - val_loss: 487.1790 - val_mse: 487.1790 - val_mae: 12.0251\n",
      "Epoch 238/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 49.9565 - mse: 49.9565 - mae: 4.4180\n",
      "Epoch 238: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.6228 - mse: 21.6228 - mae: 3.0389 - val_loss: 398.6645 - val_mse: 398.6645 - val_mae: 10.2582\n",
      "Epoch 239/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.3310 - mse: 22.3310 - mae: 3.1472\n",
      "Epoch 239: val_loss did not improve from 389.92395\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22.2601 - mse: 22.2601 - mae: 3.2224 - val_loss: 471.0254 - val_mse: 471.0254 - val_mae: 11.6010\n",
      "Epoch 240/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 28.1693 - mse: 28.1693 - mae: 3.6113\n",
      "Epoch 240: val_loss improved from 389.92395 to 385.92178, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 21.0166 - mse: 21.0166 - mae: 3.0751 - val_loss: 385.9218 - val_mse: 385.9218 - val_mae: 10.2097\n",
      "Epoch 241/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 32.1962 - mse: 32.1962 - mae: 3.8125\n",
      "Epoch 241: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 21.1505 - mse: 21.1505 - mae: 3.0587 - val_loss: 456.3380 - val_mse: 456.3380 - val_mae: 11.2671\n",
      "Epoch 242/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 26.9214 - mse: 26.9214 - mae: 3.5928\n",
      "Epoch 242: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 20.5389 - mse: 20.5389 - mae: 2.9457 - val_loss: 415.4683 - val_mse: 415.4683 - val_mae: 10.3083\n",
      "Epoch 243/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.3997 - mse: 18.3997 - mae: 2.5438\n",
      "Epoch 243: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19.2994 - mse: 19.2994 - mae: 2.9084 - val_loss: 460.8949 - val_mse: 460.8949 - val_mae: 10.9806\n",
      "Epoch 244/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.8569 - mse: 18.8569 - mae: 2.7632\n",
      "Epoch 244: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.2778 - mse: 20.2778 - mae: 2.9501 - val_loss: 392.8524 - val_mse: 392.8524 - val_mae: 10.3859\n",
      "Epoch 245/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 25.8190 - mse: 25.8190 - mae: 3.0832\n",
      "Epoch 245: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 21.9434 - mse: 21.9434 - mae: 3.0386 - val_loss: 436.8895 - val_mse: 436.8895 - val_mae: 11.0779\n",
      "Epoch 246/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 33.7252 - mse: 33.7252 - mae: 3.4182\n",
      "Epoch 246: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 28.8883 - mse: 28.8883 - mae: 3.5367 - val_loss: 460.8086 - val_mse: 460.8086 - val_mae: 11.4033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 247/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.6966 - mse: 13.6966 - mae: 2.9365\n",
      "Epoch 247: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33.0698 - mse: 33.0698 - mae: 3.9577 - val_loss: 389.6216 - val_mse: 389.6216 - val_mae: 10.6068\n",
      "Epoch 248/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.1536 - mse: 17.1536 - mae: 3.3770\n",
      "Epoch 248: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.5960 - mse: 29.5960 - mae: 3.7594 - val_loss: 517.6884 - val_mse: 517.6884 - val_mae: 13.4625\n",
      "Epoch 249/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.2779 - mse: 29.2779 - mae: 4.4155\n",
      "Epoch 249: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 34.8822 - mse: 34.8822 - mae: 4.4592 - val_loss: 402.7457 - val_mse: 402.7457 - val_mae: 10.3556\n",
      "Epoch 250/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 36.0679 - mse: 36.0679 - mae: 4.7046\n",
      "Epoch 250: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 25.4333 - mse: 25.4333 - mae: 3.8047 - val_loss: 484.6194 - val_mse: 484.6194 - val_mae: 12.2351\n",
      "Epoch 251/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.6504 - mse: 16.6504 - mae: 3.4048\n",
      "Epoch 251: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22.0067 - mse: 22.0067 - mae: 3.3368 - val_loss: 417.2980 - val_mse: 417.2980 - val_mae: 10.4654\n",
      "Epoch 252/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.2275 - mse: 19.2275 - mae: 3.3495\n",
      "Epoch 252: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.2924 - mse: 21.2924 - mae: 3.3301 - val_loss: 452.9644 - val_mse: 452.9644 - val_mae: 11.9588\n",
      "Epoch 253/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.7017 - mse: 12.7017 - mae: 3.0067\n",
      "Epoch 253: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.1903 - mse: 20.1903 - mae: 3.1648 - val_loss: 397.2348 - val_mse: 397.2348 - val_mae: 10.2926\n",
      "Epoch 254/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.1006 - mse: 29.1006 - mae: 3.9357\n",
      "Epoch 254: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.6883 - mse: 19.6883 - mae: 3.0340 - val_loss: 429.4266 - val_mse: 429.4266 - val_mae: 11.0767\n",
      "Epoch 255/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.7251 - mse: 15.7251 - mae: 2.6008\n",
      "Epoch 255: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19.6504 - mse: 19.6504 - mae: 2.8989 - val_loss: 408.0555 - val_mse: 408.0555 - val_mae: 10.6357\n",
      "Epoch 256/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.7424 - mse: 16.7424 - mae: 2.6252\n",
      "Epoch 256: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.8706 - mse: 16.8706 - mae: 2.6381 - val_loss: 430.2545 - val_mse: 430.2545 - val_mae: 10.7938\n",
      "Epoch 257/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.8391 - mse: 20.8391 - mae: 2.9736\n",
      "Epoch 257: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17.1879 - mse: 17.1879 - mae: 2.5995 - val_loss: 390.7142 - val_mse: 390.7142 - val_mae: 10.5799\n",
      "Epoch 258/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4977 - mse: 18.4977 - mae: 3.1324\n",
      "Epoch 258: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.5449 - mse: 17.5449 - mae: 2.7185 - val_loss: 426.0628 - val_mse: 426.0628 - val_mae: 10.9377\n",
      "Epoch 259/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 31.8156 - mse: 31.8156 - mae: 3.6461\n",
      "Epoch 259: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.1687 - mse: 17.1687 - mae: 2.8010 - val_loss: 392.1917 - val_mse: 392.1917 - val_mae: 10.4543\n",
      "Epoch 260/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.4529 - mse: 16.4529 - mae: 2.2851\n",
      "Epoch 260: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.0547 - mse: 18.0547 - mae: 2.8285 - val_loss: 410.1401 - val_mse: 410.1401 - val_mae: 10.6241\n",
      "Epoch 261/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.5173 - mse: 12.5173 - mae: 2.6533\n",
      "Epoch 261: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.9380 - mse: 17.9380 - mae: 2.9761 - val_loss: 412.1380 - val_mse: 412.1380 - val_mae: 10.6842\n",
      "Epoch 262/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.3679 - mse: 17.3679 - mae: 2.7256\n",
      "Epoch 262: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.2258 - mse: 18.2258 - mae: 2.9432 - val_loss: 387.1353 - val_mse: 387.1353 - val_mae: 10.4593\n",
      "Epoch 263/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.1304 - mse: 18.1304 - mae: 2.5978\n",
      "Epoch 263: val_loss did not improve from 385.92178\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.4844 - mse: 16.4844 - mae: 2.6401 - val_loss: 429.5009 - val_mse: 429.5009 - val_mae: 10.8209\n",
      "Epoch 264/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6112 - mse: 5.6112 - mae: 1.9411\n",
      "Epoch 264: val_loss improved from 385.92178 to 375.93192, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 15.9655 - mse: 15.9655 - mae: 2.6242 - val_loss: 375.9319 - val_mse: 375.9319 - val_mae: 10.3777\n",
      "Epoch 265/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6574 - mse: 6.6574 - mae: 1.9676\n",
      "Epoch 265: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 16.7861 - mse: 16.7861 - mae: 2.6692 - val_loss: 417.3850 - val_mse: 417.3850 - val_mae: 10.9573\n",
      "Epoch 266/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.4366 - mse: 15.4366 - mae: 2.7311\n",
      "Epoch 266: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.7484 - mse: 16.7484 - mae: 2.7332 - val_loss: 397.1460 - val_mse: 397.1461 - val_mae: 10.5786\n",
      "Epoch 267/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.4701 - mse: 18.4701 - mae: 2.7989\n",
      "Epoch 267: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.6010 - mse: 16.6010 - mae: 2.6850 - val_loss: 410.5092 - val_mse: 410.5091 - val_mae: 10.8912\n",
      "Epoch 268/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3680 - mse: 9.3680 - mae: 2.0363\n",
      "Epoch 268: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.0790 - mse: 16.0790 - mae: 2.6564 - val_loss: 413.4456 - val_mse: 413.4456 - val_mae: 10.6638\n",
      "Epoch 269/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9226 - mse: 9.9226 - mae: 2.2292\n",
      "Epoch 269: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0982 - mse: 15.0982 - mae: 2.5342 - val_loss: 393.9438 - val_mse: 393.9438 - val_mae: 10.8774\n",
      "Epoch 270/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.7380 - mse: 18.7380 - mae: 2.8570\n",
      "Epoch 270: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.1349 - mse: 15.1349 - mae: 2.5597 - val_loss: 409.7694 - val_mse: 409.7694 - val_mae: 10.9061\n",
      "Epoch 271/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.9644 - mse: 13.9644 - mae: 2.6979\n",
      "Epoch 271: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.7691 - mse: 14.7691 - mae: 2.5666 - val_loss: 394.9143 - val_mse: 394.9143 - val_mae: 10.8707\n",
      "Epoch 272/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.8170 - mse: 15.8170 - mae: 2.5420\n",
      "Epoch 272: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.1372 - mse: 15.1372 - mae: 2.5182 - val_loss: 380.9128 - val_mse: 380.9128 - val_mae: 10.3461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.1762 - mse: 12.1762 - mae: 2.3333\n",
      "Epoch 273: val_loss did not improve from 375.93192\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.8531 - mse: 14.8531 - mae: 2.4864 - val_loss: 426.7635 - val_mse: 426.7636 - val_mae: 11.5661\n",
      "Epoch 274/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.8698 - mse: 16.8698 - mae: 2.6927\n",
      "Epoch 274: val_loss improved from 375.93192 to 363.86652, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 16.4209 - mse: 16.4209 - mae: 2.6890 - val_loss: 363.8665 - val_mse: 363.8665 - val_mae: 10.4297\n",
      "Epoch 275/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1555 - mse: 13.1555 - mae: 2.4243\n",
      "Epoch 275: val_loss did not improve from 363.86652\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22.1477 - mse: 22.1477 - mae: 3.0758 - val_loss: 452.2062 - val_mse: 452.2062 - val_mae: 11.8578\n",
      "Epoch 276/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.4513 - mse: 19.4513 - mae: 3.0672\n",
      "Epoch 276: val_loss did not improve from 363.86652\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.3285 - mse: 19.3285 - mae: 2.9434 - val_loss: 396.7921 - val_mse: 396.7920 - val_mae: 10.4488\n",
      "Epoch 277/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5972 - mse: 7.5972 - mae: 2.1166\n",
      "Epoch 277: val_loss did not improve from 363.86652\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.7238 - mse: 16.7238 - mae: 2.7516 - val_loss: 368.3717 - val_mse: 368.3717 - val_mae: 10.6414\n",
      "Epoch 278/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 29.1758 - mse: 29.1758 - mae: 3.4097\n",
      "Epoch 278: val_loss did not improve from 363.86652\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.1387 - mse: 16.1387 - mae: 2.6436 - val_loss: 405.8281 - val_mse: 405.8281 - val_mae: 11.1110\n",
      "Epoch 279/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.1167 - mse: 16.1167 - mae: 2.4982\n",
      "Epoch 279: val_loss improved from 363.86652 to 355.01685, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 14.1414 - mse: 14.1414 - mae: 2.3425 - val_loss: 355.0168 - val_mse: 355.0168 - val_mae: 10.1792\n",
      "Epoch 280/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.0545 - mse: 16.0545 - mae: 2.7469\n",
      "Epoch 280: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.1620 - mse: 16.1620 - mae: 2.7260 - val_loss: 434.0485 - val_mse: 434.0485 - val_mae: 10.7842\n",
      "Epoch 281/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.2973 - mse: 19.2973 - mae: 3.0354\n",
      "Epoch 281: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.5692 - mse: 15.5692 - mae: 2.7125 - val_loss: 356.8861 - val_mse: 356.8861 - val_mae: 10.4031\n",
      "Epoch 282/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.0040 - mse: 20.0040 - mae: 2.9759\n",
      "Epoch 282: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.2607 - mse: 15.2607 - mae: 2.6075 - val_loss: 402.7676 - val_mse: 402.7676 - val_mae: 10.9550\n",
      "Epoch 283/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.2247 - mse: 12.2247 - mae: 2.3761\n",
      "Epoch 283: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.2945 - mse: 16.2945 - mae: 2.8026 - val_loss: 369.0654 - val_mse: 369.0654 - val_mae: 10.3047\n",
      "Epoch 284/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.9058 - mse: 7.9058 - mae: 2.1822\n",
      "Epoch 284: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.8026 - mse: 15.8026 - mae: 2.7266 - val_loss: 404.3215 - val_mse: 404.3215 - val_mae: 10.8847\n",
      "Epoch 285/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3128 - mse: 7.3128 - mae: 1.8517\n",
      "Epoch 285: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.9810 - mse: 13.9810 - mae: 2.4402 - val_loss: 384.2684 - val_mse: 384.2684 - val_mae: 10.4872\n",
      "Epoch 286/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.3155 - mse: 10.3155 - mae: 2.5607\n",
      "Epoch 286: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.0796 - mse: 16.0796 - mae: 2.6580 - val_loss: 394.7168 - val_mse: 394.7168 - val_mae: 10.9882\n",
      "Epoch 287/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.2840 - mse: 18.2840 - mae: 2.9591\n",
      "Epoch 287: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3435 - mse: 14.3435 - mae: 2.5250 - val_loss: 384.4495 - val_mse: 384.4495 - val_mae: 10.5626\n",
      "Epoch 288/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.0434 - mse: 8.0434 - mae: 1.9332\n",
      "Epoch 288: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1057 - mse: 13.1057 - mae: 2.4111 - val_loss: 417.6858 - val_mse: 417.6858 - val_mae: 11.2756\n",
      "Epoch 289/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.4910 - mse: 17.4910 - mae: 2.5739\n",
      "Epoch 289: val_loss did not improve from 355.01685\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.8178 - mse: 14.8178 - mae: 2.5675 - val_loss: 398.8753 - val_mse: 398.8753 - val_mae: 10.8695\n",
      "Epoch 290/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.1307 - mse: 14.1307 - mae: 2.3679\n",
      "Epoch 290: val_loss improved from 355.01685 to 350.03064, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 15.9303 - mse: 15.9303 - mae: 2.5408 - val_loss: 350.0306 - val_mse: 350.0306 - val_mae: 10.2696\n",
      "Epoch 291/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5109 - mse: 9.5109 - mae: 2.0718\n",
      "Epoch 291: val_loss did not improve from 350.03064\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 14.2706 - mse: 14.2706 - mae: 2.5285 - val_loss: 437.4855 - val_mse: 437.4855 - val_mae: 11.3849\n",
      "Epoch 292/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1689 - mse: 9.1689 - mae: 1.9352\n",
      "Epoch 292: val_loss did not improve from 350.03064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.0495 - mse: 15.0495 - mae: 2.4732 - val_loss: 376.5769 - val_mse: 376.5769 - val_mae: 10.7623\n",
      "Epoch 293/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5226 - mse: 7.5226 - mae: 1.9995\n",
      "Epoch 293: val_loss did not improve from 350.03064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.4635 - mse: 13.4635 - mae: 2.4115 - val_loss: 372.3701 - val_mse: 372.3701 - val_mae: 10.6261\n",
      "Epoch 294/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1428 - mse: 11.1428 - mae: 2.0967\n",
      "Epoch 294: val_loss did not improve from 350.03064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.7065 - mse: 14.7065 - mae: 2.5690 - val_loss: 439.4025 - val_mse: 439.4025 - val_mae: 11.4858\n",
      "Epoch 295/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3502 - mse: 11.3502 - mae: 2.5163\n",
      "Epoch 295: val_loss improved from 350.03064 to 336.66422, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 15.8337 - mse: 15.8337 - mae: 2.6783 - val_loss: 336.6642 - val_mse: 336.6642 - val_mae: 9.9111\n",
      "Epoch 296/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.0032 - mse: 18.0032 - mae: 2.8386\n",
      "Epoch 296: val_loss did not improve from 336.66422\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 14.2261 - mse: 14.2261 - mae: 2.7196 - val_loss: 447.5602 - val_mse: 447.5602 - val_mae: 11.3156\n",
      "Epoch 297/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 18.6523 - mse: 18.6523 - mae: 3.0553\n",
      "Epoch 297: val_loss improved from 336.66422 to 335.97955, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 17.2024 - mse: 17.2024 - mae: 2.8507 - val_loss: 335.9796 - val_mse: 335.9796 - val_mae: 10.2258\n",
      "Epoch 298/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5390 - mse: 10.5390 - mae: 2.3218\n",
      "Epoch 298: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 22.6754 - mse: 22.6754 - mae: 3.2690 - val_loss: 413.5761 - val_mse: 413.5761 - val_mae: 10.8934\n",
      "Epoch 299/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.5346 - mse: 17.5346 - mae: 2.5484\n",
      "Epoch 299: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.8763 - mse: 16.8763 - mae: 2.9706 - val_loss: 384.8170 - val_mse: 384.8170 - val_mae: 10.9911\n",
      "Epoch 300/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.2512 - mse: 6.2512 - mae: 1.8436\n",
      "Epoch 300: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.9369 - mse: 15.9369 - mae: 2.9278 - val_loss: 366.7451 - val_mse: 366.7451 - val_mae: 10.3556\n",
      "Epoch 301/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 24.2622 - mse: 24.2622 - mae: 3.3923\n",
      "Epoch 301: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.8154 - mse: 15.8154 - mae: 2.7212 - val_loss: 403.4055 - val_mse: 403.4055 - val_mae: 11.1595\n",
      "Epoch 302/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1597 - mse: 10.1597 - mae: 2.2149\n",
      "Epoch 302: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.0310 - mse: 13.0310 - mae: 2.3565 - val_loss: 379.9967 - val_mse: 379.9967 - val_mae: 10.7445\n",
      "Epoch 303/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9670 - mse: 10.9670 - mae: 2.4807\n",
      "Epoch 303: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8249 - mse: 11.8249 - mae: 2.1985 - val_loss: 431.1201 - val_mse: 431.1201 - val_mae: 11.3552\n",
      "Epoch 304/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.3563 - mse: 20.3563 - mae: 3.2359\n",
      "Epoch 304: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.8192 - mse: 14.8192 - mae: 2.5275 - val_loss: 343.1221 - val_mse: 343.1221 - val_mae: 10.3641\n",
      "Epoch 305/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.5607 - mse: 20.5607 - mae: 2.5386\n",
      "Epoch 305: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.4464 - mse: 14.4464 - mae: 2.4947 - val_loss: 390.0419 - val_mse: 390.0419 - val_mae: 10.8332\n",
      "Epoch 306/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.1843 - mse: 14.1843 - mae: 2.6886\n",
      "Epoch 306: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.4125 - mse: 12.4125 - mae: 2.4160 - val_loss: 371.9495 - val_mse: 371.9495 - val_mae: 10.9574\n",
      "Epoch 307/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5008 - mse: 4.5008 - mae: 1.7693\n",
      "Epoch 307: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9136 - mse: 11.9136 - mae: 2.2791 - val_loss: 384.1375 - val_mse: 384.1375 - val_mae: 10.5660\n",
      "Epoch 308/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.2949 - mse: 9.2949 - mae: 1.9936\n",
      "Epoch 308: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1809 - mse: 11.1809 - mae: 2.2021 - val_loss: 382.4793 - val_mse: 382.4793 - val_mae: 10.9152\n",
      "Epoch 309/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.6607 - mse: 14.6607 - mae: 2.5711\n",
      "Epoch 309: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.9033 - mse: 10.9033 - mae: 2.1143 - val_loss: 372.7038 - val_mse: 372.7039 - val_mae: 10.5140\n",
      "Epoch 310/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0491 - mse: 11.0491 - mae: 2.3496\n",
      "Epoch 310: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8253 - mse: 11.8253 - mae: 2.2400 - val_loss: 361.9698 - val_mse: 361.9698 - val_mae: 10.4592\n",
      "Epoch 311/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4706 - mse: 6.4706 - mae: 1.8240\n",
      "Epoch 311: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0077 - mse: 12.0077 - mae: 2.2843 - val_loss: 402.1433 - val_mse: 402.1433 - val_mae: 10.9936\n",
      "Epoch 312/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.0847 - mse: 14.0847 - mae: 2.4384\n",
      "Epoch 312: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11.8904 - mse: 11.8904 - mae: 2.3661 - val_loss: 353.5659 - val_mse: 353.5659 - val_mae: 10.4879\n",
      "Epoch 313/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.9979 - mse: 14.9979 - mae: 2.4531\n",
      "Epoch 313: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11.5789 - mse: 11.5789 - mae: 2.1555 - val_loss: 368.0755 - val_mse: 368.0755 - val_mae: 10.5650\n",
      "Epoch 314/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7664 - mse: 8.7664 - mae: 2.4238\n",
      "Epoch 314: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6078 - mse: 11.6078 - mae: 2.3065 - val_loss: 391.2966 - val_mse: 391.2966 - val_mae: 10.9557\n",
      "Epoch 315/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1391 - mse: 9.1391 - mae: 1.7746\n",
      "Epoch 315: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0868 - mse: 11.0868 - mae: 2.1934 - val_loss: 377.6648 - val_mse: 377.6648 - val_mae: 10.6669\n",
      "Epoch 316/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4904 - mse: 8.4904 - mae: 1.6098\n",
      "Epoch 316: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.9399 - mse: 10.9399 - mae: 2.2024 - val_loss: 411.7556 - val_mse: 411.7556 - val_mae: 11.0861\n",
      "Epoch 317/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5970 - mse: 9.5970 - mae: 2.1648\n",
      "Epoch 317: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.2171 - mse: 12.2171 - mae: 2.3736 - val_loss: 346.3564 - val_mse: 346.3564 - val_mae: 10.3094\n",
      "Epoch 318/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.7531 - mse: 8.7531 - mae: 2.0246\n",
      "Epoch 318: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0812 - mse: 14.0812 - mae: 2.5927 - val_loss: 490.4797 - val_mse: 490.4797 - val_mae: 12.5488\n",
      "Epoch 319/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 41.4587 - mse: 41.4587 - mae: 4.6853\n",
      "Epoch 319: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 25.0403 - mse: 25.0403 - mae: 3.5260 - val_loss: 364.6077 - val_mse: 364.6077 - val_mae: 10.9387\n",
      "Epoch 320/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4453 - mse: 4.4453 - mae: 1.5785\n",
      "Epoch 320: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17.8566 - mse: 17.8566 - mae: 2.8686 - val_loss: 347.4438 - val_mse: 347.4438 - val_mae: 9.9898\n",
      "Epoch 321/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.8590 - mse: 15.8590 - mae: 2.8469\n",
      "Epoch 321: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.8108 - mse: 15.8108 - mae: 2.9905 - val_loss: 433.9268 - val_mse: 433.9268 - val_mae: 11.6440\n",
      "Epoch 322/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.4235 - mse: 16.4235 - mae: 3.2855\n",
      "Epoch 322: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.6293 - mse: 14.6293 - mae: 2.7656 - val_loss: 397.6317 - val_mse: 397.6318 - val_mae: 11.0726\n",
      "Epoch 323/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.3237 - mse: 11.3237 - mae: 2.2536\n",
      "Epoch 323: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.4269 - mse: 15.4269 - mae: 2.5367 - val_loss: 336.6789 - val_mse: 336.6789 - val_mae: 10.3035\n",
      "Epoch 324/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3778 - mse: 8.3778 - mae: 2.0172\n",
      "Epoch 324: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1251 - mse: 11.1251 - mae: 2.2124 - val_loss: 435.1536 - val_mse: 435.1536 - val_mae: 11.6136\n",
      "Epoch 325/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4002 - mse: 4.4002 - mae: 1.4089\n",
      "Epoch 325: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.6176 - mse: 12.6176 - mae: 2.4433 - val_loss: 373.1972 - val_mse: 373.1972 - val_mae: 10.5234\n",
      "Epoch 326/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.8067 - mse: 11.8067 - mae: 2.4259\n",
      "Epoch 326: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8628 - mse: 11.8628 - mae: 2.2818 - val_loss: 351.7244 - val_mse: 351.7244 - val_mae: 10.2783\n",
      "Epoch 327/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 19.3807 - mse: 19.3807 - mae: 2.8240\n",
      "Epoch 327: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.7365 - mse: 10.7365 - mae: 2.1155 - val_loss: 393.3463 - val_mse: 393.3464 - val_mae: 10.9729\n",
      "Epoch 328/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.5057 - mse: 13.5057 - mae: 2.5127\n",
      "Epoch 328: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.8312 - mse: 10.8312 - mae: 2.0601 - val_loss: 396.7993 - val_mse: 396.7993 - val_mae: 11.1076\n",
      "Epoch 329/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1799 - mse: 9.1799 - mae: 2.1554\n",
      "Epoch 329: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0493 - mse: 10.0493 - mae: 2.1794 - val_loss: 353.0393 - val_mse: 353.0393 - val_mae: 10.4194\n",
      "Epoch 330/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0913 - mse: 11.0913 - mae: 2.4244\n",
      "Epoch 330: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.9754 - mse: 9.9754 - mae: 2.1941 - val_loss: 419.6649 - val_mse: 419.6649 - val_mae: 11.6383\n",
      "Epoch 331/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.8852 - mse: 17.8852 - mae: 2.9349\n",
      "Epoch 331: val_loss did not improve from 335.97955\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.6678 - mse: 12.6678 - mae: 2.3729 - val_loss: 383.8145 - val_mse: 383.8145 - val_mae: 10.7891\n",
      "Epoch 332/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.6934 - mse: 9.6934 - mae: 2.0638\n",
      "Epoch 332: val_loss improved from 335.97955 to 320.79675, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 11.2264 - mse: 11.2264 - mae: 2.3041 - val_loss: 320.7968 - val_mse: 320.7968 - val_mae: 10.1119\n",
      "Epoch 333/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.7858 - mse: 9.7858 - mae: 2.2849\n",
      "Epoch 333: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.4715 - mse: 14.4715 - mae: 2.4233 - val_loss: 434.5738 - val_mse: 434.5738 - val_mae: 12.0491\n",
      "Epoch 334/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 20.6881 - mse: 20.6881 - mae: 3.0692\n",
      "Epoch 334: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.9989 - mse: 19.9989 - mae: 3.0611 - val_loss: 416.1756 - val_mse: 416.1756 - val_mae: 12.0193\n",
      "Epoch 335/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.3274 - mse: 15.3274 - mae: 3.2831\n",
      "Epoch 335: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.0635 - mse: 18.0635 - mae: 3.2064 - val_loss: 322.6008 - val_mse: 322.6008 - val_mae: 10.0157\n",
      "Epoch 336/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 15.9642 - mse: 15.9642 - mae: 2.4024\n",
      "Epoch 336: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.1558 - mse: 14.1558 - mae: 2.4465 - val_loss: 381.2557 - val_mse: 381.2556 - val_mae: 10.6706\n",
      "Epoch 337/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5314 - mse: 8.5314 - mae: 1.9945\n",
      "Epoch 337: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.2816 - mse: 12.2816 - mae: 2.3854 - val_loss: 391.5787 - val_mse: 391.5787 - val_mae: 11.1534\n",
      "Epoch 338/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5483 - mse: 10.5483 - mae: 2.2203\n",
      "Epoch 338: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2884 - mse: 10.2884 - mae: 2.1130 - val_loss: 362.5489 - val_mse: 362.5489 - val_mae: 10.7038\n",
      "Epoch 339/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.0234 - mse: 3.0234 - mae: 1.2209\n",
      "Epoch 339: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7469 - mse: 9.7469 - mae: 2.0484 - val_loss: 321.7373 - val_mse: 321.7373 - val_mae: 10.2000\n",
      "Epoch 340/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 22.2485 - mse: 22.2485 - mae: 3.1670\n",
      "Epoch 340: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0140 - mse: 12.0140 - mae: 2.2356 - val_loss: 385.1815 - val_mse: 385.1815 - val_mae: 11.2161\n",
      "Epoch 341/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.5446 - mse: 8.5446 - mae: 2.0827\n",
      "Epoch 341: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6446 - mse: 9.6446 - mae: 2.1715 - val_loss: 382.7750 - val_mse: 382.7750 - val_mae: 10.9880\n",
      "Epoch 342/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.8810 - mse: 8.8810 - mae: 2.1630\n",
      "Epoch 342: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7601 - mse: 8.7601 - mae: 2.0268 - val_loss: 359.0554 - val_mse: 359.0554 - val_mae: 10.2749\n",
      "Epoch 343/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6147 - mse: 5.6147 - mae: 1.8000\n",
      "Epoch 343: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5803 - mse: 9.5803 - mae: 2.0713 - val_loss: 374.1540 - val_mse: 374.1540 - val_mae: 10.8440\n",
      "Epoch 344/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1168 - mse: 11.1168 - mae: 2.1383\n",
      "Epoch 344: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.1243 - mse: 10.1243 - mae: 2.1457 - val_loss: 394.2314 - val_mse: 394.2314 - val_mae: 11.0460\n",
      "Epoch 345/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.9830 - mse: 6.9830 - mae: 1.4669\n",
      "Epoch 345: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6580 - mse: 9.6580 - mae: 2.0775 - val_loss: 350.4886 - val_mse: 350.4886 - val_mae: 10.3572\n",
      "Epoch 346/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.1870 - mse: 11.1870 - mae: 1.9251\n",
      "Epoch 346: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7733 - mse: 10.7733 - mae: 2.0867 - val_loss: 403.7413 - val_mse: 403.7413 - val_mae: 11.2636\n",
      "Epoch 347/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5108 - mse: 10.5108 - mae: 2.1482\n",
      "Epoch 347: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5411 - mse: 10.5411 - mae: 2.2764 - val_loss: 355.0146 - val_mse: 355.0146 - val_mae: 10.4054\n",
      "Epoch 348/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4887 - mse: 5.4887 - mae: 1.4474\n",
      "Epoch 348: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.8685 - mse: 8.8685 - mae: 2.0874 - val_loss: 369.8526 - val_mse: 369.8526 - val_mae: 10.6123\n",
      "Epoch 349/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.6422 - mse: 5.6422 - mae: 1.5485\n",
      "Epoch 349: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9438 - mse: 8.9438 - mae: 2.0678 - val_loss: 376.6959 - val_mse: 376.6959 - val_mae: 11.0575\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5442 - mse: 4.5442 - mae: 1.4215\n",
      "Epoch 350: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.5075 - mse: 8.5075 - mae: 2.0301 - val_loss: 348.7394 - val_mse: 348.7394 - val_mae: 10.3712\n",
      "Epoch 351/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.9049 - mse: 10.9049 - mae: 2.3393\n",
      "Epoch 351: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0818 - mse: 10.0818 - mae: 2.1503 - val_loss: 367.8546 - val_mse: 367.8546 - val_mae: 10.8018\n",
      "Epoch 352/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3338 - mse: 9.3338 - mae: 1.9463\n",
      "Epoch 352: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7121 - mse: 8.7121 - mae: 1.8895 - val_loss: 401.7550 - val_mse: 401.7550 - val_mae: 11.0270\n",
      "Epoch 353/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.0476 - mse: 11.0476 - mae: 2.2865\n",
      "Epoch 353: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2201 - mse: 9.2201 - mae: 2.0511 - val_loss: 327.0490 - val_mse: 327.0490 - val_mae: 10.3214\n",
      "Epoch 354/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4611 - mse: 3.4611 - mae: 1.1543\n",
      "Epoch 354: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.3624 - mse: 11.3624 - mae: 2.2480 - val_loss: 376.2339 - val_mse: 376.2339 - val_mae: 10.5748\n",
      "Epoch 355/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.7605 - mse: 13.7605 - mae: 2.6351\n",
      "Epoch 355: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7683 - mse: 9.7683 - mae: 2.2065 - val_loss: 381.5136 - val_mse: 381.5136 - val_mae: 11.3408\n",
      "Epoch 356/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9992 - mse: 9.9992 - mae: 2.5091\n",
      "Epoch 356: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8153 - mse: 8.8153 - mae: 2.1259 - val_loss: 342.6679 - val_mse: 342.6679 - val_mae: 10.3472\n",
      "Epoch 357/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.7028 - mse: 10.7028 - mae: 2.5200\n",
      "Epoch 357: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7295 - mse: 9.7295 - mae: 2.1714 - val_loss: 357.7025 - val_mse: 357.7025 - val_mae: 10.9585\n",
      "Epoch 358/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8408 - mse: 9.8408 - mae: 2.3770\n",
      "Epoch 358: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9472 - mse: 8.9472 - mae: 1.9946 - val_loss: 403.3421 - val_mse: 403.3421 - val_mae: 10.9604\n",
      "Epoch 359/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4995 - mse: 5.4995 - mae: 1.6728\n",
      "Epoch 359: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9322 - mse: 8.9322 - mae: 1.9874 - val_loss: 330.4673 - val_mse: 330.4673 - val_mae: 10.6921\n",
      "Epoch 360/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.3762 - mse: 8.3762 - mae: 2.0665\n",
      "Epoch 360: val_loss did not improve from 320.79675\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7857 - mse: 8.7857 - mae: 2.0651 - val_loss: 385.8842 - val_mse: 385.8842 - val_mae: 10.9409\n",
      "Epoch 361/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3781 - mse: 5.3781 - mae: 1.4641\n",
      "Epoch 361: val_loss improved from 320.79675 to 313.06064, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 10.4154 - mse: 10.4154 - mae: 2.1218 - val_loss: 313.0606 - val_mse: 313.0606 - val_mae: 10.0672\n",
      "Epoch 362/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 35.5545 - mse: 35.5545 - mae: 3.7272\n",
      "Epoch 362: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 16.8907 - mse: 16.8907 - mae: 2.6286 - val_loss: 390.8047 - val_mse: 390.8047 - val_mae: 10.5025\n",
      "Epoch 363/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.2877 - mse: 11.2877 - mae: 2.6827\n",
      "Epoch 363: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11.9206 - mse: 11.9206 - mae: 2.5890 - val_loss: 391.7148 - val_mse: 391.7148 - val_mae: 11.8109\n",
      "Epoch 364/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 17.5421 - mse: 17.5421 - mae: 3.3379\n",
      "Epoch 364: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8298 - mse: 11.8298 - mae: 2.6851 - val_loss: 356.0864 - val_mse: 356.0864 - val_mae: 10.6877\n",
      "Epoch 365/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0404 - mse: 9.0404 - mae: 2.4142\n",
      "Epoch 365: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.4015 - mse: 10.4015 - mae: 2.3947 - val_loss: 362.7083 - val_mse: 362.7083 - val_mae: 10.5311\n",
      "Epoch 366/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1194 - mse: 8.1194 - mae: 1.7384\n",
      "Epoch 366: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.7367 - mse: 10.7367 - mae: 2.1659 - val_loss: 381.6718 - val_mse: 381.6718 - val_mae: 10.8334\n",
      "Epoch 367/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.2577 - mse: 13.2577 - mae: 2.4337\n",
      "Epoch 367: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3987 - mse: 7.3987 - mae: 1.8226 - val_loss: 358.9206 - val_mse: 358.9206 - val_mae: 10.4340\n",
      "Epoch 368/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.0060 - mse: 7.0060 - mae: 1.8171\n",
      "Epoch 368: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6139 - mse: 7.6139 - mae: 1.8388 - val_loss: 354.7986 - val_mse: 354.7986 - val_mae: 10.5146\n",
      "Epoch 369/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7529 - mse: 3.7529 - mae: 1.3899\n",
      "Epoch 369: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6660 - mse: 7.6660 - mae: 1.8033 - val_loss: 401.3806 - val_mse: 401.3806 - val_mae: 11.6247\n",
      "Epoch 370/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14.6484 - mse: 14.6484 - mae: 2.7289\n",
      "Epoch 370: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8090 - mse: 8.8090 - mae: 2.0529 - val_loss: 370.8244 - val_mse: 370.8244 - val_mae: 10.7441\n",
      "Epoch 371/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.6747 - mse: 8.6747 - mae: 1.9520\n",
      "Epoch 371: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.0116 - mse: 8.0116 - mae: 1.9199 - val_loss: 339.4202 - val_mse: 339.4202 - val_mae: 10.1217\n",
      "Epoch 372/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4132 - mse: 5.4132 - mae: 1.8152\n",
      "Epoch 372: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9.2040 - mse: 9.2040 - mae: 2.0716 - val_loss: 379.2245 - val_mse: 379.2245 - val_mae: 11.0250\n",
      "Epoch 373/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8625 - mse: 7.8625 - mae: 1.8111\n",
      "Epoch 373: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.8696 - mse: 10.8696 - mae: 2.0587 - val_loss: 420.7337 - val_mse: 420.7337 - val_mae: 11.7023\n",
      "Epoch 374/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1772 - mse: 13.1772 - mae: 2.3188\n",
      "Epoch 374: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9.1107 - mse: 9.1107 - mae: 1.9633 - val_loss: 337.4112 - val_mse: 337.4112 - val_mae: 10.4010\n",
      "Epoch 375/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 11.7379 - mse: 11.7379 - mae: 2.3774\n",
      "Epoch 375: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.0717 - mse: 8.0717 - mae: 1.8725 - val_loss: 321.9466 - val_mse: 321.9466 - val_mae: 10.1074\n",
      "Epoch 376/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9689 - mse: 9.9689 - mae: 2.2132\n",
      "Epoch 376: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11.2896 - mse: 11.2896 - mae: 2.2749 - val_loss: 386.3700 - val_mse: 386.3700 - val_mae: 11.2024\n",
      "Epoch 377/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8320 - mse: 6.8320 - mae: 1.9907\n",
      "Epoch 377: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7.3353 - mse: 7.3353 - mae: 2.0716 - val_loss: 327.8246 - val_mse: 327.8246 - val_mae: 10.2979\n",
      "Epoch 378/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 12.0395 - mse: 12.0395 - mae: 2.0736\n",
      "Epoch 378: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.6347 - mse: 6.6347 - mae: 1.7587 - val_loss: 352.4898 - val_mse: 352.4898 - val_mae: 10.6026\n",
      "Epoch 379/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5384 - mse: 4.5384 - mae: 1.3563\n",
      "Epoch 379: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.9537 - mse: 6.9537 - mae: 1.8000 - val_loss: 344.6247 - val_mse: 344.6247 - val_mae: 10.6013\n",
      "Epoch 380/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.8735 - mse: 3.8735 - mae: 1.4703\n",
      "Epoch 380: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.7528 - mse: 6.7528 - mae: 1.7383 - val_loss: 375.0278 - val_mse: 375.0278 - val_mae: 10.8849\n",
      "Epoch 381/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1136 - mse: 10.1136 - mae: 1.9406\n",
      "Epoch 381: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.3912 - mse: 8.3912 - mae: 1.9911 - val_loss: 370.1318 - val_mse: 370.1319 - val_mae: 11.0301\n",
      "Epoch 382/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.0373 - mse: 6.0373 - mae: 1.6368\n",
      "Epoch 382: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.4955 - mse: 6.4955 - mae: 1.7520 - val_loss: 374.0544 - val_mse: 374.0544 - val_mae: 10.7294\n",
      "Epoch 383/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.8056 - mse: 5.8056 - mae: 1.5455\n",
      "Epoch 383: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.1435 - mse: 6.1435 - mae: 1.7040 - val_loss: 314.6808 - val_mse: 314.6808 - val_mae: 10.0954\n",
      "Epoch 384/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.0824 - mse: 8.0824 - mae: 2.1173\n",
      "Epoch 384: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 9.2528 - mse: 9.2528 - mae: 2.0610 - val_loss: 384.0879 - val_mse: 384.0879 - val_mae: 11.0427\n",
      "Epoch 385/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9289 - mse: 2.9289 - mae: 1.1752\n",
      "Epoch 385: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.9257 - mse: 6.9257 - mae: 1.7704 - val_loss: 379.8879 - val_mse: 379.8879 - val_mae: 11.3398\n",
      "Epoch 386/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6533 - mse: 10.6533 - mae: 2.1843\n",
      "Epoch 386: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.5147 - mse: 6.5147 - mae: 1.7020 - val_loss: 352.1534 - val_mse: 352.1534 - val_mae: 10.4755\n",
      "Epoch 387/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6121 - mse: 6.6121 - mae: 1.6329\n",
      "Epoch 387: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.2070 - mse: 6.2070 - mae: 1.6801 - val_loss: 350.9366 - val_mse: 350.9366 - val_mae: 10.4617\n",
      "Epoch 388/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0141 - mse: 9.0141 - mae: 1.8619\n",
      "Epoch 388: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.5667 - mse: 6.5667 - mae: 1.6848 - val_loss: 357.4338 - val_mse: 357.4338 - val_mae: 10.6947\n",
      "Epoch 389/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5734 - mse: 4.5734 - mae: 1.2764\n",
      "Epoch 389: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4521 - mse: 6.4521 - mae: 1.7613 - val_loss: 386.1091 - val_mse: 386.1092 - val_mae: 11.0588\n",
      "Epoch 390/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6534 - mse: 3.6534 - mae: 1.1793\n",
      "Epoch 390: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.1386 - mse: 6.1386 - mae: 1.6669 - val_loss: 347.0512 - val_mse: 347.0512 - val_mae: 10.4904\n",
      "Epoch 391/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.1682 - mse: 9.1682 - mae: 1.9330\n",
      "Epoch 391: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.8442 - mse: 5.8442 - mae: 1.6181 - val_loss: 352.0781 - val_mse: 352.0781 - val_mae: 10.3918\n",
      "Epoch 392/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3228 - mse: 3.3228 - mae: 1.2981\n",
      "Epoch 392: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.8650 - mse: 5.8650 - mae: 1.6185 - val_loss: 401.7930 - val_mse: 401.7930 - val_mae: 11.5123\n",
      "Epoch 393/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.1175 - mse: 4.1175 - mae: 1.6261\n",
      "Epoch 393: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.5027 - mse: 8.5027 - mae: 2.1191 - val_loss: 340.6252 - val_mse: 340.6252 - val_mae: 10.3625\n",
      "Epoch 394/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.6957 - mse: 6.6957 - mae: 1.8092\n",
      "Epoch 394: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7.6267 - mse: 7.6267 - mae: 1.9358 - val_loss: 349.2764 - val_mse: 349.2764 - val_mae: 10.4301\n",
      "Epoch 395/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.4502 - mse: 9.4502 - mae: 2.2668\n",
      "Epoch 395: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5441 - mse: 7.5441 - mae: 1.8261 - val_loss: 348.5435 - val_mse: 348.5435 - val_mae: 10.5834\n",
      "Epoch 396/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9156 - mse: 2.9156 - mae: 1.2381\n",
      "Epoch 396: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7.3287 - mse: 7.3287 - mae: 1.7452 - val_loss: 411.5155 - val_mse: 411.5155 - val_mae: 11.1460\n",
      "Epoch 397/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.7373 - mse: 7.7373 - mae: 1.6331\n",
      "Epoch 397: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 8.5539 - mse: 8.5539 - mae: 2.0256 - val_loss: 349.4626 - val_mse: 349.4626 - val_mae: 10.6976\n",
      "Epoch 398/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6737 - mse: 3.6737 - mae: 1.4163\n",
      "Epoch 398: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.8723 - mse: 6.8723 - mae: 1.8645 - val_loss: 358.2837 - val_mse: 358.2837 - val_mae: 10.6348\n",
      "Epoch 399/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5386 - mse: 9.5386 - mae: 2.2965\n",
      "Epoch 399: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.8287 - mse: 6.8287 - mae: 1.8644 - val_loss: 348.2068 - val_mse: 348.2068 - val_mae: 10.5655\n",
      "Epoch 400/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.0199 - mse: 4.0199 - mae: 1.3366\n",
      "Epoch 400: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.2987 - mse: 5.2987 - mae: 1.5303 - val_loss: 414.5516 - val_mse: 414.5516 - val_mae: 11.3423\n",
      "Epoch 401/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.5000 - mse: 10.5000 - mae: 2.0776\n",
      "Epoch 401: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4436 - mse: 7.4436 - mae: 1.7990 - val_loss: 347.2408 - val_mse: 347.2408 - val_mae: 10.5792\n",
      "Epoch 402/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.4974 - mse: 5.4974 - mae: 1.4905\n",
      "Epoch 402: val_loss did not improve from 313.06064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6036 - mse: 6.6036 - mae: 1.7153 - val_loss: 350.7351 - val_mse: 350.7351 - val_mae: 10.5389\n",
      "Epoch 403/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2937 - mse: 4.2937 - mae: 1.5272\n",
      "Epoch 403: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.0930 - mse: 6.0930 - mae: 1.6594 - val_loss: 350.7464 - val_mse: 350.7464 - val_mae: 10.6313\n",
      "Epoch 404/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9444 - mse: 4.9444 - mae: 1.2330\n",
      "Epoch 404: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8801 - mse: 4.8801 - mae: 1.4372 - val_loss: 386.8202 - val_mse: 386.8202 - val_mae: 10.9972\n",
      "Epoch 405/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.4285 - mse: 4.4285 - mae: 1.4705\n",
      "Epoch 405: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4166 - mse: 6.4166 - mae: 1.6771 - val_loss: 363.9049 - val_mse: 363.9049 - val_mae: 10.5903\n",
      "Epoch 406/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7756 - mse: 4.7756 - mae: 1.3322\n",
      "Epoch 406: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3520 - mse: 5.3520 - mae: 1.5268 - val_loss: 354.0121 - val_mse: 354.0121 - val_mae: 10.5134\n",
      "Epoch 407/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.0385 - mse: 9.0385 - mae: 1.9218\n",
      "Epoch 407: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0011 - mse: 5.0011 - mae: 1.4871 - val_loss: 333.9642 - val_mse: 333.9642 - val_mae: 10.3250\n",
      "Epoch 408/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3551 - mse: 5.3551 - mae: 1.4601\n",
      "Epoch 408: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8464 - mse: 7.8464 - mae: 2.0047 - val_loss: 393.6212 - val_mse: 393.6212 - val_mae: 11.1399\n",
      "Epoch 409/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.3485 - mse: 7.3485 - mae: 1.7511\n",
      "Epoch 409: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5094 - mse: 7.5094 - mae: 1.8803 - val_loss: 388.3990 - val_mse: 388.3990 - val_mae: 11.5390\n",
      "Epoch 410/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.4304 - mse: 7.4304 - mae: 2.1365\n",
      "Epoch 410: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.3008 - mse: 11.3008 - mae: 2.5200 - val_loss: 336.3459 - val_mse: 336.3459 - val_mae: 10.3408\n",
      "Epoch 411/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.1461 - mse: 8.1461 - mae: 1.9812\n",
      "Epoch 411: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6433 - mse: 6.6433 - mae: 1.8177 - val_loss: 329.9715 - val_mse: 329.9715 - val_mae: 10.5004\n",
      "Epoch 412/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.8987 - mse: 2.8987 - mae: 1.3537\n",
      "Epoch 412: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.1974 - mse: 9.1974 - mae: 1.9697 - val_loss: 405.0710 - val_mse: 405.0710 - val_mae: 11.0803\n",
      "Epoch 413/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.1445 - mse: 7.1445 - mae: 1.9769\n",
      "Epoch 413: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.0572 - mse: 7.0572 - mae: 1.8623 - val_loss: 396.8614 - val_mse: 396.8614 - val_mae: 11.3717\n",
      "Epoch 414/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.1832 - mse: 13.1832 - mae: 2.8987\n",
      "Epoch 414: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.6656 - mse: 6.6656 - mae: 1.8246 - val_loss: 406.9416 - val_mse: 406.9416 - val_mae: 10.9772\n",
      "Epoch 415/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.5338 - mse: 5.5338 - mae: 1.4970\n",
      "Epoch 415: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.4559 - mse: 7.4559 - mae: 1.7945 - val_loss: 328.2445 - val_mse: 328.2445 - val_mae: 10.4349\n",
      "Epoch 416/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.6893 - mse: 10.6893 - mae: 2.4632\n",
      "Epoch 416: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7603 - mse: 6.7603 - mae: 1.8554 - val_loss: 344.4678 - val_mse: 344.4678 - val_mae: 10.5678\n",
      "Epoch 417/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.6934 - mse: 7.6934 - mae: 2.1421\n",
      "Epoch 417: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.8574 - mse: 5.8574 - mae: 1.8036 - val_loss: 339.4627 - val_mse: 339.4627 - val_mae: 11.0378\n",
      "Epoch 418/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.2774 - mse: 4.2774 - mae: 1.6305\n",
      "Epoch 418: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.9554 - mse: 5.9554 - mae: 1.6955 - val_loss: 376.5743 - val_mse: 376.5743 - val_mae: 10.9033\n",
      "Epoch 419/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.5527 - mse: 9.5527 - mae: 1.9731\n",
      "Epoch 419: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 7.0557 - mse: 7.0557 - mae: 1.8991 - val_loss: 335.0463 - val_mse: 335.0463 - val_mae: 10.8435\n",
      "Epoch 420/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2454 - mse: 3.2454 - mae: 1.4540\n",
      "Epoch 420: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6.2941 - mse: 6.2941 - mae: 1.8218 - val_loss: 355.0769 - val_mse: 355.0769 - val_mae: 10.5803\n",
      "Epoch 421/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.8314 - mse: 7.8314 - mae: 1.8492\n",
      "Epoch 421: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.9390 - mse: 4.9390 - mae: 1.5546 - val_loss: 338.3405 - val_mse: 338.3405 - val_mae: 10.8187\n",
      "Epoch 422/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.5352 - mse: 4.5352 - mae: 1.4939\n",
      "Epoch 422: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 5.3406 - mse: 5.3406 - mae: 1.5589 - val_loss: 367.2297 - val_mse: 367.2297 - val_mae: 10.6848\n",
      "Epoch 423/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.9382 - mse: 5.9382 - mae: 1.6840\n",
      "Epoch 423: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 4.9268 - mse: 4.9268 - mae: 1.4816 - val_loss: 365.9429 - val_mse: 365.9429 - val_mae: 10.9854\n",
      "Epoch 424/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9079 - mse: 4.9079 - mae: 1.6600\n",
      "Epoch 424: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.2460 - mse: 4.2460 - mae: 1.4065 - val_loss: 350.1415 - val_mse: 350.1415 - val_mae: 10.4575\n",
      "Epoch 425/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.2455 - mse: 3.2455 - mae: 1.3241\n",
      "Epoch 425: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0823 - mse: 5.0823 - mae: 1.4921 - val_loss: 334.7955 - val_mse: 334.7955 - val_mae: 10.3342\n",
      "Epoch 426/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.2859 - mse: 2.2859 - mae: 1.1445\n",
      "Epoch 426: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.6970 - mse: 4.6970 - mae: 1.5566 - val_loss: 396.7731 - val_mse: 396.7731 - val_mae: 11.1970\n",
      "Epoch 427/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.3684 - mse: 6.3684 - mae: 1.8936\n",
      "Epoch 427: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0164 - mse: 5.0164 - mae: 1.5218 - val_loss: 367.5266 - val_mse: 367.5266 - val_mae: 10.6708\n",
      "Epoch 428/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.7829 - mse: 3.7829 - mae: 1.3679\n",
      "Epoch 428: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6735 - mse: 4.6735 - mae: 1.4277 - val_loss: 347.7246 - val_mse: 347.7246 - val_mae: 10.5316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 429/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1.4786 - mse: 1.4786 - mae: 0.8869\n",
      "Epoch 429: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.2273 - mse: 4.2273 - mae: 1.3222 - val_loss: 335.3195 - val_mse: 335.3195 - val_mae: 10.3207\n",
      "Epoch 430/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.4306 - mse: 6.4306 - mae: 1.8572\n",
      "Epoch 430: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7510 - mse: 4.7510 - mae: 1.5450 - val_loss: 396.0068 - val_mse: 396.0068 - val_mae: 11.7398\n",
      "Epoch 431/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1274 - mse: 5.1274 - mae: 1.9716\n",
      "Epoch 431: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7795 - mse: 5.7795 - mae: 1.7792 - val_loss: 360.3118 - val_mse: 360.3118 - val_mae: 10.7023\n",
      "Epoch 432/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.3502 - mse: 9.3502 - mae: 2.0623\n",
      "Epoch 432: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5580 - mse: 5.5580 - mae: 1.5473 - val_loss: 315.4216 - val_mse: 315.4216 - val_mae: 9.9970\n",
      "Epoch 433/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.9765 - mse: 16.9765 - mae: 2.7813\n",
      "Epoch 433: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4406 - mse: 7.4406 - mae: 1.7497 - val_loss: 369.2048 - val_mse: 369.2049 - val_mae: 10.5484\n",
      "Epoch 434/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5014 - mse: 3.5014 - mae: 1.5383\n",
      "Epoch 434: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4224 - mse: 5.4224 - mae: 1.5821 - val_loss: 363.3633 - val_mse: 363.3633 - val_mae: 10.8131\n",
      "Epoch 435/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.2259 - mse: 5.2259 - mae: 1.5359\n",
      "Epoch 435: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5011 - mse: 4.5011 - mae: 1.5061 - val_loss: 345.3731 - val_mse: 345.3731 - val_mae: 10.2793\n",
      "Epoch 436/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.9912 - mse: 9.9912 - mae: 1.9128\n",
      "Epoch 436: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8759 - mse: 4.8759 - mae: 1.4159 - val_loss: 327.7035 - val_mse: 327.7035 - val_mae: 10.2411\n",
      "Epoch 437/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1268 - mse: 5.1268 - mae: 1.6441\n",
      "Epoch 437: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.7970 - mse: 6.7970 - mae: 1.8024 - val_loss: 366.3193 - val_mse: 366.3192 - val_mae: 10.8564\n",
      "Epoch 438/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.8166 - mse: 6.8166 - mae: 1.8502\n",
      "Epoch 438: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7067 - mse: 6.7067 - mae: 1.8420 - val_loss: 385.5561 - val_mse: 385.5561 - val_mae: 11.1537\n",
      "Epoch 439/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.2262 - mse: 7.2262 - mae: 1.8782\n",
      "Epoch 439: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.8856 - mse: 5.8856 - mae: 1.7204 - val_loss: 363.1104 - val_mse: 363.1104 - val_mae: 10.7046\n",
      "Epoch 440/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.5989 - mse: 2.5989 - mae: 1.1658\n",
      "Epoch 440: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5377 - mse: 5.5377 - mae: 1.6662 - val_loss: 365.2730 - val_mse: 365.2730 - val_mae: 10.8510\n",
      "Epoch 441/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7999 - mse: 4.7999 - mae: 1.3994\n",
      "Epoch 441: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4790 - mse: 5.4790 - mae: 1.6484 - val_loss: 326.9164 - val_mse: 326.9164 - val_mae: 10.4362\n",
      "Epoch 442/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.4882 - mse: 3.4882 - mae: 1.2068\n",
      "Epoch 442: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.2904 - mse: 5.2904 - mae: 1.6064 - val_loss: 433.7190 - val_mse: 433.7191 - val_mae: 10.9780\n",
      "Epoch 443/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1042 - mse: 10.1042 - mae: 1.6973\n",
      "Epoch 443: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9755 - mse: 11.9755 - mae: 2.1350 - val_loss: 352.6677 - val_mse: 352.6677 - val_mae: 11.9114\n",
      "Epoch 444/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 16.3731 - mse: 16.3731 - mae: 3.2822\n",
      "Epoch 444: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0171 - mse: 11.0171 - mae: 2.5184 - val_loss: 346.4404 - val_mse: 346.4404 - val_mae: 10.6968\n",
      "Epoch 445/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 13.9409 - mse: 13.9409 - mae: 3.3201\n",
      "Epoch 445: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.3555 - mse: 13.3555 - mae: 2.9054 - val_loss: 321.3109 - val_mse: 321.3109 - val_mae: 10.4362\n",
      "Epoch 446/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 10.1717 - mse: 10.1717 - mae: 2.3520\n",
      "Epoch 446: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0523 - mse: 7.0523 - mae: 1.8345 - val_loss: 374.1864 - val_mse: 374.1864 - val_mae: 11.1543\n",
      "Epoch 447/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.6840 - mse: 3.6840 - mae: 1.2855\n",
      "Epoch 447: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4109 - mse: 6.4109 - mae: 1.6863 - val_loss: 331.1700 - val_mse: 331.1700 - val_mae: 10.6172\n",
      "Epoch 448/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.7630 - mse: 4.7630 - mae: 1.3702\n",
      "Epoch 448: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.1746 - mse: 5.1746 - mae: 1.5560 - val_loss: 377.8977 - val_mse: 377.8977 - val_mae: 10.6696\n",
      "Epoch 449/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.1271 - mse: 5.1271 - mae: 1.4577\n",
      "Epoch 449: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.1697 - mse: 5.1697 - mae: 1.5704 - val_loss: 341.3045 - val_mse: 341.3045 - val_mae: 10.6921\n",
      "Epoch 450/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 8.4286 - mse: 8.4286 - mae: 2.1723\n",
      "Epoch 450: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.0461 - mse: 6.0461 - mae: 1.7319 - val_loss: 355.4695 - val_mse: 355.4695 - val_mae: 10.5282\n",
      "Epoch 451/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.9916 - mse: 4.9916 - mae: 1.8662\n",
      "Epoch 451: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7266 - mse: 5.7266 - mae: 1.8720 - val_loss: 352.5759 - val_mse: 352.5759 - val_mae: 11.1201\n",
      "Epoch 452/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 7.5495 - mse: 7.5495 - mae: 2.3309\n",
      "Epoch 452: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0507 - mse: 5.0507 - mae: 1.6069 - val_loss: 359.2359 - val_mse: 359.2359 - val_mae: 10.7569\n",
      "Epoch 453/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.3491 - mse: 3.3491 - mae: 1.5578\n",
      "Epoch 453: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1003 - mse: 4.1003 - mae: 1.4277 - val_loss: 318.2733 - val_mse: 318.2733 - val_mae: 10.4312\n",
      "Epoch 454/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3.5541 - mse: 3.5541 - mae: 1.1980\n",
      "Epoch 454: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9999 - mse: 4.9999 - mae: 1.4557 - val_loss: 364.2064 - val_mse: 364.2064 - val_mae: 10.7520\n",
      "Epoch 455/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.3817 - mse: 4.3817 - mae: 1.4192\n",
      "Epoch 455: val_loss did not improve from 313.06064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5289 - mse: 3.5289 - mae: 1.3060 - val_loss: 330.3181 - val_mse: 330.3181 - val_mae: 10.3551\n",
      "Epoch 456/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2.9546 - mse: 2.9546 - mae: 1.2191\n",
      "Epoch 456: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5158 - mse: 5.5158 - mae: 1.6037 - val_loss: 341.9641 - val_mse: 341.9641 - val_mae: 10.2951\n",
      "Epoch 457/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6699 - mse: 4.6699 - mae: 1.7556\n",
      "Epoch 457: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.4439 - mse: 5.4439 - mae: 1.7048 - val_loss: 367.7351 - val_mse: 367.7351 - val_mae: 11.7413\n",
      "Epoch 458/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4.6679 - mse: 4.6679 - mae: 1.4703\n",
      "Epoch 458: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5222 - mse: 5.5222 - mae: 1.6493 - val_loss: 394.4278 - val_mse: 394.4278 - val_mae: 11.1102\n",
      "Epoch 459/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9.8200 - mse: 9.8200 - mae: 1.8849\n",
      "Epoch 459: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.0510 - mse: 8.0510 - mae: 1.8445 - val_loss: 315.5732 - val_mse: 315.5732 - val_mae: 10.2089\n",
      "Epoch 460/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5.3752 - mse: 5.3752 - mae: 1.6721\n",
      "Epoch 460: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0500 - mse: 7.0500 - mae: 1.8792 - val_loss: 340.6214 - val_mse: 340.6214 - val_mae: 10.4564\n",
      "Epoch 461/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 6.5041 - mse: 6.5041 - mae: 2.1055\n",
      "Epoch 461: val_loss did not improve from 313.06064\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0009 - mse: 5.0009 - mae: 1.6457 - val_loss: 355.6279 - val_mse: 355.6279 - val_mae: 10.6751\n"
     ]
    }
   ],
   "source": [
    "hist3 = model3.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val), callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 89ms/step\n",
      "예측값 :  [158.73125]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "model3.load_weights(ckpt_name)\n",
    "\n",
    "preds = model3.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cbee57b50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i for i in range(len(preds))]\n",
    "plt.plot(x_axis,preds,label='preds')\n",
    "plt.plot(x_axis,Y_val,label='labels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9825695427189226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model3.load_weights(ckpt_name)을 통해 가장 학습이 잘 된 상태의 모델을 불러와 향상된 R2 값을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLy-C-0wBNT1"
   },
   "source": [
    "### 1.4 Dropout 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiUFhDQxd8a1"
   },
   "source": [
    "Dropout을 적용해 모델의 오버피팅을 완화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ABxqaT1udU0U"
   },
   "outputs": [],
   "source": [
    "model4 = tf.keras.Sequential()\n",
    "\n",
    "dropout_layer = tf.keras.layers.Dropout(0.1, input_shape=(2,)) # 0.2는 dropout이 걸릴 확률\n",
    "\n",
    "model4.add(tf.keras.Input(shape = 3))\n",
    "model4.add(Dense(100))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(dropout_layer)\n",
    "model4.add(Dense(200))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(dropout_layer)\n",
    "model4.add(Dense(300))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(dropout_layer)\n",
    "model4.add(Dense(200))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(dropout_layer)\n",
    "model4.add(Dense(100))\n",
    "model4.add(Activation('relu'))\n",
    "model4.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BxJ0zCIdUxO",
    "outputId": "f70d2821-f098-447b-d1ca-b586fbc84131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 100)               400       \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 300)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 46ms/step - loss: 22777.7461 - mse: 22777.7461 - mae: 100.8619 - val_loss: 31290.3691 - val_mse: 31290.3691 - val_mae: 118.0773\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14503.6016 - mse: 14503.6016 - mae: 72.8512 - val_loss: 9696.1885 - val_mse: 9696.1885 - val_mae: 73.2404\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7351.9360 - mse: 7351.9360 - mae: 68.2338 - val_loss: 8067.3999 - val_mse: 8067.3999 - val_mae: 41.0097\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3180.7366 - mse: 3180.7366 - mae: 29.7595 - val_loss: 3326.6450 - val_mse: 3326.6450 - val_mae: 33.6670\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2749.9006 - mse: 2749.9006 - mae: 35.6237 - val_loss: 2878.5132 - val_mse: 2878.5132 - val_mae: 33.0265\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1688.2661 - mse: 1688.2661 - mae: 25.8926 - val_loss: 4163.7808 - val_mse: 4163.7808 - val_mae: 30.7577\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1612.2390 - mse: 1612.2390 - mae: 25.9229 - val_loss: 2574.3027 - val_mse: 2574.3027 - val_mae: 34.0879\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2156.7659 - mse: 2156.7659 - mae: 31.2083 - val_loss: 2558.4636 - val_mse: 2558.4636 - val_mae: 30.5183\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1720.4349 - mse: 1720.4349 - mae: 26.6762 - val_loss: 3368.2756 - val_mse: 3368.2756 - val_mae: 27.4628\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1200.2584 - mse: 1200.2584 - mae: 23.8825 - val_loss: 2566.9919 - val_mse: 2566.9919 - val_mae: 28.4167\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1569.7733 - mse: 1569.7733 - mae: 26.3807 - val_loss: 2729.2837 - val_mse: 2729.2837 - val_mae: 25.6425\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1129.8640 - mse: 1129.8640 - mae: 22.5566 - val_loss: 2681.2854 - val_mse: 2681.2854 - val_mae: 24.4818\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1386.2842 - mse: 1386.2842 - mae: 21.2286 - val_loss: 2328.0486 - val_mse: 2328.0486 - val_mae: 25.1921\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1094.1356 - mse: 1094.1356 - mae: 20.6843 - val_loss: 2155.0586 - val_mse: 2155.0586 - val_mae: 25.7943\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1183.8451 - mse: 1183.8451 - mae: 22.5230 - val_loss: 2246.9106 - val_mse: 2246.9106 - val_mae: 23.8220\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1378.4160 - mse: 1378.4160 - mae: 22.0242 - val_loss: 2408.6943 - val_mse: 2408.6943 - val_mae: 22.9786\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1128.9956 - mse: 1128.9956 - mae: 20.0548 - val_loss: 2498.5869 - val_mse: 2498.5869 - val_mae: 23.0238\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 978.0346 - mse: 978.0346 - mae: 18.6635 - val_loss: 1842.9417 - val_mse: 1842.9417 - val_mae: 23.7767\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1042.9088 - mse: 1042.9088 - mae: 20.8303 - val_loss: 2173.6685 - val_mse: 2173.6685 - val_mae: 22.1587\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1319.2087 - mse: 1319.2087 - mae: 18.8391 - val_loss: 2130.3853 - val_mse: 2130.3853 - val_mae: 22.5134\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1057.3484 - mse: 1057.3484 - mae: 19.9114 - val_loss: 1915.1698 - val_mse: 1915.1698 - val_mae: 22.2654\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1124.3337 - mse: 1124.3337 - mae: 19.6277 - val_loss: 2042.2092 - val_mse: 2042.2092 - val_mae: 21.4625\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 830.9842 - mse: 830.9842 - mae: 17.8872 - val_loss: 1951.3832 - val_mse: 1951.3832 - val_mae: 21.1656\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1148.0096 - mse: 1148.0096 - mae: 21.4397 - val_loss: 1679.2722 - val_mse: 1679.2722 - val_mae: 21.4570\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 940.5474 - mse: 940.5474 - mae: 19.7848 - val_loss: 2641.4565 - val_mse: 2641.4565 - val_mae: 25.0442\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1057.7961 - mse: 1057.7961 - mae: 19.1908 - val_loss: 1597.5181 - val_mse: 1597.5181 - val_mae: 20.9086\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1128.2520 - mse: 1128.2520 - mae: 21.0829 - val_loss: 1709.8138 - val_mse: 1709.8138 - val_mae: 20.5188\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 815.1419 - mse: 815.1419 - mae: 17.9774 - val_loss: 2776.8389 - val_mse: 2776.8389 - val_mae: 26.2444\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1022.9871 - mse: 1022.9871 - mae: 19.2125 - val_loss: 1633.2618 - val_mse: 1633.2618 - val_mae: 19.7963\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 845.6996 - mse: 845.6996 - mae: 17.8432 - val_loss: 1927.1837 - val_mse: 1927.1837 - val_mae: 20.1786\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 798.7219 - mse: 798.7219 - mae: 17.1785 - val_loss: 1580.2328 - val_mse: 1580.2328 - val_mae: 18.8560\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1082.2600 - mse: 1082.2600 - mae: 18.7362 - val_loss: 1645.6354 - val_mse: 1645.6354 - val_mae: 19.1884\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 926.9424 - mse: 926.9424 - mae: 17.8045 - val_loss: 2041.2283 - val_mse: 2041.2283 - val_mae: 20.8901\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 812.5424 - mse: 812.5424 - mae: 15.6962 - val_loss: 1515.4684 - val_mse: 1515.4684 - val_mae: 18.0724\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 717.5380 - mse: 717.5380 - mae: 16.4204 - val_loss: 1376.6600 - val_mse: 1376.6600 - val_mae: 18.2044\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1081.7809 - mse: 1081.7809 - mae: 18.5744 - val_loss: 1607.2045 - val_mse: 1607.2047 - val_mae: 18.9109\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 655.5913 - mse: 655.5913 - mae: 15.1880 - val_loss: 1428.0634 - val_mse: 1428.0634 - val_mae: 18.0565\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 841.7006 - mse: 841.7006 - mae: 17.4132 - val_loss: 1637.9840 - val_mse: 1637.9840 - val_mae: 19.3169\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 625.8210 - mse: 625.8210 - mae: 15.0996 - val_loss: 1942.5215 - val_mse: 1942.5215 - val_mae: 20.1506\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1125.1428 - mse: 1125.1428 - mae: 16.9684 - val_loss: 1408.9119 - val_mse: 1408.9119 - val_mae: 17.1520\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 708.6588 - mse: 708.6588 - mae: 15.6476 - val_loss: 1372.0626 - val_mse: 1372.0626 - val_mae: 16.7530\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 743.0521 - mse: 743.0521 - mae: 15.7009 - val_loss: 1535.7781 - val_mse: 1535.7781 - val_mae: 18.5050\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 781.5483 - mse: 781.5483 - mae: 14.4167 - val_loss: 1519.1453 - val_mse: 1519.1453 - val_mae: 17.9758\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 927.8151 - mse: 927.8151 - mae: 15.9721 - val_loss: 1290.3002 - val_mse: 1290.3002 - val_mae: 16.5627\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 763.9615 - mse: 763.9615 - mae: 16.2852 - val_loss: 1355.9974 - val_mse: 1355.9974 - val_mae: 17.0535\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 723.6308 - mse: 723.6308 - mae: 15.2982 - val_loss: 1336.9415 - val_mse: 1336.9415 - val_mae: 17.1688\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 634.0850 - mse: 634.0850 - mae: 14.3987 - val_loss: 1380.3170 - val_mse: 1380.3171 - val_mae: 17.3005\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 576.3403 - mse: 576.3403 - mae: 14.4536 - val_loss: 1427.5449 - val_mse: 1427.5449 - val_mae: 17.0304\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 819.3839 - mse: 819.3838 - mae: 16.0497 - val_loss: 1510.0920 - val_mse: 1510.0920 - val_mae: 17.3032\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 562.1553 - mse: 562.1553 - mae: 13.4866 - val_loss: 1188.6331 - val_mse: 1188.6331 - val_mae: 16.2531\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 678.4852 - mse: 678.4852 - mae: 14.5365 - val_loss: 1653.2073 - val_mse: 1653.2073 - val_mae: 18.2576\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 587.1260 - mse: 587.1260 - mae: 13.8519 - val_loss: 1310.3926 - val_mse: 1310.3926 - val_mae: 16.1034\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 744.3800 - mse: 744.3800 - mae: 15.0526 - val_loss: 1373.7844 - val_mse: 1373.7843 - val_mae: 16.3091\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 797.0560 - mse: 797.0560 - mae: 15.9914 - val_loss: 1283.6722 - val_mse: 1283.6722 - val_mae: 16.2361\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 784.6221 - mse: 784.6221 - mae: 14.8417 - val_loss: 1286.9006 - val_mse: 1286.9006 - val_mae: 16.6664\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 619.9257 - mse: 619.9257 - mae: 14.2626 - val_loss: 1470.2328 - val_mse: 1470.2328 - val_mae: 17.7396\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 619.1057 - mse: 619.1057 - mae: 14.6723 - val_loss: 1577.9125 - val_mse: 1577.9127 - val_mae: 17.9317\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 535.2181 - mse: 535.2181 - mae: 13.7483 - val_loss: 1290.3601 - val_mse: 1290.3601 - val_mae: 15.9344\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 702.3875 - mse: 702.3875 - mae: 13.7197 - val_loss: 1680.1957 - val_mse: 1680.1957 - val_mae: 18.1426\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 669.9027 - mse: 669.9027 - mae: 14.1840 - val_loss: 1398.3654 - val_mse: 1398.3654 - val_mae: 16.3433\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 607.4978 - mse: 607.4978 - mae: 13.5793 - val_loss: 1246.1212 - val_mse: 1246.1212 - val_mae: 16.2711\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 550.8251 - mse: 550.8251 - mae: 14.1225 - val_loss: 1548.2216 - val_mse: 1548.2216 - val_mae: 18.1333\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 762.0121 - mse: 762.0121 - mae: 14.5065 - val_loss: 1375.8723 - val_mse: 1375.8723 - val_mae: 16.7919\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 582.3292 - mse: 582.3292 - mae: 14.1446 - val_loss: 1504.2908 - val_mse: 1504.2908 - val_mae: 17.0363\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 498.7111 - mse: 498.7111 - mae: 13.0610 - val_loss: 1211.5720 - val_mse: 1211.5720 - val_mae: 15.6310\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 690.1140 - mse: 690.1140 - mae: 14.6715 - val_loss: 1243.6753 - val_mse: 1243.6753 - val_mae: 16.4143\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 764.6893 - mse: 764.6893 - mae: 15.2897 - val_loss: 1155.8533 - val_mse: 1155.8533 - val_mae: 16.2787\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 618.3106 - mse: 618.3106 - mae: 14.3284 - val_loss: 1456.5872 - val_mse: 1456.5872 - val_mae: 17.3336\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 580.6377 - mse: 580.6377 - mae: 13.5012 - val_loss: 1376.4856 - val_mse: 1376.4857 - val_mae: 16.2716\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 586.7888 - mse: 586.7888 - mae: 13.5690 - val_loss: 1083.5924 - val_mse: 1083.5924 - val_mae: 14.9079\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 685.9487 - mse: 685.9487 - mae: 13.7627 - val_loss: 1728.5209 - val_mse: 1728.5209 - val_mae: 19.6908\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 898.6732 - mse: 898.6732 - mae: 15.0820 - val_loss: 1579.9326 - val_mse: 1579.9326 - val_mae: 17.8196\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 463.5528 - mse: 463.5528 - mae: 12.6646 - val_loss: 1365.5687 - val_mse: 1365.5686 - val_mae: 15.7380\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 840.9895 - mse: 840.9895 - mae: 14.7012 - val_loss: 1224.0194 - val_mse: 1224.0194 - val_mae: 15.1677\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 744.7335 - mse: 744.7335 - mae: 14.0040 - val_loss: 1187.0067 - val_mse: 1187.0067 - val_mae: 16.7945\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 858.0898 - mse: 858.0898 - mae: 15.9765 - val_loss: 2150.7981 - val_mse: 2150.7981 - val_mae: 24.1039\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 650.5433 - mse: 650.5433 - mae: 13.7821 - val_loss: 1060.1902 - val_mse: 1060.1902 - val_mae: 14.8465\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 603.4294 - mse: 603.4294 - mae: 15.1966 - val_loss: 1174.6823 - val_mse: 1174.6823 - val_mae: 15.2211\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 753.4661 - mse: 753.4661 - mae: 14.2428 - val_loss: 1339.5430 - val_mse: 1339.5430 - val_mae: 17.5997\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 465.5705 - mse: 465.5705 - mae: 12.4785 - val_loss: 1276.3757 - val_mse: 1276.3757 - val_mae: 16.9320\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 538.9589 - mse: 538.9589 - mae: 12.7909 - val_loss: 1237.1250 - val_mse: 1237.1250 - val_mae: 15.6181\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.1912 - mse: 509.1912 - mae: 12.4210 - val_loss: 1029.8673 - val_mse: 1029.8673 - val_mae: 14.6602\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 438.3068 - mse: 438.3068 - mae: 12.7627 - val_loss: 1437.9142 - val_mse: 1437.9142 - val_mae: 17.2869\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 469.7661 - mse: 469.7661 - mae: 12.3871 - val_loss: 999.7726 - val_mse: 999.7726 - val_mae: 14.4778\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 565.5045 - mse: 565.5045 - mae: 14.0667 - val_loss: 1283.2191 - val_mse: 1283.2191 - val_mae: 15.9978\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 416.1516 - mse: 416.1516 - mae: 12.4991 - val_loss: 1240.9331 - val_mse: 1240.9331 - val_mae: 15.8209\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 346.4290 - mse: 346.4290 - mae: 11.3773 - val_loss: 1144.4967 - val_mse: 1144.4967 - val_mae: 14.8985\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 511.0011 - mse: 511.0011 - mae: 12.6920 - val_loss: 989.2589 - val_mse: 989.2589 - val_mae: 14.6502\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 487.5882 - mse: 487.5882 - mae: 12.9042 - val_loss: 1081.7559 - val_mse: 1081.7559 - val_mae: 15.8158\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 794.0903 - mse: 794.0903 - mae: 14.3458 - val_loss: 1244.8848 - val_mse: 1244.8848 - val_mae: 16.3392\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 575.8716 - mse: 575.8716 - mae: 13.6081 - val_loss: 1000.7087 - val_mse: 1000.7087 - val_mae: 14.8369\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 470.5511 - mse: 470.5511 - mae: 12.2688 - val_loss: 957.1143 - val_mse: 957.1143 - val_mae: 14.5345\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 416.3323 - mse: 416.3323 - mae: 11.8841 - val_loss: 1235.8829 - val_mse: 1235.8829 - val_mae: 16.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 427.7151 - mse: 427.7151 - mae: 11.8621 - val_loss: 1267.0277 - val_mse: 1267.0277 - val_mae: 15.7012\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 566.9584 - mse: 566.9584 - mae: 12.7054 - val_loss: 890.2122 - val_mse: 890.2122 - val_mae: 13.7646\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 660.0261 - mse: 660.0261 - mae: 13.6051 - val_loss: 1328.4158 - val_mse: 1328.4158 - val_mae: 16.3974\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 740.3101 - mse: 740.3101 - mae: 13.9047 - val_loss: 864.5618 - val_mse: 864.5618 - val_mae: 14.1591\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 690.5539 - mse: 690.5539 - mae: 14.5386 - val_loss: 1070.2493 - val_mse: 1070.2493 - val_mae: 15.6151\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 749.6703 - mse: 749.6703 - mae: 15.3182 - val_loss: 1315.8835 - val_mse: 1315.8835 - val_mae: 16.2933\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 741.1901 - mse: 741.1901 - mae: 13.9616 - val_loss: 741.4916 - val_mse: 741.4916 - val_mae: 13.5947\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 563.2220 - mse: 563.2220 - mae: 13.1785 - val_loss: 979.2932 - val_mse: 979.2932 - val_mae: 15.2295\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 539.8688 - mse: 539.8688 - mae: 13.6631 - val_loss: 1453.0052 - val_mse: 1453.0051 - val_mae: 18.7121\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 681.5549 - mse: 681.5549 - mae: 13.9616 - val_loss: 929.2270 - val_mse: 929.2270 - val_mae: 13.5235\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 443.5772 - mse: 443.5772 - mae: 12.1158 - val_loss: 949.2426 - val_mse: 949.2426 - val_mae: 13.7434\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 430.3795 - mse: 430.3795 - mae: 11.6040 - val_loss: 1207.1266 - val_mse: 1207.1266 - val_mae: 16.1894\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 440.5961 - mse: 440.5961 - mae: 12.6708 - val_loss: 919.0870 - val_mse: 919.0870 - val_mae: 13.8105\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 327.3263 - mse: 327.3263 - mae: 11.4826 - val_loss: 895.1318 - val_mse: 895.1318 - val_mae: 13.3354\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 390.9808 - mse: 390.9808 - mae: 12.0215 - val_loss: 1233.3597 - val_mse: 1233.3597 - val_mae: 16.0147\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 633.8928 - mse: 633.8928 - mae: 13.0173 - val_loss: 979.0001 - val_mse: 979.0001 - val_mae: 13.6980\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 536.3148 - mse: 536.3148 - mae: 13.1707 - val_loss: 791.4529 - val_mse: 791.4529 - val_mae: 13.0450\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 383.4829 - mse: 383.4829 - mae: 11.4629 - val_loss: 1365.1072 - val_mse: 1365.1072 - val_mae: 17.3314\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 418.0504 - mse: 418.0504 - mae: 11.9012 - val_loss: 1225.1321 - val_mse: 1225.1321 - val_mae: 15.7943\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 453.7353 - mse: 453.7353 - mae: 11.7776 - val_loss: 756.3182 - val_mse: 756.3182 - val_mae: 13.0112\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 536.8318 - mse: 536.8318 - mae: 13.3275 - val_loss: 1114.1000 - val_mse: 1114.1000 - val_mae: 15.7544\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.2071 - mse: 367.2071 - mae: 11.2261 - val_loss: 991.2485 - val_mse: 991.2485 - val_mae: 15.0970\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 315.2911 - mse: 315.2911 - mae: 11.2452 - val_loss: 873.2724 - val_mse: 873.2724 - val_mae: 13.5899\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 377.7243 - mse: 377.7243 - mae: 11.7544 - val_loss: 811.1458 - val_mse: 811.1458 - val_mae: 12.9424\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 429.8623 - mse: 429.8623 - mae: 12.3389 - val_loss: 928.9781 - val_mse: 928.9781 - val_mae: 13.9470\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 418.6217 - mse: 418.6217 - mae: 11.6010 - val_loss: 1183.6892 - val_mse: 1183.6892 - val_mae: 15.6984\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 430.6650 - mse: 430.6650 - mae: 11.7384 - val_loss: 923.8062 - val_mse: 923.8062 - val_mae: 12.9457\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 356.8448 - mse: 356.8448 - mae: 11.3422 - val_loss: 1216.3982 - val_mse: 1216.3982 - val_mae: 14.9503\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 395.9707 - mse: 395.9707 - mae: 11.1604 - val_loss: 958.2537 - val_mse: 958.2537 - val_mae: 13.1207\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 270.4720 - mse: 270.4720 - mae: 10.3757 - val_loss: 747.9568 - val_mse: 747.9568 - val_mae: 12.2831\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 391.2761 - mse: 391.2761 - mae: 11.6011 - val_loss: 989.3522 - val_mse: 989.3522 - val_mae: 14.4626\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 385.9619 - mse: 385.9619 - mae: 11.4954 - val_loss: 695.7802 - val_mse: 695.7802 - val_mae: 12.8845\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 460.7642 - mse: 460.7642 - mae: 12.7362 - val_loss: 814.9737 - val_mse: 814.9738 - val_mae: 13.6057\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.0144 - mse: 285.0144 - mae: 10.0621 - val_loss: 1085.8772 - val_mse: 1085.8772 - val_mae: 15.0802\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 529.6816 - mse: 529.6816 - mae: 12.8655 - val_loss: 774.3600 - val_mse: 774.3600 - val_mae: 12.4530\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 338.2493 - mse: 338.2493 - mae: 11.1503 - val_loss: 1117.7843 - val_mse: 1117.7843 - val_mae: 15.3351\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 466.9461 - mse: 466.9461 - mae: 12.7533 - val_loss: 574.6345 - val_mse: 574.6345 - val_mae: 12.3862\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 401.9932 - mse: 401.9932 - mae: 11.7747 - val_loss: 830.4333 - val_mse: 830.4333 - val_mae: 13.8653\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 382.3553 - mse: 382.3553 - mae: 12.0321 - val_loss: 1168.0651 - val_mse: 1168.0651 - val_mae: 16.4062\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 403.4891 - mse: 403.4891 - mae: 11.7974 - val_loss: 786.5319 - val_mse: 786.5319 - val_mae: 12.1164\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 298.1075 - mse: 298.1075 - mae: 10.5290 - val_loss: 1154.5729 - val_mse: 1154.5729 - val_mae: 14.6687\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 478.9227 - mse: 478.9227 - mae: 11.9006 - val_loss: 1323.6010 - val_mse: 1323.6010 - val_mae: 16.1764\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 486.3823 - mse: 486.3823 - mae: 12.2460 - val_loss: 785.1129 - val_mse: 785.1129 - val_mae: 12.4212\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 409.1128 - mse: 409.1128 - mae: 11.7267 - val_loss: 1038.4237 - val_mse: 1038.4237 - val_mae: 14.1998\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 260.9721 - mse: 260.9721 - mae: 10.2220 - val_loss: 681.3641 - val_mse: 681.3641 - val_mae: 12.7415\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 404.9052 - mse: 404.9052 - mae: 11.4178 - val_loss: 679.5664 - val_mse: 679.5664 - val_mae: 12.9753\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 439.1758 - mse: 439.1758 - mae: 12.5657 - val_loss: 1269.0505 - val_mse: 1269.0505 - val_mae: 17.4434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 231.2881 - mse: 231.2881 - mae: 9.7794 - val_loss: 1120.6548 - val_mse: 1120.6548 - val_mae: 14.8512\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 425.3221 - mse: 425.3221 - mae: 11.8612 - val_loss: 688.3070 - val_mse: 688.3070 - val_mae: 11.9582\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 379.0567 - mse: 379.0567 - mae: 11.9686 - val_loss: 1111.2109 - val_mse: 1111.2109 - val_mae: 15.3870\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 364.2196 - mse: 364.2196 - mae: 10.8674 - val_loss: 914.5898 - val_mse: 914.5898 - val_mae: 13.9981\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 351.5112 - mse: 351.5112 - mae: 10.9031 - val_loss: 808.6713 - val_mse: 808.6713 - val_mae: 12.7414\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 314.3465 - mse: 314.3465 - mae: 10.4489 - val_loss: 842.7744 - val_mse: 842.7744 - val_mae: 12.3498\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 372.5473 - mse: 372.5473 - mae: 11.0186 - val_loss: 1013.6524 - val_mse: 1013.6524 - val_mae: 13.3228\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.5146 - mse: 292.5146 - mae: 10.9109 - val_loss: 882.9499 - val_mse: 882.9499 - val_mae: 12.7687\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 301.7787 - mse: 301.7787 - mae: 10.1501 - val_loss: 1180.4436 - val_mse: 1180.4436 - val_mae: 17.1164\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 337.3719 - mse: 337.3719 - mae: 11.4599 - val_loss: 1003.6108 - val_mse: 1003.6108 - val_mae: 14.6162\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 370.0695 - mse: 370.0695 - mae: 11.8085 - val_loss: 1010.8111 - val_mse: 1010.8111 - val_mae: 13.2506\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 253.7423 - mse: 253.7423 - mae: 10.2788 - val_loss: 904.5775 - val_mse: 904.5775 - val_mae: 12.5684\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 432.9518 - mse: 432.9518 - mae: 12.1739 - val_loss: 758.5434 - val_mse: 758.5434 - val_mae: 12.8642\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 263.9507 - mse: 263.9507 - mae: 10.6901 - val_loss: 990.3312 - val_mse: 990.3312 - val_mae: 15.0378\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 499.3214 - mse: 499.3214 - mae: 12.7417 - val_loss: 873.0975 - val_mse: 873.0975 - val_mae: 13.2969\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 432.8089 - mse: 432.8089 - mae: 11.9467 - val_loss: 755.4077 - val_mse: 755.4077 - val_mae: 11.7130\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 292.7068 - mse: 292.7068 - mae: 10.1261 - val_loss: 975.9418 - val_mse: 975.9418 - val_mae: 14.2023\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 455.1645 - mse: 455.1645 - mae: 12.1873 - val_loss: 696.6716 - val_mse: 696.6716 - val_mae: 12.9095\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 294.6863 - mse: 294.6863 - mae: 10.4417 - val_loss: 713.4636 - val_mse: 713.4636 - val_mae: 13.9529\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.0515 - mse: 242.0515 - mae: 10.0350 - val_loss: 796.0731 - val_mse: 796.0731 - val_mae: 14.4039\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 328.2029 - mse: 328.2029 - mae: 10.7164 - val_loss: 612.6730 - val_mse: 612.6730 - val_mae: 12.2095\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 299.2904 - mse: 299.2904 - mae: 10.4733 - val_loss: 886.8621 - val_mse: 886.8621 - val_mae: 14.1507\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 313.0154 - mse: 313.0154 - mae: 11.4732 - val_loss: 762.4512 - val_mse: 762.4512 - val_mae: 13.0258\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.2050 - mse: 271.2050 - mae: 9.8776 - val_loss: 731.9018 - val_mse: 731.9018 - val_mae: 12.2148\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 519.3347 - mse: 519.3347 - mae: 12.4457 - val_loss: 586.5035 - val_mse: 586.5035 - val_mae: 11.6212\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 419.3626 - mse: 419.3626 - mae: 11.7752 - val_loss: 673.6342 - val_mse: 673.6342 - val_mae: 12.8511\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 371.4403 - mse: 371.4403 - mae: 10.3297 - val_loss: 745.0504 - val_mse: 745.0505 - val_mae: 13.5526\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 260.2073 - mse: 260.2073 - mae: 10.3983 - val_loss: 460.3350 - val_mse: 460.3350 - val_mae: 11.3372\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 304.0566 - mse: 304.0566 - mae: 10.7125 - val_loss: 667.2765 - val_mse: 667.2765 - val_mae: 12.7981\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 216.6909 - mse: 216.6909 - mae: 8.8439 - val_loss: 874.5970 - val_mse: 874.5970 - val_mae: 14.4677\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 276.9137 - mse: 276.9137 - mae: 9.9141 - val_loss: 783.2321 - val_mse: 783.2321 - val_mae: 12.1861\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 428.8980 - mse: 428.8980 - mae: 11.9396 - val_loss: 951.0366 - val_mse: 951.0366 - val_mae: 13.8600\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 327.3116 - mse: 327.3116 - mae: 10.6685 - val_loss: 689.0511 - val_mse: 689.0511 - val_mae: 11.6619\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.5573 - mse: 208.5573 - mae: 9.2866 - val_loss: 717.4268 - val_mse: 717.4267 - val_mae: 12.4374\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 276.2277 - mse: 276.2277 - mae: 10.1256 - val_loss: 1196.8237 - val_mse: 1196.8237 - val_mae: 17.2707\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 254.6302 - mse: 254.6302 - mae: 9.9918 - val_loss: 926.4455 - val_mse: 926.4455 - val_mae: 12.7764\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 438.7039 - mse: 438.7039 - mae: 11.6774 - val_loss: 605.2596 - val_mse: 605.2596 - val_mae: 10.8104\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 481.7955 - mse: 481.7955 - mae: 12.4490 - val_loss: 482.1454 - val_mse: 482.1454 - val_mae: 11.4835\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 374.2108 - mse: 374.2108 - mae: 11.1915 - val_loss: 698.0912 - val_mse: 698.0912 - val_mae: 13.9257\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.1742 - mse: 251.1742 - mae: 9.8532 - val_loss: 568.4472 - val_mse: 568.4472 - val_mae: 11.2474\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.4195 - mse: 264.4195 - mae: 10.2719 - val_loss: 523.8005 - val_mse: 523.8005 - val_mae: 11.0869\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 337.7873 - mse: 337.7873 - mae: 11.1298 - val_loss: 1472.9955 - val_mse: 1472.9954 - val_mae: 18.4649\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 311.6256 - mse: 311.6256 - mae: 10.2494 - val_loss: 982.4651 - val_mse: 982.4651 - val_mae: 12.5501\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 273.4717 - mse: 273.4717 - mae: 10.1877 - val_loss: 562.5734 - val_mse: 562.5734 - val_mae: 11.0399\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 314.5503 - mse: 314.5503 - mae: 10.4968 - val_loss: 778.9568 - val_mse: 778.9568 - val_mae: 15.0606\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 327.7690 - mse: 327.7690 - mae: 10.9257 - val_loss: 632.7768 - val_mse: 632.7768 - val_mae: 13.6545\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 301.1178 - mse: 301.1178 - mae: 11.2752 - val_loss: 1037.0415 - val_mse: 1037.0415 - val_mae: 15.9318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 565.7833 - mse: 565.7833 - mae: 12.3366 - val_loss: 538.0101 - val_mse: 538.0101 - val_mae: 10.8873\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 251.7011 - mse: 251.7011 - mae: 9.9526 - val_loss: 613.0240 - val_mse: 613.0240 - val_mae: 11.8950\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.4654 - mse: 254.4654 - mae: 10.0666 - val_loss: 933.7845 - val_mse: 933.7845 - val_mae: 16.6499\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212.1072 - mse: 212.1072 - mae: 9.3764 - val_loss: 816.9938 - val_mse: 816.9938 - val_mae: 14.4326\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 312.7793 - mse: 312.7793 - mae: 10.5583 - val_loss: 818.5045 - val_mse: 818.5045 - val_mae: 13.1597\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 370.4307 - mse: 370.4307 - mae: 10.7993 - val_loss: 856.7607 - val_mse: 856.7607 - val_mae: 12.8266\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.7150 - mse: 240.7150 - mae: 9.7591 - val_loss: 627.1285 - val_mse: 627.1285 - val_mae: 11.8467\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.5189 - mse: 251.5189 - mae: 9.4857 - val_loss: 913.5334 - val_mse: 913.5334 - val_mae: 15.2898\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 175.4926 - mse: 175.4926 - mae: 8.5459 - val_loss: 557.3370 - val_mse: 557.3370 - val_mae: 11.5913\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 350.9075 - mse: 350.9075 - mae: 10.7743 - val_loss: 500.1395 - val_mse: 500.1395 - val_mae: 11.7099\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 348.6024 - mse: 348.6024 - mae: 10.6841 - val_loss: 796.4601 - val_mse: 796.4601 - val_mae: 15.3237\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 309.7397 - mse: 309.7397 - mae: 10.1512 - val_loss: 809.7570 - val_mse: 809.7570 - val_mae: 13.6590\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.5855 - mse: 228.5855 - mae: 9.9191 - val_loss: 633.2474 - val_mse: 633.2474 - val_mae: 11.4815\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 331.4629 - mse: 331.4629 - mae: 10.9170 - val_loss: 786.8724 - val_mse: 786.8724 - val_mae: 12.7565\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 377.3103 - mse: 377.3103 - mae: 11.3162 - val_loss: 391.0392 - val_mse: 391.0392 - val_mae: 10.9794\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 467.8790 - mse: 467.8789 - mae: 13.2143 - val_loss: 841.2241 - val_mse: 841.2241 - val_mae: 15.4928\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 206.2584 - mse: 206.2584 - mae: 9.0418 - val_loss: 794.3442 - val_mse: 794.3442 - val_mae: 12.6462\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 233.4534 - mse: 233.4534 - mae: 9.7499 - val_loss: 781.7728 - val_mse: 781.7729 - val_mae: 11.7312\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.0344 - mse: 261.0344 - mae: 9.7051 - val_loss: 694.9203 - val_mse: 694.9203 - val_mae: 11.2847\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.8343 - mse: 306.8343 - mae: 10.1433 - val_loss: 699.1303 - val_mse: 699.1303 - val_mae: 12.3298\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 315.4387 - mse: 315.4387 - mae: 9.6148 - val_loss: 581.4879 - val_mse: 581.4879 - val_mae: 11.8163\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 190.5973 - mse: 190.5973 - mae: 9.6214 - val_loss: 737.3909 - val_mse: 737.3909 - val_mae: 12.8070\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 197.2410 - mse: 197.2410 - mae: 9.3454 - val_loss: 884.7523 - val_mse: 884.7523 - val_mae: 13.3761\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 326.0156 - mse: 326.0156 - mae: 10.4514 - val_loss: 489.1119 - val_mse: 489.1119 - val_mae: 10.1602\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 204.6312 - mse: 204.6312 - mae: 9.2860 - val_loss: 824.3010 - val_mse: 824.3010 - val_mae: 14.5899\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.9571 - mse: 288.9571 - mae: 10.2695 - val_loss: 661.5061 - val_mse: 661.5061 - val_mae: 12.3949\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 331.3656 - mse: 331.3656 - mae: 10.8908 - val_loss: 600.4457 - val_mse: 600.4457 - val_mae: 11.6342\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.6918 - mse: 229.6918 - mae: 8.9058 - val_loss: 541.7841 - val_mse: 541.7841 - val_mae: 11.9892\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.3860 - mse: 243.3860 - mae: 10.0826 - val_loss: 741.3341 - val_mse: 741.3340 - val_mae: 13.9044\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.3790 - mse: 292.3790 - mae: 10.5009 - val_loss: 753.6248 - val_mse: 753.6248 - val_mae: 12.3879\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 479.2030 - mse: 479.2030 - mae: 11.7091 - val_loss: 451.8546 - val_mse: 451.8546 - val_mae: 10.1670\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 311.4552 - mse: 311.4552 - mae: 10.3020 - val_loss: 1111.3652 - val_mse: 1111.3652 - val_mae: 18.1269\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 346.8912 - mse: 346.8912 - mae: 10.6448 - val_loss: 395.5936 - val_mse: 395.5936 - val_mae: 11.9925\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 530.0976 - mse: 530.0976 - mae: 13.6676 - val_loss: 502.0777 - val_mse: 502.0777 - val_mae: 12.6364\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 304.1938 - mse: 304.1938 - mae: 10.6399 - val_loss: 788.4394 - val_mse: 788.4394 - val_mae: 15.3281\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 248.8447 - mse: 248.8447 - mae: 9.8954 - val_loss: 526.4556 - val_mse: 526.4556 - val_mae: 11.4414\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.3039 - mse: 244.3039 - mae: 9.6813 - val_loss: 540.6780 - val_mse: 540.6780 - val_mae: 12.4431\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 399.1108 - mse: 399.1108 - mae: 11.8437 - val_loss: 736.3085 - val_mse: 736.3085 - val_mae: 14.5895\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 203.2574 - mse: 203.2574 - mae: 9.3173 - val_loss: 617.5037 - val_mse: 617.5037 - val_mae: 11.8500\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 293.7190 - mse: 293.7190 - mae: 9.8551 - val_loss: 580.3624 - val_mse: 580.3624 - val_mae: 11.0919\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 335.6257 - mse: 335.6257 - mae: 10.1324 - val_loss: 766.1793 - val_mse: 766.1793 - val_mae: 13.8273\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.0802 - mse: 204.0802 - mae: 9.1014 - val_loss: 798.5004 - val_mse: 798.5004 - val_mae: 14.2984\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 390.1021 - mse: 390.1021 - mae: 10.6906 - val_loss: 633.1267 - val_mse: 633.1267 - val_mae: 11.9921\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6855 - mse: 227.6855 - mae: 9.7263 - val_loss: 592.1054 - val_mse: 592.1054 - val_mae: 11.5071\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 273.3344 - mse: 273.3344 - mae: 9.8932 - val_loss: 1020.3715 - val_mse: 1020.3715 - val_mae: 14.9688\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 300.6340 - mse: 300.6340 - mae: 10.3628 - val_loss: 706.6967 - val_mse: 706.6967 - val_mae: 11.8797\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 250.5873 - mse: 250.5873 - mae: 9.4838 - val_loss: 483.0470 - val_mse: 483.0470 - val_mae: 10.7142\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 197.6087 - mse: 197.6087 - mae: 9.4695 - val_loss: 537.1794 - val_mse: 537.1794 - val_mae: 11.5921\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 256.4771 - mse: 256.4771 - mae: 10.7245 - val_loss: 740.8079 - val_mse: 740.8079 - val_mae: 13.2633\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 198.4809 - mse: 198.4809 - mae: 9.2036 - val_loss: 515.3945 - val_mse: 515.3945 - val_mae: 9.9953\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 204.6924 - mse: 204.6924 - mae: 8.6939 - val_loss: 663.8591 - val_mse: 663.8591 - val_mae: 12.0681\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 206.0398 - mse: 206.0398 - mae: 9.3270 - val_loss: 619.2235 - val_mse: 619.2235 - val_mae: 11.3343\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 202.2311 - mse: 202.2311 - mae: 8.9245 - val_loss: 596.6245 - val_mse: 596.6245 - val_mae: 11.3354\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 173.5176 - mse: 173.5176 - mae: 8.6774 - val_loss: 596.4951 - val_mse: 596.4951 - val_mae: 11.7484\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 272.7587 - mse: 272.7587 - mae: 9.7011 - val_loss: 701.9802 - val_mse: 701.9803 - val_mae: 12.9677\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.9211 - mse: 200.9211 - mae: 8.3864 - val_loss: 556.5798 - val_mse: 556.5798 - val_mae: 11.6471\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 178.3359 - mse: 178.3359 - mae: 8.6748 - val_loss: 520.1873 - val_mse: 520.1873 - val_mae: 11.1462\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 234.1587 - mse: 234.1587 - mae: 8.9204 - val_loss: 704.4124 - val_mse: 704.4124 - val_mae: 12.2365\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 281.3160 - mse: 281.3160 - mae: 10.0535 - val_loss: 835.6326 - val_mse: 835.6326 - val_mae: 12.6805\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 252.6542 - mse: 252.6542 - mae: 9.7729 - val_loss: 405.6989 - val_mse: 405.6989 - val_mae: 10.2341\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 393.8294 - mse: 393.8294 - mae: 10.3613 - val_loss: 1541.5198 - val_mse: 1541.5198 - val_mae: 20.7664\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 452.8693 - mse: 452.8693 - mae: 10.9374 - val_loss: 742.1302 - val_mse: 742.1302 - val_mae: 10.5136\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 495.5491 - mse: 495.5491 - mae: 13.5937 - val_loss: 468.3348 - val_mse: 468.3348 - val_mae: 10.6663\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 353.4709 - mse: 353.4709 - mae: 11.1993 - val_loss: 1388.0017 - val_mse: 1388.0017 - val_mae: 22.9039\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 402.3451 - mse: 402.3451 - mae: 11.8173 - val_loss: 361.4385 - val_mse: 361.4385 - val_mae: 10.2289\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 455.5884 - mse: 455.5884 - mae: 10.7652 - val_loss: 1055.1561 - val_mse: 1055.1561 - val_mae: 16.8650\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 340.3036 - mse: 340.3036 - mae: 11.0321 - val_loss: 951.4131 - val_mse: 951.4131 - val_mae: 13.8904\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 303.3300 - mse: 303.3300 - mae: 10.3438 - val_loss: 501.5754 - val_mse: 501.5754 - val_mae: 9.9407\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 211.8391 - mse: 211.8391 - mae: 9.1859 - val_loss: 811.1630 - val_mse: 811.1630 - val_mae: 13.4975\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 199.2710 - mse: 199.2710 - mae: 9.4814 - val_loss: 1084.3690 - val_mse: 1084.3690 - val_mae: 16.0045\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 241.7581 - mse: 241.7581 - mae: 10.0407 - val_loss: 545.3486 - val_mse: 545.3486 - val_mae: 10.5147\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 382.2436 - mse: 382.2436 - mae: 11.5428 - val_loss: 817.7166 - val_mse: 817.7165 - val_mae: 13.1551\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 382.0510 - mse: 382.0510 - mae: 10.6138 - val_loss: 1104.9133 - val_mse: 1104.9133 - val_mae: 17.5742\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 303.9587 - mse: 303.9587 - mae: 9.0114 - val_loss: 577.6309 - val_mse: 577.6309 - val_mae: 11.8538\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 220.7708 - mse: 220.7708 - mae: 9.0764 - val_loss: 400.0565 - val_mse: 400.0565 - val_mae: 9.7307\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 136.8769 - mse: 136.8769 - mae: 7.6484 - val_loss: 579.4017 - val_mse: 579.4017 - val_mae: 13.0963\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 202.8076 - mse: 202.8076 - mae: 9.1287 - val_loss: 662.2477 - val_mse: 662.2477 - val_mae: 13.8752\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 169.1143 - mse: 169.1143 - mae: 8.7052 - val_loss: 507.7583 - val_mse: 507.7583 - val_mae: 10.8338\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 334.9526 - mse: 334.9526 - mae: 10.8270 - val_loss: 595.5663 - val_mse: 595.5663 - val_mae: 11.5308\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 278.4828 - mse: 278.4828 - mae: 9.3306 - val_loss: 734.7387 - val_mse: 734.7387 - val_mae: 13.3070\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4266 - mse: 223.4266 - mae: 9.0530 - val_loss: 545.6834 - val_mse: 545.6834 - val_mae: 10.9428\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.3575 - mse: 239.3575 - mae: 9.7784 - val_loss: 735.9990 - val_mse: 735.9990 - val_mae: 13.6216\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 299.2226 - mse: 299.2226 - mae: 9.1960 - val_loss: 421.8427 - val_mse: 421.8427 - val_mae: 10.4327\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 359.6233 - mse: 359.6233 - mae: 10.5293 - val_loss: 605.0931 - val_mse: 605.0931 - val_mae: 13.6586\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 275.9771 - mse: 275.9771 - mae: 9.6871 - val_loss: 1197.3624 - val_mse: 1197.3624 - val_mae: 17.8646\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 161.5174 - mse: 161.5174 - mae: 8.6826 - val_loss: 732.1630 - val_mse: 732.1630 - val_mae: 11.8408\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.4414 - mse: 240.4414 - mae: 10.3079 - val_loss: 637.6181 - val_mse: 637.6181 - val_mae: 10.6426\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 149.3748 - mse: 149.3748 - mae: 7.6395 - val_loss: 954.1055 - val_mse: 954.1055 - val_mae: 14.2827\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.8202 - mse: 254.8202 - mae: 9.7065 - val_loss: 705.9584 - val_mse: 705.9584 - val_mae: 11.4745\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 220.9572 - mse: 220.9572 - mae: 9.2494 - val_loss: 1004.8956 - val_mse: 1004.8956 - val_mae: 13.7960\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.7409 - mse: 292.7409 - mae: 10.0183 - val_loss: 776.3510 - val_mse: 776.3510 - val_mae: 11.0975\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 173.6544 - mse: 173.6544 - mae: 8.5185 - val_loss: 527.2468 - val_mse: 527.2468 - val_mae: 10.1005\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 296.7839 - mse: 296.7839 - mae: 9.7286 - val_loss: 710.9325 - val_mse: 710.9325 - val_mae: 13.9118\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 191.2615 - mse: 191.2615 - mae: 9.3169 - val_loss: 595.8259 - val_mse: 595.8259 - val_mae: 13.1746\n",
      "Epoch 282/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 317.2597 - mse: 317.2597 - mae: 9.7763 - val_loss: 465.5692 - val_mse: 465.5692 - val_mae: 11.4185\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 191.1298 - mse: 191.1298 - mae: 9.2914 - val_loss: 526.4765 - val_mse: 526.4765 - val_mae: 12.5359\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.5240 - mse: 211.5240 - mae: 8.3855 - val_loss: 300.6115 - val_mse: 300.6115 - val_mae: 9.4713\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 415.4281 - mse: 415.4281 - mae: 11.0028 - val_loss: 654.5676 - val_mse: 654.5676 - val_mae: 13.2771\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 273.9329 - mse: 273.9329 - mae: 9.9606 - val_loss: 818.9905 - val_mse: 818.9905 - val_mae: 13.8683\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 279.1841 - mse: 279.1841 - mae: 9.9377 - val_loss: 442.9497 - val_mse: 442.9497 - val_mae: 9.7563\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 266.5175 - mse: 266.5175 - mae: 10.3378 - val_loss: 544.2148 - val_mse: 544.2148 - val_mae: 11.7166\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 173.2121 - mse: 173.2121 - mae: 9.2835 - val_loss: 893.7594 - val_mse: 893.7595 - val_mae: 17.6234\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5978 - mse: 225.5978 - mae: 9.6103 - val_loss: 284.8761 - val_mse: 284.8761 - val_mae: 9.3092\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 260.3986 - mse: 260.3986 - mae: 9.8147 - val_loss: 325.6495 - val_mse: 325.6495 - val_mae: 10.4689\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 352.1849 - mse: 352.1849 - mae: 11.3295 - val_loss: 1327.9323 - val_mse: 1327.9323 - val_mae: 22.5487\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 346.6890 - mse: 346.6890 - mae: 11.3928 - val_loss: 559.0332 - val_mse: 559.0332 - val_mae: 11.5866\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 433.2187 - mse: 433.2187 - mae: 11.7826 - val_loss: 682.4457 - val_mse: 682.4457 - val_mae: 11.5163\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 376.1829 - mse: 376.1829 - mae: 10.1836 - val_loss: 1042.0804 - val_mse: 1042.0804 - val_mae: 14.6949\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 342.1929 - mse: 342.1929 - mae: 9.6648 - val_loss: 436.3361 - val_mse: 436.3361 - val_mae: 10.2781\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 380.8167 - mse: 380.8167 - mae: 11.0954 - val_loss: 651.6130 - val_mse: 651.6130 - val_mae: 13.8751\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 272.6010 - mse: 272.6010 - mae: 9.8550 - val_loss: 801.4459 - val_mse: 801.4459 - val_mae: 15.3112\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 285.5040 - mse: 285.5040 - mae: 8.9158 - val_loss: 465.8206 - val_mse: 465.8206 - val_mae: 10.5654\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 271.3476 - mse: 271.3476 - mae: 10.2828 - val_loss: 445.5318 - val_mse: 445.5318 - val_mae: 10.3307\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 194.1132 - mse: 194.1132 - mae: 8.8222 - val_loss: 655.5886 - val_mse: 655.5886 - val_mae: 12.7984\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 288.6596 - mse: 288.6596 - mae: 9.5228 - val_loss: 469.3577 - val_mse: 469.3577 - val_mae: 10.9377\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 181.7579 - mse: 181.7579 - mae: 8.8029 - val_loss: 800.9829 - val_mse: 800.9829 - val_mae: 15.5546\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 350.4409 - mse: 350.4409 - mae: 9.7078 - val_loss: 367.8880 - val_mse: 367.8880 - val_mae: 10.3862\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 242.4891 - mse: 242.4891 - mae: 8.8645 - val_loss: 787.3995 - val_mse: 787.3995 - val_mae: 14.0572\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 323.7803 - mse: 323.7803 - mae: 9.7598 - val_loss: 779.9811 - val_mse: 779.9810 - val_mae: 12.9428\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 247.1228 - mse: 247.1228 - mae: 9.7870 - val_loss: 364.2615 - val_mse: 364.2615 - val_mae: 9.7595\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 265.2668 - mse: 265.2668 - mae: 9.5393 - val_loss: 471.1783 - val_mse: 471.1783 - val_mae: 12.0670\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 249.1984 - mse: 249.1984 - mae: 9.2628 - val_loss: 564.8669 - val_mse: 564.8669 - val_mae: 13.2733\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 172.2609 - mse: 172.2609 - mae: 8.6458 - val_loss: 446.7762 - val_mse: 446.7762 - val_mae: 11.0079\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 183.0835 - mse: 183.0835 - mae: 8.5836 - val_loss: 533.0125 - val_mse: 533.0125 - val_mae: 11.5802\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 169.6583 - mse: 169.6583 - mae: 8.3009 - val_loss: 529.0182 - val_mse: 529.0182 - val_mae: 11.9230\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7801 - mse: 223.7801 - mae: 8.7480 - val_loss: 476.8369 - val_mse: 476.8369 - val_mae: 11.4952\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 270.4743 - mse: 270.4743 - mae: 8.8145 - val_loss: 372.7902 - val_mse: 372.7902 - val_mae: 10.0268\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 196.1645 - mse: 196.1645 - mae: 8.6493 - val_loss: 417.2483 - val_mse: 417.2483 - val_mae: 10.8714\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 152.5215 - mse: 152.5215 - mae: 8.1556 - val_loss: 629.5582 - val_mse: 629.5582 - val_mae: 13.6169\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.2064 - mse: 238.2064 - mae: 8.9420 - val_loss: 448.6886 - val_mse: 448.6886 - val_mae: 11.2045\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 383.4297 - mse: 383.4297 - mae: 10.6757 - val_loss: 371.4008 - val_mse: 371.4008 - val_mae: 9.9968\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 199.3543 - mse: 199.3543 - mae: 9.3585 - val_loss: 571.7524 - val_mse: 571.7524 - val_mae: 12.1752\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 104.7812 - mse: 104.7812 - mae: 6.8894 - val_loss: 631.8975 - val_mse: 631.8975 - val_mae: 12.6254\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220.8717 - mse: 220.8717 - mae: 8.8549 - val_loss: 537.2030 - val_mse: 537.2030 - val_mae: 11.2401\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 174.2260 - mse: 174.2260 - mae: 8.5022 - val_loss: 571.6189 - val_mse: 571.6189 - val_mae: 11.5802\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 160.6391 - mse: 160.6391 - mae: 7.5904 - val_loss: 764.6293 - val_mse: 764.6293 - val_mae: 14.2218\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 187.0989 - mse: 187.0989 - mae: 8.4309 - val_loss: 610.6778 - val_mse: 610.6778 - val_mae: 13.2179\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.8410 - mse: 168.8410 - mae: 8.3478 - val_loss: 482.8529 - val_mse: 482.8529 - val_mae: 12.1190\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 231.6313 - mse: 231.6313 - mae: 8.8135 - val_loss: 395.2704 - val_mse: 395.2704 - val_mae: 10.9445\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.1104 - mse: 241.1104 - mae: 9.3490 - val_loss: 844.4081 - val_mse: 844.4081 - val_mae: 14.8440\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 298.9118 - mse: 298.9118 - mae: 9.1248 - val_loss: 470.4378 - val_mse: 470.4378 - val_mae: 10.8166\n",
      "Epoch 329/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2226 - mse: 221.2226 - mae: 8.6834 - val_loss: 690.8727 - val_mse: 690.8727 - val_mae: 13.1362\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5574 - mse: 226.5574 - mae: 8.5213 - val_loss: 543.5620 - val_mse: 543.5620 - val_mae: 11.6246\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 145.7606 - mse: 145.7606 - mae: 7.2510 - val_loss: 407.4253 - val_mse: 407.4253 - val_mae: 10.1597\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 291.2202 - mse: 291.2202 - mae: 9.3147 - val_loss: 608.6429 - val_mse: 608.6429 - val_mae: 12.5938\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 164.7229 - mse: 164.7229 - mae: 8.3010 - val_loss: 485.8614 - val_mse: 485.8614 - val_mae: 11.4880\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 243.6268 - mse: 243.6268 - mae: 9.3468 - val_loss: 512.4607 - val_mse: 512.4607 - val_mae: 12.0805\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.4830 - mse: 215.4830 - mae: 8.6678 - val_loss: 614.6790 - val_mse: 614.6790 - val_mae: 12.4931\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5456 - mse: 224.5456 - mae: 9.5311 - val_loss: 383.8921 - val_mse: 383.8921 - val_mae: 9.7435\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 153.4669 - mse: 153.4669 - mae: 7.9650 - val_loss: 640.9926 - val_mse: 640.9926 - val_mae: 12.9341\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.8918 - mse: 306.8918 - mae: 9.3827 - val_loss: 428.7255 - val_mse: 428.7255 - val_mae: 11.0443\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 198.6821 - mse: 198.6821 - mae: 8.8619 - val_loss: 647.7768 - val_mse: 647.7768 - val_mae: 13.4136\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 140.7646 - mse: 140.7646 - mae: 7.4265 - val_loss: 505.8461 - val_mse: 505.8461 - val_mae: 11.6236\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 250.6763 - mse: 250.6763 - mae: 8.8206 - val_loss: 426.8064 - val_mse: 426.8064 - val_mae: 11.1775\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 166.5563 - mse: 166.5563 - mae: 7.6859 - val_loss: 546.5835 - val_mse: 546.5835 - val_mae: 13.0824\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 137.2820 - mse: 137.2820 - mae: 7.8204 - val_loss: 451.6037 - val_mse: 451.6037 - val_mae: 11.7527\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 203.0857 - mse: 203.0857 - mae: 8.2039 - val_loss: 298.8833 - val_mse: 298.8833 - val_mae: 9.5657\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 184.3340 - mse: 184.3340 - mae: 8.7010 - val_loss: 542.8446 - val_mse: 542.8446 - val_mae: 13.5215\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 174.5119 - mse: 174.5119 - mae: 8.1770 - val_loss: 373.9524 - val_mse: 373.9524 - val_mae: 10.8609\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 475.1785 - mse: 475.1785 - mae: 9.8069 - val_loss: 541.6150 - val_mse: 541.6150 - val_mae: 12.5811\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.2386 - mse: 306.2386 - mae: 9.1749 - val_loss: 887.7846 - val_mse: 887.7846 - val_mae: 14.4843\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 184.9462 - mse: 184.9462 - mae: 8.8060 - val_loss: 323.0344 - val_mse: 323.0344 - val_mae: 9.5373\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 428.4188 - mse: 428.4188 - mae: 11.6730 - val_loss: 720.7133 - val_mse: 720.7133 - val_mae: 16.0533\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 322.3309 - mse: 322.3309 - mae: 11.4494 - val_loss: 1091.6823 - val_mse: 1091.6823 - val_mae: 18.8497\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.7318 - mse: 147.7318 - mae: 8.1227 - val_loss: 451.6042 - val_mse: 451.6042 - val_mae: 10.0448\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.9753 - mse: 214.9753 - mae: 9.4866 - val_loss: 531.4238 - val_mse: 531.4238 - val_mae: 10.7604\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 132.6909 - mse: 132.6909 - mae: 7.2459 - val_loss: 821.1566 - val_mse: 821.1567 - val_mae: 13.9644\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 426.0591 - mse: 426.0591 - mae: 9.8143 - val_loss: 344.9868 - val_mse: 344.9868 - val_mae: 9.2792\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 283.9713 - mse: 283.9713 - mae: 9.5058 - val_loss: 603.9830 - val_mse: 603.9830 - val_mae: 12.2390\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 231.6315 - mse: 231.6315 - mae: 9.1434 - val_loss: 562.5967 - val_mse: 562.5967 - val_mae: 11.7227\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 274.2833 - mse: 274.2833 - mae: 9.1456 - val_loss: 278.6016 - val_mse: 278.6016 - val_mae: 8.8665\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 238.9055 - mse: 238.9055 - mae: 8.8932 - val_loss: 940.3466 - val_mse: 940.3466 - val_mae: 17.6082\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 343.1325 - mse: 343.1325 - mae: 10.3506 - val_loss: 734.5932 - val_mse: 734.5932 - val_mae: 13.5049\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 197.2896 - mse: 197.2896 - mae: 8.6166 - val_loss: 359.6330 - val_mse: 359.6330 - val_mae: 9.1508\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 226.2568 - mse: 226.2568 - mae: 9.4275 - val_loss: 546.6013 - val_mse: 546.6013 - val_mae: 12.5890\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 159.5534 - mse: 159.5534 - mae: 8.2680 - val_loss: 489.9461 - val_mse: 489.9461 - val_mae: 13.2681\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 122.7066 - mse: 122.7066 - mae: 7.5589 - val_loss: 270.4162 - val_mse: 270.4162 - val_mae: 9.5618\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 155.8642 - mse: 155.8642 - mae: 7.9950 - val_loss: 496.9734 - val_mse: 496.9734 - val_mae: 13.2710\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 201.9903 - mse: 201.9903 - mae: 8.8162 - val_loss: 725.5801 - val_mse: 725.5801 - val_mae: 14.7271\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 203.2240 - mse: 203.2240 - mae: 8.3811 - val_loss: 292.7284 - val_mse: 292.7284 - val_mae: 9.0329\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 250.5864 - mse: 250.5864 - mae: 9.9864 - val_loss: 708.3394 - val_mse: 708.3395 - val_mae: 12.9391\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 216.3820 - mse: 216.3820 - mae: 8.7089 - val_loss: 794.2857 - val_mse: 794.2857 - val_mae: 13.4888\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 173.1313 - mse: 173.1313 - mae: 8.4787 - val_loss: 355.7042 - val_mse: 355.7042 - val_mae: 9.4556\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.2018 - mse: 215.2018 - mae: 8.9769 - val_loss: 535.9721 - val_mse: 535.9721 - val_mae: 12.7599\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.9547 - mse: 238.9547 - mae: 9.3016 - val_loss: 830.9841 - val_mse: 830.9842 - val_mae: 17.3422\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.2428 - mse: 204.2428 - mae: 8.1399 - val_loss: 298.9714 - val_mse: 298.9714 - val_mae: 10.0883\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.8713 - mse: 236.8713 - mae: 9.0339 - val_loss: 489.7197 - val_mse: 489.7197 - val_mae: 12.2673\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 191.6781 - mse: 191.6781 - mae: 8.0649 - val_loss: 649.6029 - val_mse: 649.6029 - val_mae: 13.4150\n",
      "Epoch 376/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 188.8069 - mse: 188.8069 - mae: 7.4283 - val_loss: 287.0620 - val_mse: 287.0620 - val_mae: 9.6436\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 170.0572 - mse: 170.0572 - mae: 8.0355 - val_loss: 438.7285 - val_mse: 438.7285 - val_mae: 11.6965\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 200.3116 - mse: 200.3116 - mae: 8.5567 - val_loss: 414.4665 - val_mse: 414.4665 - val_mae: 11.1796\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.8965 - mse: 147.8965 - mae: 7.5352 - val_loss: 463.4541 - val_mse: 463.4541 - val_mae: 11.6674\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 141.1232 - mse: 141.1232 - mae: 7.4250 - val_loss: 279.5754 - val_mse: 279.5754 - val_mae: 10.2789\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 205.1266 - mse: 205.1266 - mae: 8.7108 - val_loss: 435.2419 - val_mse: 435.2420 - val_mae: 12.7833\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 148.6331 - mse: 148.6331 - mae: 7.6088 - val_loss: 413.6333 - val_mse: 413.6333 - val_mae: 12.0595\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.0079 - mse: 204.0079 - mae: 8.8652 - val_loss: 417.5930 - val_mse: 417.5930 - val_mae: 11.8393\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.6921 - mse: 236.6921 - mae: 8.7917 - val_loss: 827.0446 - val_mse: 827.0446 - val_mae: 15.0738\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 220.6990 - mse: 220.6990 - mae: 8.5930 - val_loss: 326.8435 - val_mse: 326.8435 - val_mae: 9.4468\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 174.3445 - mse: 174.3445 - mae: 8.0316 - val_loss: 547.5554 - val_mse: 547.5554 - val_mae: 12.6253\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 163.0962 - mse: 163.0962 - mae: 7.6341 - val_loss: 562.2474 - val_mse: 562.2474 - val_mae: 13.9574\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 187.4206 - mse: 187.4206 - mae: 7.6027 - val_loss: 482.0951 - val_mse: 482.0951 - val_mae: 12.5022\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.9957 - mse: 168.9957 - mae: 8.1440 - val_loss: 438.8375 - val_mse: 438.8375 - val_mae: 11.3624\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 198.6643 - mse: 198.6643 - mae: 8.3101 - val_loss: 506.5853 - val_mse: 506.5853 - val_mae: 12.3354\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 106.4427 - mse: 106.4427 - mae: 7.1372 - val_loss: 448.4522 - val_mse: 448.4522 - val_mae: 11.5876\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 145.3497 - mse: 145.3497 - mae: 7.3489 - val_loss: 349.7229 - val_mse: 349.7229 - val_mae: 10.2347\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 193.7245 - mse: 193.7245 - mae: 7.9326 - val_loss: 530.9194 - val_mse: 530.9194 - val_mae: 12.4130\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.3403 - mse: 204.3403 - mae: 8.1045 - val_loss: 565.3779 - val_mse: 565.3779 - val_mae: 12.5543\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 142.4605 - mse: 142.4605 - mae: 7.5819 - val_loss: 499.3112 - val_mse: 499.3112 - val_mae: 11.6705\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 177.2695 - mse: 177.2695 - mae: 7.7966 - val_loss: 641.9311 - val_mse: 641.9311 - val_mae: 12.7987\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 138.2891 - mse: 138.2891 - mae: 7.5338 - val_loss: 291.0712 - val_mse: 291.0712 - val_mae: 9.3979\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 215.7776 - mse: 215.7776 - mae: 8.8202 - val_loss: 664.2973 - val_mse: 664.2973 - val_mae: 14.6637\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 197.8429 - mse: 197.8429 - mae: 8.2960 - val_loss: 434.7416 - val_mse: 434.7416 - val_mae: 11.3740\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.1286 - mse: 223.1286 - mae: 8.2223 - val_loss: 412.7876 - val_mse: 412.7876 - val_mae: 10.7401\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 134.4202 - mse: 134.4202 - mae: 7.5938 - val_loss: 368.4130 - val_mse: 368.4130 - val_mae: 10.3448\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 151.0313 - mse: 151.0313 - mae: 7.4809 - val_loss: 572.3064 - val_mse: 572.3064 - val_mae: 12.4338\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 134.7508 - mse: 134.7508 - mae: 7.2235 - val_loss: 603.8746 - val_mse: 603.8746 - val_mae: 12.4886\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 159.9646 - mse: 159.9646 - mae: 7.7267 - val_loss: 497.1379 - val_mse: 497.1379 - val_mae: 10.9399\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 215.8993 - mse: 215.8993 - mae: 8.3309 - val_loss: 414.8891 - val_mse: 414.8891 - val_mae: 10.2574\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 239.7576 - mse: 239.7576 - mae: 8.9002 - val_loss: 629.9726 - val_mse: 629.9726 - val_mae: 14.4885\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 198.7510 - mse: 198.7510 - mae: 8.6210 - val_loss: 576.7794 - val_mse: 576.7794 - val_mae: 13.6283\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 193.6021 - mse: 193.6021 - mae: 8.5071 - val_loss: 347.4698 - val_mse: 347.4698 - val_mae: 9.7740\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 128.8185 - mse: 128.8185 - mae: 7.5127 - val_loss: 505.2608 - val_mse: 505.2608 - val_mae: 12.2439\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 205.2788 - mse: 205.2788 - mae: 7.8374 - val_loss: 409.1086 - val_mse: 409.1086 - val_mae: 11.3927\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.1244 - mse: 240.1244 - mae: 8.1655 - val_loss: 257.2970 - val_mse: 257.2970 - val_mae: 9.5078\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.4404 - mse: 262.4404 - mae: 9.0097 - val_loss: 638.8965 - val_mse: 638.8965 - val_mae: 14.6430\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 147.1952 - mse: 147.1952 - mae: 7.3806 - val_loss: 465.1945 - val_mse: 465.1945 - val_mae: 12.0203\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 217.5217 - mse: 217.5217 - mae: 9.0601 - val_loss: 243.9385 - val_mse: 243.9385 - val_mae: 8.9010\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 192.2140 - mse: 192.2140 - mae: 8.0347 - val_loss: 311.8321 - val_mse: 311.8321 - val_mae: 11.3793\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8318 - mse: 223.8318 - mae: 8.3152 - val_loss: 852.8012 - val_mse: 852.8012 - val_mae: 17.3258\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 230.5613 - mse: 230.5613 - mae: 8.6669 - val_loss: 596.8881 - val_mse: 596.8881 - val_mae: 12.4560\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 124.0343 - mse: 124.0343 - mae: 7.4270 - val_loss: 453.2173 - val_mse: 453.2173 - val_mae: 10.7430\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.0836 - mse: 137.0836 - mae: 7.9044 - val_loss: 631.2305 - val_mse: 631.2305 - val_mae: 13.3175\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 214.9737 - mse: 214.9737 - mae: 8.9289 - val_loss: 405.1935 - val_mse: 405.1936 - val_mae: 11.0806\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.0301 - mse: 223.0301 - mae: 8.8200 - val_loss: 733.2328 - val_mse: 733.2328 - val_mae: 14.2944\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 190.2675 - mse: 190.2675 - mae: 8.3504 - val_loss: 773.9370 - val_mse: 773.9370 - val_mae: 13.2718\n",
      "Epoch 423/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 188.2539 - mse: 188.2539 - mae: 8.0304 - val_loss: 482.3386 - val_mse: 482.3386 - val_mae: 10.2218\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 164.7622 - mse: 164.7622 - mae: 7.3872 - val_loss: 536.7443 - val_mse: 536.7443 - val_mae: 11.7688\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 126.8963 - mse: 126.8963 - mae: 7.1888 - val_loss: 455.7603 - val_mse: 455.7603 - val_mae: 12.0698\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 242.2440 - mse: 242.2440 - mae: 8.5271 - val_loss: 428.5252 - val_mse: 428.5252 - val_mae: 12.6153\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 185.8698 - mse: 185.8698 - mae: 8.2755 - val_loss: 718.2474 - val_mse: 718.2474 - val_mae: 14.5943\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 218.5320 - mse: 218.5320 - mae: 8.5085 - val_loss: 378.4066 - val_mse: 378.4066 - val_mae: 10.4059\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 230.2442 - mse: 230.2442 - mae: 8.1620 - val_loss: 528.5940 - val_mse: 528.5940 - val_mae: 11.9822\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 191.1719 - mse: 191.1719 - mae: 8.2919 - val_loss: 530.7860 - val_mse: 530.7860 - val_mae: 12.0780\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 143.9105 - mse: 143.9105 - mae: 7.8793 - val_loss: 419.7470 - val_mse: 419.7471 - val_mae: 11.0972\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 165.2735 - mse: 165.2735 - mae: 7.8220 - val_loss: 482.4965 - val_mse: 482.4965 - val_mae: 11.8451\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 163.2496 - mse: 163.2496 - mae: 7.4366 - val_loss: 466.3994 - val_mse: 466.3994 - val_mae: 11.7912\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.1147 - mse: 120.1147 - mae: 7.1444 - val_loss: 633.0769 - val_mse: 633.0769 - val_mae: 13.7122\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.7115 - mse: 168.7115 - mae: 8.1125 - val_loss: 339.3577 - val_mse: 339.3577 - val_mae: 10.1533\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 196.9044 - mse: 196.9044 - mae: 8.1763 - val_loss: 551.6568 - val_mse: 551.6568 - val_mae: 12.7124\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 341.2600 - mse: 341.2600 - mae: 9.5146 - val_loss: 457.9193 - val_mse: 457.9193 - val_mae: 11.4415\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 208.1620 - mse: 208.1620 - mae: 7.7241 - val_loss: 300.4563 - val_mse: 300.4563 - val_mae: 9.7516\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 171.8342 - mse: 171.8342 - mae: 7.6589 - val_loss: 413.8280 - val_mse: 413.8280 - val_mae: 11.7529\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 236.4733 - mse: 236.4733 - mae: 8.5691 - val_loss: 504.9109 - val_mse: 504.9109 - val_mae: 13.6492\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 199.2659 - mse: 199.2659 - mae: 8.5346 - val_loss: 481.9609 - val_mse: 481.9609 - val_mae: 12.1017\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.8193 - mse: 200.8193 - mae: 8.4693 - val_loss: 743.3911 - val_mse: 743.3911 - val_mae: 13.7104\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.6959 - mse: 168.6959 - mae: 8.0027 - val_loss: 422.0009 - val_mse: 422.0009 - val_mae: 9.9267\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 262.4316 - mse: 262.4316 - mae: 9.6590 - val_loss: 310.6648 - val_mse: 310.6648 - val_mae: 10.0592\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 184.4304 - mse: 184.4304 - mae: 8.9282 - val_loss: 422.8550 - val_mse: 422.8550 - val_mae: 12.8312\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 255.8909 - mse: 255.8909 - mae: 8.8991 - val_loss: 243.7705 - val_mse: 243.7705 - val_mae: 9.6154\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 206.2598 - mse: 206.2598 - mae: 8.4153 - val_loss: 392.5719 - val_mse: 392.5719 - val_mae: 12.1964\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 160.0244 - mse: 160.0244 - mae: 7.9537 - val_loss: 639.2947 - val_mse: 639.2947 - val_mae: 14.7540\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 118.6423 - mse: 118.6423 - mae: 7.0070 - val_loss: 445.9665 - val_mse: 445.9665 - val_mae: 11.3635\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 135.7835 - mse: 135.7835 - mae: 6.9669 - val_loss: 406.4106 - val_mse: 406.4107 - val_mae: 10.6479\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6541 - mse: 226.6541 - mae: 7.7169 - val_loss: 341.5705 - val_mse: 341.5705 - val_mae: 10.6636\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 150.1288 - mse: 150.1288 - mae: 7.5245 - val_loss: 303.6921 - val_mse: 303.6921 - val_mae: 10.3945\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 266.6768 - mse: 266.6768 - mae: 8.3609 - val_loss: 725.7079 - val_mse: 725.7079 - val_mae: 13.6127\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 206.4450 - mse: 206.4450 - mae: 8.5093 - val_loss: 401.4392 - val_mse: 401.4392 - val_mae: 10.1403\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 175.8063 - mse: 175.8063 - mae: 8.3538 - val_loss: 531.7173 - val_mse: 531.7173 - val_mae: 11.8333\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 219.2949 - mse: 219.2949 - mae: 8.0370 - val_loss: 689.9413 - val_mse: 689.9413 - val_mae: 14.3430\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 180.5845 - mse: 180.5845 - mae: 8.0599 - val_loss: 517.5840 - val_mse: 517.5840 - val_mae: 12.4210\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 190.4539 - mse: 190.4539 - mae: 7.9302 - val_loss: 724.8409 - val_mse: 724.8409 - val_mae: 14.0420\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 182.8328 - mse: 182.8328 - mae: 8.2100 - val_loss: 656.8127 - val_mse: 656.8127 - val_mae: 12.9531\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 187.3594 - mse: 187.3594 - mae: 8.0070 - val_loss: 453.0797 - val_mse: 453.0797 - val_mae: 10.5401\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 150.0144 - mse: 150.0144 - mae: 7.4682 - val_loss: 761.2545 - val_mse: 761.2545 - val_mae: 12.9794\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 174.7652 - mse: 174.7652 - mae: 7.7335 - val_loss: 411.3751 - val_mse: 411.3751 - val_mae: 9.4522\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 196.5944 - mse: 196.5944 - mae: 8.4546 - val_loss: 471.2066 - val_mse: 471.2066 - val_mae: 12.1474\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 158.9244 - mse: 158.9244 - mae: 7.6110 - val_loss: 622.2430 - val_mse: 622.2430 - val_mae: 15.0202\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 161.5298 - mse: 161.5298 - mae: 8.2286 - val_loss: 454.6879 - val_mse: 454.6879 - val_mae: 11.3683\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 194.2732 - mse: 194.2732 - mae: 8.2043 - val_loss: 522.4680 - val_mse: 522.4680 - val_mae: 11.6286\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 131.9012 - mse: 131.9012 - mae: 7.3060 - val_loss: 722.2250 - val_mse: 722.2250 - val_mae: 13.8327\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 378.6631 - mse: 378.6631 - mae: 10.1459 - val_loss: 282.0417 - val_mse: 282.0417 - val_mae: 9.3173\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3364 - mse: 208.3364 - mae: 8.4670 - val_loss: 822.6005 - val_mse: 822.6005 - val_mae: 17.3006\n",
      "Epoch 470/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 395.0150 - mse: 395.0150 - mae: 10.1323 - val_loss: 439.1421 - val_mse: 439.1421 - val_mae: 12.8474\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 253.8880 - mse: 253.8880 - mae: 9.2429 - val_loss: 316.2660 - val_mse: 316.2660 - val_mae: 10.7161\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 195.0548 - mse: 195.0548 - mae: 7.8688 - val_loss: 466.7209 - val_mse: 466.7209 - val_mae: 11.9282\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.6424 - mse: 211.6424 - mae: 8.2114 - val_loss: 399.5693 - val_mse: 399.5693 - val_mae: 10.8092\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 197.5365 - mse: 197.5365 - mae: 7.7391 - val_loss: 300.9568 - val_mse: 300.9568 - val_mae: 9.8474\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6448 - mse: 207.6448 - mae: 8.5434 - val_loss: 352.9140 - val_mse: 352.9140 - val_mae: 10.6358\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 169.0471 - mse: 169.0471 - mae: 8.0876 - val_loss: 450.8850 - val_mse: 450.8850 - val_mae: 11.3707\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 216.9747 - mse: 216.9747 - mae: 8.4137 - val_loss: 387.5864 - val_mse: 387.5864 - val_mae: 10.9304\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 192.7714 - mse: 192.7714 - mae: 8.2852 - val_loss: 272.1635 - val_mse: 272.1635 - val_mae: 10.2860\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 184.4415 - mse: 184.4415 - mae: 7.8898 - val_loss: 357.3119 - val_mse: 357.3119 - val_mae: 12.5127\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.7429 - mse: 210.7429 - mae: 8.5602 - val_loss: 779.9318 - val_mse: 779.9318 - val_mae: 16.5260\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 274.2656 - mse: 274.2656 - mae: 8.9467 - val_loss: 268.3166 - val_mse: 268.3166 - val_mae: 8.9708\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 187.1214 - mse: 187.1214 - mae: 8.4511 - val_loss: 230.8174 - val_mse: 230.8174 - val_mae: 9.3706\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.6050 - mse: 211.6050 - mae: 8.3399 - val_loss: 600.0847 - val_mse: 600.0847 - val_mae: 15.9834\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 320.8771 - mse: 320.8771 - mae: 10.1407 - val_loss: 279.3168 - val_mse: 279.3168 - val_mae: 10.5782\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 336.2808 - mse: 336.2808 - mae: 10.5733 - val_loss: 214.4237 - val_mse: 214.4237 - val_mae: 9.0506\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 166.5784 - mse: 166.5784 - mae: 8.0113 - val_loss: 594.7998 - val_mse: 594.7998 - val_mae: 15.6737\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 166.2939 - mse: 166.2939 - mae: 7.9381 - val_loss: 367.6623 - val_mse: 367.6623 - val_mae: 11.2235\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 171.4337 - mse: 171.4337 - mae: 8.1764 - val_loss: 292.7404 - val_mse: 292.7404 - val_mae: 10.1753\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 166.4101 - mse: 166.4101 - mae: 7.7062 - val_loss: 435.6254 - val_mse: 435.6253 - val_mae: 12.9617\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 381.9380 - mse: 381.9380 - mae: 9.4204 - val_loss: 353.1756 - val_mse: 353.1756 - val_mae: 12.4937\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.5216 - mse: 223.5216 - mae: 8.0911 - val_loss: 328.5244 - val_mse: 328.5244 - val_mae: 11.6558\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 159.9064 - mse: 159.9064 - mae: 7.2416 - val_loss: 313.6503 - val_mse: 313.6503 - val_mae: 10.6130\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 198.4825 - mse: 198.4825 - mae: 8.5194 - val_loss: 427.2311 - val_mse: 427.2311 - val_mae: 11.6437\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 264.3546 - mse: 264.3546 - mae: 8.1680 - val_loss: 566.3497 - val_mse: 566.3497 - val_mae: 12.5038\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 142.4051 - mse: 142.4051 - mae: 7.6950 - val_loss: 404.4574 - val_mse: 404.4574 - val_mae: 10.2229\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 181.7781 - mse: 181.7781 - mae: 7.4232 - val_loss: 389.2210 - val_mse: 389.2210 - val_mae: 10.6927\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.5520 - mse: 137.5520 - mae: 7.2656 - val_loss: 438.1297 - val_mse: 438.1297 - val_mae: 12.5587\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 233.8736 - mse: 233.8736 - mae: 7.9984 - val_loss: 254.8806 - val_mse: 254.8806 - val_mae: 10.4340\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 196.8523 - mse: 196.8523 - mae: 7.0857 - val_loss: 452.7822 - val_mse: 452.7822 - val_mae: 12.9694\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 88.7677 - mse: 88.7677 - mae: 6.3478 - val_loss: 483.2593 - val_mse: 483.2593 - val_mae: 12.4154\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 133.7190 - mse: 133.7190 - mae: 6.5169 - val_loss: 406.6288 - val_mse: 406.6288 - val_mae: 11.5000\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 98.5055 - mse: 98.5055 - mae: 6.5879 - val_loss: 443.7706 - val_mse: 443.7706 - val_mae: 11.7914\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 247.7781 - mse: 247.7781 - mae: 8.3072 - val_loss: 377.0292 - val_mse: 377.0292 - val_mae: 11.5355\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 270.9749 - mse: 270.9749 - mae: 8.5736 - val_loss: 621.0273 - val_mse: 621.0273 - val_mae: 13.3287\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 124.0920 - mse: 124.0920 - mae: 7.2863 - val_loss: 416.5099 - val_mse: 416.5099 - val_mae: 10.7874\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 188.7003 - mse: 188.7003 - mae: 7.7954 - val_loss: 298.7008 - val_mse: 298.7008 - val_mae: 10.3047\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 248.1507 - mse: 248.1507 - mae: 7.7583 - val_loss: 418.8290 - val_mse: 418.8289 - val_mae: 13.0916\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 119.4820 - mse: 119.4820 - mae: 6.9965 - val_loss: 424.1381 - val_mse: 424.1381 - val_mae: 13.1954\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.5895 - mse: 147.5895 - mae: 7.1405 - val_loss: 484.9687 - val_mse: 484.9687 - val_mae: 12.4953\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 276.6560 - mse: 276.6560 - mae: 8.4768 - val_loss: 314.8441 - val_mse: 314.8441 - val_mae: 10.0230\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 108.8898 - mse: 108.8898 - mae: 6.6839 - val_loss: 478.2733 - val_mse: 478.2733 - val_mae: 12.0919\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 138.4324 - mse: 138.4324 - mae: 7.5149 - val_loss: 408.4085 - val_mse: 408.4085 - val_mae: 11.7990\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 225.6195 - mse: 225.6195 - mae: 8.5743 - val_loss: 335.7696 - val_mse: 335.7696 - val_mae: 11.1748\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 133.3465 - mse: 133.3465 - mae: 6.8362 - val_loss: 342.4294 - val_mse: 342.4294 - val_mae: 11.4179\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 108.7067 - mse: 108.7067 - mae: 7.3601 - val_loss: 695.5094 - val_mse: 695.5094 - val_mae: 15.1606\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 149.8192 - mse: 149.8192 - mae: 7.5883 - val_loss: 433.2638 - val_mse: 433.2639 - val_mae: 10.9336\n",
      "Epoch 517/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 227.5603 - mse: 227.5603 - mae: 8.7433 - val_loss: 261.7254 - val_mse: 261.7254 - val_mae: 9.4078\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 262.6491 - mse: 262.6491 - mae: 8.9677 - val_loss: 775.8719 - val_mse: 775.8719 - val_mae: 17.1312\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.5127 - mse: 285.5127 - mae: 9.6613 - val_loss: 373.5699 - val_mse: 373.5699 - val_mae: 11.3135\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 193.6176 - mse: 193.6176 - mae: 7.9965 - val_loss: 264.2231 - val_mse: 264.2231 - val_mae: 9.5555\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 191.8475 - mse: 191.8475 - mae: 7.0616 - val_loss: 478.6184 - val_mse: 478.6184 - val_mae: 12.2281\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 115.1314 - mse: 115.1314 - mae: 6.4434 - val_loss: 475.9267 - val_mse: 475.9267 - val_mae: 12.0139\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 131.7930 - mse: 131.7930 - mae: 6.8660 - val_loss: 264.8892 - val_mse: 264.8892 - val_mae: 9.5392\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 178.5051 - mse: 178.5051 - mae: 8.0330 - val_loss: 734.2461 - val_mse: 734.2461 - val_mae: 14.9304\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 288.3038 - mse: 288.3038 - mae: 8.5490 - val_loss: 604.4196 - val_mse: 604.4196 - val_mae: 12.8257\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 164.7075 - mse: 164.7075 - mae: 7.7142 - val_loss: 398.8450 - val_mse: 398.8449 - val_mae: 10.5359\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 152.0361 - mse: 152.0361 - mae: 7.8995 - val_loss: 354.7761 - val_mse: 354.7761 - val_mae: 10.6216\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 115.8354 - mse: 115.8354 - mae: 7.0797 - val_loss: 389.5041 - val_mse: 389.5041 - val_mae: 11.8376\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144.7723 - mse: 144.7723 - mae: 7.6824 - val_loss: 379.8795 - val_mse: 379.8795 - val_mae: 12.1075\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 202.8351 - mse: 202.8351 - mae: 8.1114 - val_loss: 442.1704 - val_mse: 442.1704 - val_mae: 11.5491\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 148.9115 - mse: 148.9115 - mae: 7.3954 - val_loss: 580.3398 - val_mse: 580.3398 - val_mae: 12.2823\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 127.5761 - mse: 127.5761 - mae: 6.9096 - val_loss: 632.7405 - val_mse: 632.7405 - val_mae: 12.3150\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 184.2048 - mse: 184.2048 - mae: 7.6866 - val_loss: 333.0603 - val_mse: 333.0603 - val_mae: 9.3962\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 226.4669 - mse: 226.4669 - mae: 8.7391 - val_loss: 455.5157 - val_mse: 455.5156 - val_mae: 12.9267\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 209.5461 - mse: 209.5461 - mae: 7.9622 - val_loss: 392.7593 - val_mse: 392.7593 - val_mae: 12.2683\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 144.5844 - mse: 144.5844 - mae: 7.1015 - val_loss: 294.1756 - val_mse: 294.1756 - val_mae: 10.9627\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.0622 - mse: 221.0622 - mae: 6.7280 - val_loss: 293.7198 - val_mse: 293.7198 - val_mae: 11.2539\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 149.2182 - mse: 149.2182 - mae: 7.2170 - val_loss: 419.9671 - val_mse: 419.9671 - val_mae: 12.7638\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 215.3681 - mse: 215.3681 - mae: 8.5049 - val_loss: 237.5571 - val_mse: 237.5571 - val_mae: 9.6033\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 170.8774 - mse: 170.8774 - mae: 6.9841 - val_loss: 400.6538 - val_mse: 400.6538 - val_mae: 11.7794\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 176.3628 - mse: 176.3628 - mae: 8.2625 - val_loss: 718.9995 - val_mse: 718.9995 - val_mae: 14.0185\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 198.4637 - mse: 198.4637 - mae: 8.3826 - val_loss: 276.4605 - val_mse: 276.4605 - val_mae: 9.3604\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 174.5710 - mse: 174.5710 - mae: 8.1668 - val_loss: 418.0429 - val_mse: 418.0429 - val_mae: 11.8769\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.2285 - mse: 223.2285 - mae: 8.4588 - val_loss: 524.3717 - val_mse: 524.3717 - val_mae: 14.1842\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 150.7096 - mse: 150.7096 - mae: 7.3032 - val_loss: 436.6635 - val_mse: 436.6635 - val_mae: 12.5117\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 196.7912 - mse: 196.7912 - mae: 7.9941 - val_loss: 590.4706 - val_mse: 590.4706 - val_mae: 12.7383\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 133.1000 - mse: 133.1000 - mae: 7.2040 - val_loss: 455.0040 - val_mse: 455.0040 - val_mae: 10.5750\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 181.0148 - mse: 181.0148 - mae: 8.1580 - val_loss: 401.2267 - val_mse: 401.2268 - val_mae: 10.4057\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 133.7180 - mse: 133.7180 - mae: 7.3196 - val_loss: 593.1343 - val_mse: 593.1343 - val_mae: 14.5440\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 169.8514 - mse: 169.8514 - mae: 8.2450 - val_loss: 380.3156 - val_mse: 380.3156 - val_mae: 11.4944\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 142.7721 - mse: 142.7721 - mae: 7.0219 - val_loss: 253.1728 - val_mse: 253.1728 - val_mae: 9.0503\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.7913 - mse: 246.7913 - mae: 9.1919 - val_loss: 536.4003 - val_mse: 536.4003 - val_mae: 13.2923\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 165.3189 - mse: 165.3189 - mae: 7.5787 - val_loss: 301.8395 - val_mse: 301.8395 - val_mae: 10.2326\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 145.1302 - mse: 145.1302 - mae: 7.3310 - val_loss: 208.3741 - val_mse: 208.3742 - val_mae: 8.6881\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 168.5624 - mse: 168.5624 - mae: 7.9836 - val_loss: 423.0653 - val_mse: 423.0653 - val_mae: 12.3927\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 295.4007 - mse: 295.4007 - mae: 9.4032 - val_loss: 357.8125 - val_mse: 357.8125 - val_mae: 11.7004\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 181.0494 - mse: 181.0494 - mae: 7.7283 - val_loss: 578.2496 - val_mse: 578.2496 - val_mae: 13.2210\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 197.0565 - mse: 197.0565 - mae: 8.3472 - val_loss: 405.3444 - val_mse: 405.3444 - val_mae: 9.9935\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 206.7081 - mse: 206.7081 - mae: 8.6568 - val_loss: 355.4949 - val_mse: 355.4949 - val_mae: 9.2081\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 129.7608 - mse: 129.7608 - mae: 6.6120 - val_loss: 501.9547 - val_mse: 501.9547 - val_mae: 11.6445\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 175.3674 - mse: 175.3674 - mae: 8.1291 - val_loss: 569.6752 - val_mse: 569.6752 - val_mae: 12.5739\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 188.5641 - mse: 188.5641 - mae: 7.5343 - val_loss: 335.3730 - val_mse: 335.3730 - val_mae: 10.1365\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 148.5719 - mse: 148.5719 - mae: 7.5307 - val_loss: 453.1086 - val_mse: 453.1086 - val_mae: 12.2815\n",
      "Epoch 564/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 168.4439 - mse: 168.4439 - mae: 7.5525 - val_loss: 386.8161 - val_mse: 386.8161 - val_mae: 12.1137\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.3624 - mse: 137.3624 - mae: 7.4312 - val_loss: 410.1382 - val_mse: 410.1382 - val_mae: 12.0962\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.6720 - mse: 240.6720 - mae: 8.6286 - val_loss: 372.9322 - val_mse: 372.9322 - val_mae: 10.6461\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.8819 - mse: 200.8819 - mae: 7.6733 - val_loss: 795.5975 - val_mse: 795.5975 - val_mae: 14.5218\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 213.4767 - mse: 213.4767 - mae: 8.4804 - val_loss: 331.3739 - val_mse: 331.3739 - val_mae: 9.0368\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.4624 - mse: 200.4624 - mae: 8.3140 - val_loss: 591.6912 - val_mse: 591.6912 - val_mae: 12.9149\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 157.8947 - mse: 157.8947 - mae: 7.2323 - val_loss: 468.7143 - val_mse: 468.7143 - val_mae: 12.0382\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 167.7821 - mse: 167.7821 - mae: 7.9055 - val_loss: 253.3375 - val_mse: 253.3375 - val_mae: 9.2965\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 169.5619 - mse: 169.5619 - mae: 7.9480 - val_loss: 360.8650 - val_mse: 360.8650 - val_mae: 11.9670\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 219.2940 - mse: 219.2940 - mae: 7.8002 - val_loss: 270.5767 - val_mse: 270.5767 - val_mae: 10.3920\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 190.7652 - mse: 190.7652 - mae: 7.8153 - val_loss: 422.7250 - val_mse: 422.7250 - val_mae: 12.2249\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 114.2770 - mse: 114.2770 - mae: 6.7292 - val_loss: 482.1334 - val_mse: 482.1334 - val_mae: 12.1469\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 192.3103 - mse: 192.3103 - mae: 8.4358 - val_loss: 298.7345 - val_mse: 298.7345 - val_mae: 9.7500\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 193.2272 - mse: 193.2272 - mae: 7.8334 - val_loss: 325.9956 - val_mse: 325.9956 - val_mae: 10.9005\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 227.8098 - mse: 227.8098 - mae: 8.0405 - val_loss: 633.0054 - val_mse: 633.0054 - val_mae: 14.7558\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 192.9388 - mse: 192.9388 - mae: 8.5355 - val_loss: 733.0002 - val_mse: 733.0002 - val_mae: 14.2149\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 116.2525 - mse: 116.2525 - mae: 6.9946 - val_loss: 312.8374 - val_mse: 312.8374 - val_mae: 9.5881\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 273.3134 - mse: 273.3134 - mae: 9.6353 - val_loss: 337.7165 - val_mse: 337.7165 - val_mae: 9.8718\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 153.9191 - mse: 153.9191 - mae: 7.2165 - val_loss: 696.1506 - val_mse: 696.1506 - val_mae: 15.6543\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 166.4925 - mse: 166.4925 - mae: 7.6641 - val_loss: 346.3956 - val_mse: 346.3956 - val_mae: 10.7741\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 152.5436 - mse: 152.5436 - mae: 7.0625 - val_loss: 543.8591 - val_mse: 543.8591 - val_mae: 12.8182\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 181.6823 - mse: 181.6823 - mae: 7.3895 - val_loss: 423.6461 - val_mse: 423.6461 - val_mae: 11.1209\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 146.6368 - mse: 146.6368 - mae: 6.9814 - val_loss: 397.4330 - val_mse: 397.4329 - val_mae: 10.8634\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 188.7770 - mse: 188.7770 - mae: 7.7102 - val_loss: 522.9131 - val_mse: 522.9131 - val_mae: 12.7194\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 171.4069 - mse: 171.4069 - mae: 8.2412 - val_loss: 718.4269 - val_mse: 718.4269 - val_mae: 14.7095\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.1419 - mse: 229.1419 - mae: 8.0122 - val_loss: 232.9657 - val_mse: 232.9658 - val_mae: 9.5298\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 239.1560 - mse: 239.1560 - mae: 8.2799 - val_loss: 559.7681 - val_mse: 559.7681 - val_mae: 13.7713\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 142.1600 - mse: 142.1600 - mae: 7.8091 - val_loss: 560.7459 - val_mse: 560.7459 - val_mae: 13.2855\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 111.1069 - mse: 111.1069 - mae: 6.7659 - val_loss: 403.5881 - val_mse: 403.5881 - val_mae: 10.6276\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 335.7452 - mse: 335.7452 - mae: 8.2003 - val_loss: 487.1299 - val_mse: 487.1299 - val_mae: 11.3601\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 155.6989 - mse: 155.6989 - mae: 7.0672 - val_loss: 414.0543 - val_mse: 414.0543 - val_mae: 10.5945\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 112.3022 - mse: 112.3022 - mae: 7.1737 - val_loss: 386.3654 - val_mse: 386.3654 - val_mae: 10.6549\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 195.0669 - mse: 195.0669 - mae: 7.7672 - val_loss: 512.0365 - val_mse: 512.0365 - val_mae: 12.2608\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 138.4869 - mse: 138.4869 - mae: 6.6907 - val_loss: 365.1091 - val_mse: 365.1091 - val_mae: 10.4739\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 168.2405 - mse: 168.2405 - mae: 7.3119 - val_loss: 242.2467 - val_mse: 242.2467 - val_mae: 9.6066\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 139.1381 - mse: 139.1381 - mae: 7.1636 - val_loss: 892.0632 - val_mse: 892.0632 - val_mae: 18.0941\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 241.5047 - mse: 241.5047 - mae: 8.5529 - val_loss: 414.7744 - val_mse: 414.7744 - val_mae: 10.1784\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 309.9434 - mse: 309.9434 - mae: 10.1349 - val_loss: 311.2844 - val_mse: 311.2844 - val_mae: 9.4757\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 180.9728 - mse: 180.9728 - mae: 7.7781 - val_loss: 523.9909 - val_mse: 523.9909 - val_mae: 13.3359\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 142.2157 - mse: 142.2157 - mae: 7.1208 - val_loss: 214.2024 - val_mse: 214.2024 - val_mae: 9.3983\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 185.5704 - mse: 185.5704 - mae: 7.5352 - val_loss: 391.2902 - val_mse: 391.2902 - val_mae: 12.3631\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 155.5810 - mse: 155.5810 - mae: 7.5870 - val_loss: 474.2603 - val_mse: 474.2603 - val_mae: 12.8345\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 217.8506 - mse: 217.8506 - mae: 8.1664 - val_loss: 255.8682 - val_mse: 255.8682 - val_mae: 9.8825\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 233.7415 - mse: 233.7415 - mae: 8.3563 - val_loss: 543.3946 - val_mse: 543.3946 - val_mae: 12.6839\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 312.1377 - mse: 312.1377 - mae: 9.2834 - val_loss: 570.2305 - val_mse: 570.2305 - val_mae: 11.8718\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 247.8335 - mse: 247.8335 - mae: 8.6020 - val_loss: 233.6071 - val_mse: 233.6071 - val_mae: 9.3001\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 196.8393 - mse: 196.8393 - mae: 8.3429 - val_loss: 371.5284 - val_mse: 371.5284 - val_mae: 13.7144\n",
      "Epoch 611/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 231.8577 - mse: 231.8577 - mae: 8.3539 - val_loss: 292.0812 - val_mse: 292.0812 - val_mae: 12.0604\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 185.2799 - mse: 185.2799 - mae: 7.6261 - val_loss: 307.5087 - val_mse: 307.5087 - val_mae: 11.3203\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144.3167 - mse: 144.3167 - mae: 7.3330 - val_loss: 262.4503 - val_mse: 262.4503 - val_mae: 9.9091\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 175.8005 - mse: 175.8005 - mae: 7.1964 - val_loss: 237.3780 - val_mse: 237.3780 - val_mae: 10.1369\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 160.4937 - mse: 160.4937 - mae: 7.5416 - val_loss: 336.8856 - val_mse: 336.8856 - val_mae: 12.0120\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 113.8150 - mse: 113.8150 - mae: 6.5036 - val_loss: 386.2459 - val_mse: 386.2459 - val_mae: 12.3494\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.7208 - mse: 208.7208 - mae: 7.7961 - val_loss: 361.4933 - val_mse: 361.4933 - val_mae: 11.9333\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 128.3976 - mse: 128.3976 - mae: 6.8922 - val_loss: 385.3651 - val_mse: 385.3651 - val_mae: 11.8523\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 195.1053 - mse: 195.1053 - mae: 7.3292 - val_loss: 273.4050 - val_mse: 273.4050 - val_mae: 10.0435\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 136.8204 - mse: 136.8204 - mae: 6.3400 - val_loss: 358.0448 - val_mse: 358.0448 - val_mae: 10.6644\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 119.0845 - mse: 119.0845 - mae: 6.7058 - val_loss: 451.1067 - val_mse: 451.1067 - val_mae: 11.4078\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220.1613 - mse: 220.1613 - mae: 7.7977 - val_loss: 339.8255 - val_mse: 339.8255 - val_mae: 10.3633\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 186.6464 - mse: 186.6464 - mae: 8.1025 - val_loss: 284.8539 - val_mse: 284.8539 - val_mae: 10.4937\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 146.5712 - mse: 146.5712 - mae: 7.0944 - val_loss: 358.1167 - val_mse: 358.1167 - val_mae: 12.8025\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 148.9732 - mse: 148.9732 - mae: 7.7725 - val_loss: 620.9297 - val_mse: 620.9297 - val_mae: 15.7355\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 135.1236 - mse: 135.1236 - mae: 7.6091 - val_loss: 371.6746 - val_mse: 371.6746 - val_mae: 11.0580\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 177.2509 - mse: 177.2509 - mae: 7.5262 - val_loss: 236.4665 - val_mse: 236.4665 - val_mae: 9.4610\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 188.5465 - mse: 188.5465 - mae: 7.9116 - val_loss: 635.7239 - val_mse: 635.7239 - val_mae: 15.8186\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149.8797 - mse: 149.8797 - mae: 8.3188 - val_loss: 474.7542 - val_mse: 474.7542 - val_mae: 12.6390\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.5106 - mse: 207.5106 - mae: 8.4710 - val_loss: 321.6839 - val_mse: 321.6839 - val_mae: 10.0155\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 245.0595 - mse: 245.0595 - mae: 8.7353 - val_loss: 319.8823 - val_mse: 319.8823 - val_mae: 9.8039\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 165.8105 - mse: 165.8105 - mae: 7.2449 - val_loss: 584.5547 - val_mse: 584.5547 - val_mae: 14.8656\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.5888 - mse: 137.5888 - mae: 7.4585 - val_loss: 358.8309 - val_mse: 358.8309 - val_mae: 11.5247\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 185.1008 - mse: 185.1008 - mae: 7.5559 - val_loss: 326.7231 - val_mse: 326.7231 - val_mae: 10.6688\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.2390 - mse: 229.2390 - mae: 7.4497 - val_loss: 341.9108 - val_mse: 341.9108 - val_mae: 10.9470\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 245.2457 - mse: 245.2457 - mae: 8.6725 - val_loss: 283.0982 - val_mse: 283.0982 - val_mae: 10.1152\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 162.3822 - mse: 162.3822 - mae: 7.8261 - val_loss: 547.4431 - val_mse: 547.4431 - val_mae: 13.2081\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.0514 - mse: 200.0514 - mae: 7.4004 - val_loss: 236.9209 - val_mse: 236.9209 - val_mae: 9.0327\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 171.8016 - mse: 171.8016 - mae: 7.8826 - val_loss: 364.3637 - val_mse: 364.3637 - val_mae: 11.3989\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.3707 - mse: 204.3707 - mae: 8.1064 - val_loss: 837.1791 - val_mse: 837.1791 - val_mae: 16.7094\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 253.9065 - mse: 253.9065 - mae: 8.2234 - val_loss: 240.4958 - val_mse: 240.4958 - val_mae: 8.9148\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.8110 - mse: 147.8110 - mae: 7.5469 - val_loss: 515.2563 - val_mse: 515.2563 - val_mae: 11.4655\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 175.5467 - mse: 175.5467 - mae: 7.6798 - val_loss: 487.1832 - val_mse: 487.1832 - val_mae: 11.3089\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 98.7737 - mse: 98.7737 - mae: 6.2895 - val_loss: 314.7528 - val_mse: 314.7528 - val_mae: 9.3956\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.0626 - mse: 147.0626 - mae: 7.8255 - val_loss: 391.7694 - val_mse: 391.7694 - val_mae: 11.3650\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 239.7914 - mse: 239.7914 - mae: 7.9866 - val_loss: 274.1623 - val_mse: 274.1623 - val_mae: 10.5269\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 191.2776 - mse: 191.2776 - mae: 7.0306 - val_loss: 285.1907 - val_mse: 285.1907 - val_mae: 10.8464\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 142.2858 - mse: 142.2858 - mae: 7.1686 - val_loss: 368.9975 - val_mse: 368.9975 - val_mae: 12.2943\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 197.2431 - mse: 197.2431 - mae: 8.1666 - val_loss: 346.4894 - val_mse: 346.4894 - val_mae: 11.1009\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 130.8754 - mse: 130.8754 - mae: 7.1574 - val_loss: 408.9013 - val_mse: 408.9013 - val_mae: 11.1438\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 170.1700 - mse: 170.1700 - mae: 7.6085 - val_loss: 236.6257 - val_mse: 236.6257 - val_mae: 9.4371\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 273.5984 - mse: 273.5984 - mae: 8.5988 - val_loss: 893.9019 - val_mse: 893.9019 - val_mae: 16.9719\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 269.0013 - mse: 269.0013 - mae: 8.4585 - val_loss: 578.4955 - val_mse: 578.4955 - val_mae: 11.9245\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 238.0184 - mse: 238.0184 - mae: 9.0929 - val_loss: 215.2998 - val_mse: 215.2998 - val_mae: 9.0178\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 219.3012 - mse: 219.3012 - mae: 8.3460 - val_loss: 779.1632 - val_mse: 779.1632 - val_mae: 18.3541\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 301.2825 - mse: 301.2825 - mae: 10.7144 - val_loss: 526.3492 - val_mse: 526.3492 - val_mae: 14.2322\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 198.9940 - mse: 198.9940 - mae: 8.2285 - val_loss: 242.3981 - val_mse: 242.3981 - val_mae: 9.7045\n",
      "Epoch 658/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 213.8306 - mse: 213.8306 - mae: 7.7403 - val_loss: 753.6757 - val_mse: 753.6757 - val_mae: 16.6026\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 173.7160 - mse: 173.7160 - mae: 7.3195 - val_loss: 313.7570 - val_mse: 313.7570 - val_mae: 9.8499\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 143.9051 - mse: 143.9051 - mae: 7.2950 - val_loss: 222.1683 - val_mse: 222.1683 - val_mae: 8.9688\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 167.0052 - mse: 167.0052 - mae: 7.4825 - val_loss: 550.8749 - val_mse: 550.8749 - val_mae: 14.0460\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 103.9346 - mse: 103.9346 - mae: 6.3754 - val_loss: 430.9196 - val_mse: 430.9196 - val_mae: 11.7177\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 136.7714 - mse: 136.7714 - mae: 7.1346 - val_loss: 281.3246 - val_mse: 281.3246 - val_mae: 9.6784\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 112.3261 - mse: 112.3261 - mae: 6.6974 - val_loss: 390.2623 - val_mse: 390.2623 - val_mae: 10.9806\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 158.1592 - mse: 158.1592 - mae: 6.9844 - val_loss: 767.5860 - val_mse: 767.5860 - val_mae: 15.8115\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 186.0110 - mse: 186.0110 - mae: 7.0841 - val_loss: 472.4610 - val_mse: 472.4610 - val_mae: 10.9069\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 101.8044 - mse: 101.8044 - mae: 6.3746 - val_loss: 348.0194 - val_mse: 348.0194 - val_mae: 9.6161\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.9964 - mse: 120.9964 - mae: 6.7528 - val_loss: 454.2368 - val_mse: 454.2368 - val_mae: 11.7336\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 180.6322 - mse: 180.6322 - mae: 8.0888 - val_loss: 366.2806 - val_mse: 366.2806 - val_mae: 11.4721\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 132.0661 - mse: 132.0661 - mae: 6.6591 - val_loss: 534.7198 - val_mse: 534.7198 - val_mae: 13.4265\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 128.2791 - mse: 128.2791 - mae: 6.5021 - val_loss: 344.3014 - val_mse: 344.3014 - val_mae: 10.8625\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 131.0574 - mse: 131.0574 - mae: 7.1389 - val_loss: 302.6142 - val_mse: 302.6142 - val_mae: 10.6051\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 203.9947 - mse: 203.9947 - mae: 7.8780 - val_loss: 426.4385 - val_mse: 426.4385 - val_mae: 12.4839\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 128.8485 - mse: 128.8485 - mae: 7.0941 - val_loss: 306.5195 - val_mse: 306.5195 - val_mae: 10.1996\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 109.1689 - mse: 109.1689 - mae: 6.2067 - val_loss: 363.4710 - val_mse: 363.4710 - val_mae: 10.9693\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.1635 - mse: 204.1635 - mae: 7.4427 - val_loss: 430.3840 - val_mse: 430.3840 - val_mae: 11.7196\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 275.6037 - mse: 275.6037 - mae: 7.8370 - val_loss: 430.4238 - val_mse: 430.4238 - val_mae: 11.1360\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 185.9380 - mse: 185.9380 - mae: 7.4346 - val_loss: 305.9314 - val_mse: 305.9314 - val_mae: 9.8357\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 116.6464 - mse: 116.6464 - mae: 6.5157 - val_loss: 385.1047 - val_mse: 385.1047 - val_mae: 11.2211\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 161.6636 - mse: 161.6636 - mae: 7.2666 - val_loss: 374.2196 - val_mse: 374.2196 - val_mae: 11.6241\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 150.2942 - mse: 150.2942 - mae: 7.2929 - val_loss: 445.7937 - val_mse: 445.7937 - val_mae: 12.5962\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.0111 - mse: 137.0111 - mae: 7.1092 - val_loss: 497.0405 - val_mse: 497.0405 - val_mae: 12.0719\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 150.5549 - mse: 150.5549 - mae: 6.6518 - val_loss: 563.3926 - val_mse: 563.3926 - val_mae: 11.4354\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 203.5183 - mse: 203.5183 - mae: 8.0241 - val_loss: 403.5041 - val_mse: 403.5041 - val_mae: 9.7490\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 161.0317 - mse: 161.0317 - mae: 7.1321 - val_loss: 486.4020 - val_mse: 486.4020 - val_mae: 12.5233\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 153.6334 - mse: 153.6334 - mae: 7.4904 - val_loss: 312.3944 - val_mse: 312.3944 - val_mae: 10.4814\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 144.8732 - mse: 144.8732 - mae: 7.0909 - val_loss: 329.0782 - val_mse: 329.0782 - val_mae: 11.3461\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.8297 - mse: 238.8297 - mae: 8.3390 - val_loss: 680.0925 - val_mse: 680.0925 - val_mae: 15.9577\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 275.9502 - mse: 275.9502 - mae: 8.3650 - val_loss: 421.0731 - val_mse: 421.0731 - val_mae: 10.8010\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 258.3909 - mse: 258.3909 - mae: 9.1505 - val_loss: 299.6090 - val_mse: 299.6090 - val_mae: 9.5778\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 151.1216 - mse: 151.1216 - mae: 7.0270 - val_loss: 820.1398 - val_mse: 820.1398 - val_mae: 17.2142\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 249.1900 - mse: 249.1900 - mae: 8.3032 - val_loss: 236.6589 - val_mse: 236.6589 - val_mae: 9.4589\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 190.8514 - mse: 190.8514 - mae: 7.8025 - val_loss: 220.5011 - val_mse: 220.5011 - val_mae: 9.7256\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 199.0085 - mse: 199.0085 - mae: 7.3393 - val_loss: 338.2955 - val_mse: 338.2955 - val_mae: 12.6216\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 159.7580 - mse: 159.7580 - mae: 7.3507 - val_loss: 377.0650 - val_mse: 377.0650 - val_mae: 12.6156\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 163.7493 - mse: 163.7493 - mae: 7.2124 - val_loss: 375.1021 - val_mse: 375.1021 - val_mae: 11.2043\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 178.9990 - mse: 178.9990 - mae: 8.1113 - val_loss: 197.4343 - val_mse: 197.4343 - val_mae: 8.7161\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 227.5387 - mse: 227.5387 - mae: 8.0256 - val_loss: 588.9362 - val_mse: 588.9362 - val_mae: 15.1596\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.1445 - mse: 237.1445 - mae: 8.9910 - val_loss: 330.6180 - val_mse: 330.6180 - val_mae: 11.4404\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 164.2898 - mse: 164.2898 - mae: 7.1766 - val_loss: 289.3549 - val_mse: 289.3549 - val_mae: 10.2062\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 191.6449 - mse: 191.6449 - mae: 7.8872 - val_loss: 467.8580 - val_mse: 467.8580 - val_mae: 12.5626\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 193.5341 - mse: 193.5341 - mae: 7.5786 - val_loss: 635.1411 - val_mse: 635.1411 - val_mae: 13.9581\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 179.3508 - mse: 179.3508 - mae: 7.2662 - val_loss: 381.4221 - val_mse: 381.4221 - val_mae: 9.8485\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 154.2819 - mse: 154.2819 - mae: 7.3441 - val_loss: 285.2372 - val_mse: 285.2372 - val_mae: 9.2288\n",
      "Epoch 705/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 104.3050 - mse: 104.3050 - mae: 6.3169 - val_loss: 438.1628 - val_mse: 438.1628 - val_mae: 12.8741\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 160.0609 - mse: 160.0609 - mae: 7.7760 - val_loss: 307.8811 - val_mse: 307.8811 - val_mae: 11.4644\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 180.9516 - mse: 180.9516 - mae: 7.3233 - val_loss: 296.5570 - val_mse: 296.5570 - val_mae: 10.9383\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 116.2674 - mse: 116.2674 - mae: 6.6132 - val_loss: 725.0966 - val_mse: 725.0966 - val_mae: 16.4923\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 219.0488 - mse: 219.0488 - mae: 7.5420 - val_loss: 232.3720 - val_mse: 232.3720 - val_mae: 9.7452\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 125.6375 - mse: 125.6375 - mae: 6.9077 - val_loss: 255.0631 - val_mse: 255.0631 - val_mae: 10.1863\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 184.2645 - mse: 184.2645 - mae: 7.3393 - val_loss: 342.1153 - val_mse: 342.1153 - val_mae: 12.3817\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 145.5155 - mse: 145.5155 - mae: 6.8836 - val_loss: 339.5642 - val_mse: 339.5642 - val_mae: 12.0891\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 127.3855 - mse: 127.3855 - mae: 6.8608 - val_loss: 319.7536 - val_mse: 319.7536 - val_mae: 11.1824\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 154.1199 - mse: 154.1199 - mae: 7.0842 - val_loss: 635.9977 - val_mse: 635.9977 - val_mae: 13.9363\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.2712 - mse: 212.2712 - mae: 7.9720 - val_loss: 593.8376 - val_mse: 593.8376 - val_mae: 12.0596\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.4227 - mse: 211.4227 - mae: 8.0090 - val_loss: 281.6441 - val_mse: 281.6441 - val_mae: 9.4674\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 122.1043 - mse: 122.1043 - mae: 6.9978 - val_loss: 718.5833 - val_mse: 718.5833 - val_mae: 15.6527\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 184.6359 - mse: 184.6359 - mae: 7.7486 - val_loss: 312.1853 - val_mse: 312.1853 - val_mae: 10.1980\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 180.5929 - mse: 180.5929 - mae: 8.5346 - val_loss: 327.3284 - val_mse: 327.3284 - val_mae: 10.7540\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 80.6041 - mse: 80.6041 - mae: 6.3191 - val_loss: 769.8085 - val_mse: 769.8085 - val_mae: 16.5189\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 180.4969 - mse: 180.4969 - mae: 7.8674 - val_loss: 373.1588 - val_mse: 373.1588 - val_mae: 10.8527\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 176.9911 - mse: 176.9911 - mae: 7.5785 - val_loss: 447.5500 - val_mse: 447.5500 - val_mae: 11.3336\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 140.4216 - mse: 140.4216 - mae: 7.2319 - val_loss: 676.7809 - val_mse: 676.7809 - val_mae: 13.8077\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 83.4009 - mse: 83.4009 - mae: 6.0079 - val_loss: 313.9730 - val_mse: 313.9730 - val_mae: 9.4143\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9331 - mse: 222.9331 - mae: 8.4351 - val_loss: 524.2827 - val_mse: 524.2827 - val_mae: 12.0138\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.7855 - mse: 269.7855 - mae: 9.2650 - val_loss: 1046.2628 - val_mse: 1046.2628 - val_mae: 17.1810\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 281.2852 - mse: 281.2852 - mae: 8.0754 - val_loss: 370.1496 - val_mse: 370.1496 - val_mae: 10.3338\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 325.0690 - mse: 325.0690 - mae: 10.6251 - val_loss: 478.8925 - val_mse: 478.8925 - val_mae: 12.0795\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 326.8026 - mse: 326.8026 - mae: 9.6910 - val_loss: 766.7218 - val_mse: 766.7218 - val_mae: 16.6508\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 203.9538 - mse: 203.9538 - mae: 7.6356 - val_loss: 248.6428 - val_mse: 248.6428 - val_mae: 9.3120\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 216.0479 - mse: 216.0479 - mae: 9.1034 - val_loss: 331.3274 - val_mse: 331.3274 - val_mae: 11.7490\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165.9289 - mse: 165.9289 - mae: 7.7288 - val_loss: 625.0648 - val_mse: 625.0648 - val_mae: 16.4760\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 194.3337 - mse: 194.3337 - mae: 7.7497 - val_loss: 417.3824 - val_mse: 417.3824 - val_mae: 12.5146\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 148.9158 - mse: 148.9158 - mae: 7.3557 - val_loss: 369.9826 - val_mse: 369.9826 - val_mae: 10.7745\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 153.0406 - mse: 153.0406 - mae: 7.3854 - val_loss: 402.2587 - val_mse: 402.2587 - val_mae: 10.7646\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149.9032 - mse: 149.9032 - mae: 7.3170 - val_loss: 496.0087 - val_mse: 496.0087 - val_mae: 11.6566\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 132.3554 - mse: 132.3554 - mae: 7.2243 - val_loss: 431.1162 - val_mse: 431.1162 - val_mae: 11.0520\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 85.9893 - mse: 85.9893 - mae: 6.0249 - val_loss: 478.5997 - val_mse: 478.5997 - val_mae: 11.5968\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 117.8092 - mse: 117.8092 - mae: 6.2931 - val_loss: 416.5946 - val_mse: 416.5946 - val_mae: 10.7433\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 120.2792 - mse: 120.2792 - mae: 6.9774 - val_loss: 279.7541 - val_mse: 279.7541 - val_mae: 9.5481\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144.7558 - mse: 144.7558 - mae: 6.9372 - val_loss: 383.5141 - val_mse: 383.5141 - val_mae: 12.5711\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 121.6235 - mse: 121.6235 - mae: 6.5464 - val_loss: 386.8831 - val_mse: 386.8831 - val_mae: 12.3258\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 98.1609 - mse: 98.1609 - mae: 6.2778 - val_loss: 278.1368 - val_mse: 278.1368 - val_mae: 10.3308\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 141.7616 - mse: 141.7616 - mae: 6.5952 - val_loss: 504.7489 - val_mse: 504.7489 - val_mae: 13.2566\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 139.4853 - mse: 139.4853 - mae: 6.7065 - val_loss: 278.7055 - val_mse: 278.7055 - val_mae: 9.9259\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 97.6420 - mse: 97.6420 - mae: 6.2227 - val_loss: 276.2903 - val_mse: 276.2903 - val_mae: 9.9727\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 166.6872 - mse: 166.6872 - mae: 7.3547 - val_loss: 435.9686 - val_mse: 435.9686 - val_mae: 13.0752\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 142.5890 - mse: 142.5890 - mae: 7.3099 - val_loss: 499.6111 - val_mse: 499.6111 - val_mae: 13.9839\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 125.6156 - mse: 125.6156 - mae: 6.5442 - val_loss: 278.4521 - val_mse: 278.4521 - val_mae: 10.1064\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 133.8469 - mse: 133.8469 - mae: 7.1542 - val_loss: 328.9289 - val_mse: 328.9289 - val_mae: 11.1714\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 117.9354 - mse: 117.9354 - mae: 6.7884 - val_loss: 517.9427 - val_mse: 517.9427 - val_mae: 13.7537\n",
      "Epoch 752/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 187.2738 - mse: 187.2738 - mae: 7.4230 - val_loss: 323.8862 - val_mse: 323.8862 - val_mae: 10.7024\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 134.6520 - mse: 134.6520 - mae: 6.8610 - val_loss: 390.4915 - val_mse: 390.4915 - val_mae: 11.5810\n",
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 140.0899 - mse: 140.0899 - mae: 7.2205 - val_loss: 310.0239 - val_mse: 310.0239 - val_mae: 11.1285\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 178.7143 - mse: 178.7143 - mae: 7.6879 - val_loss: 599.2997 - val_mse: 599.2997 - val_mae: 14.6090\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 164.9057 - mse: 164.9057 - mae: 7.5341 - val_loss: 382.8805 - val_mse: 382.8805 - val_mae: 10.3913\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 213.1146 - mse: 213.1146 - mae: 8.2933 - val_loss: 288.5584 - val_mse: 288.5584 - val_mae: 9.5380\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 232.7023 - mse: 232.7023 - mae: 8.1840 - val_loss: 547.8785 - val_mse: 547.8785 - val_mae: 14.4415\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 122.0438 - mse: 122.0438 - mae: 6.9136 - val_loss: 608.7818 - val_mse: 608.7818 - val_mae: 15.2039\n",
      "Epoch 760/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 230.6180 - mse: 230.6180 - mae: 7.8049 - val_loss: 311.8430 - val_mse: 311.8430 - val_mae: 10.6651\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 125.1092 - mse: 125.1092 - mae: 6.8436 - val_loss: 548.3857 - val_mse: 548.3857 - val_mae: 12.8819\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 138.4319 - mse: 138.4319 - mae: 6.5671 - val_loss: 360.5435 - val_mse: 360.5435 - val_mae: 11.0359\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149.8773 - mse: 149.8773 - mae: 7.3425 - val_loss: 628.9684 - val_mse: 628.9684 - val_mae: 13.3099\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 125.9679 - mse: 125.9679 - mae: 6.4048 - val_loss: 453.5233 - val_mse: 453.5233 - val_mae: 11.0961\n",
      "Epoch 765/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 120.1457 - mse: 120.1457 - mae: 6.0981 - val_loss: 533.8270 - val_mse: 533.8270 - val_mae: 12.2918\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 127.7972 - mse: 127.7972 - mae: 6.7502 - val_loss: 454.5456 - val_mse: 454.5456 - val_mae: 11.8597\n",
      "Epoch 767/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 87.9831 - mse: 87.9831 - mae: 6.0451 - val_loss: 368.2851 - val_mse: 368.2851 - val_mae: 11.1960\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 81.6274 - mse: 81.6274 - mae: 5.9087 - val_loss: 408.5508 - val_mse: 408.5508 - val_mae: 12.0763\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 164.5201 - mse: 164.5201 - mae: 7.4368 - val_loss: 624.3314 - val_mse: 624.3314 - val_mae: 13.5556\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 141.7146 - mse: 141.7146 - mae: 6.6035 - val_loss: 291.7799 - val_mse: 291.7799 - val_mae: 9.5100\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 167.9524 - mse: 167.9524 - mae: 6.9757 - val_loss: 527.2744 - val_mse: 527.2744 - val_mae: 11.6296\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 115.5118 - mse: 115.5118 - mae: 6.3427 - val_loss: 543.7743 - val_mse: 543.7743 - val_mae: 11.8638\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149.2223 - mse: 149.2223 - mae: 7.1296 - val_loss: 296.0483 - val_mse: 296.0483 - val_mae: 9.3621\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 254.5932 - mse: 254.5932 - mae: 7.7545 - val_loss: 1039.6884 - val_mse: 1039.6884 - val_mae: 18.3947\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 204.1464 - mse: 204.1464 - mae: 8.3038 - val_loss: 508.8167 - val_mse: 508.8167 - val_mae: 10.3226\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 194.9467 - mse: 194.9468 - mae: 8.3711 - val_loss: 455.9567 - val_mse: 455.9567 - val_mae: 9.9856\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 124.9306 - mse: 124.9306 - mae: 6.9189 - val_loss: 790.5660 - val_mse: 790.5660 - val_mae: 15.4307\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 239.6317 - mse: 239.6317 - mae: 7.8371 - val_loss: 526.2635 - val_mse: 526.2635 - val_mae: 11.5041\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 125.7597 - mse: 125.7597 - mae: 7.1124 - val_loss: 566.2743 - val_mse: 566.2743 - val_mae: 11.4350\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 169.9626 - mse: 169.9626 - mae: 7.6467 - val_loss: 382.7904 - val_mse: 382.7904 - val_mae: 10.0058\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 157.5066 - mse: 157.5066 - mae: 7.9413 - val_loss: 316.0783 - val_mse: 316.0783 - val_mae: 10.2939\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165.7792 - mse: 165.7792 - mae: 7.5394 - val_loss: 605.1596 - val_mse: 605.1596 - val_mae: 15.1382\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9835 - mse: 221.9835 - mae: 8.8733 - val_loss: 763.6894 - val_mse: 763.6894 - val_mae: 15.1593\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 160.1889 - mse: 160.1889 - mae: 7.7547 - val_loss: 431.4244 - val_mse: 431.4244 - val_mae: 10.1883\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 159.4981 - mse: 159.4981 - mae: 7.4643 - val_loss: 538.2094 - val_mse: 538.2094 - val_mae: 10.7905\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 90.1550 - mse: 90.1550 - mae: 6.3096 - val_loss: 487.7980 - val_mse: 487.7980 - val_mae: 11.8092\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 198.2144 - mse: 198.2144 - mae: 7.6806 - val_loss: 370.7140 - val_mse: 370.7140 - val_mae: 10.7367\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 91.6354 - mse: 91.6354 - mae: 6.3782 - val_loss: 294.8379 - val_mse: 294.8379 - val_mae: 10.2978\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 109.6824 - mse: 109.6824 - mae: 6.4279 - val_loss: 299.4350 - val_mse: 299.4350 - val_mae: 10.8122\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 175.8615 - mse: 175.8615 - mae: 7.2913 - val_loss: 520.6486 - val_mse: 520.6486 - val_mae: 13.4519\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 203.3266 - mse: 203.3266 - mae: 7.2419 - val_loss: 261.6703 - val_mse: 261.6703 - val_mae: 9.2934\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 110.5700 - mse: 110.5700 - mae: 6.6208 - val_loss: 543.8171 - val_mse: 543.8171 - val_mae: 12.3110\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6706 - mse: 222.6706 - mae: 7.9590 - val_loss: 350.5319 - val_mse: 350.5319 - val_mae: 10.5815\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 190.6965 - mse: 190.6965 - mae: 7.2609 - val_loss: 387.1497 - val_mse: 387.1497 - val_mae: 11.4124\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 135.7648 - mse: 135.7648 - mae: 6.7746 - val_loss: 464.7624 - val_mse: 464.7624 - val_mae: 11.5933\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 192.2309 - mse: 192.2309 - mae: 7.3631 - val_loss: 351.1018 - val_mse: 351.1018 - val_mae: 10.0943\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 203.3964 - mse: 203.3964 - mae: 8.2558 - val_loss: 458.3028 - val_mse: 458.3028 - val_mae: 12.0267\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 167.2971 - mse: 167.2971 - mae: 7.3200 - val_loss: 556.6729 - val_mse: 556.6729 - val_mae: 13.4299\n",
      "Epoch 799/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 118.7651 - mse: 118.7651 - mae: 6.0759 - val_loss: 297.5006 - val_mse: 297.5006 - val_mae: 9.9198\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 106.8016 - mse: 106.8016 - mae: 6.1726 - val_loss: 585.9984 - val_mse: 585.9984 - val_mae: 14.1362\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 183.3355 - mse: 183.3355 - mae: 7.2534 - val_loss: 378.0685 - val_mse: 378.0685 - val_mae: 10.8301\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 185.2654 - mse: 185.2654 - mae: 7.1924 - val_loss: 289.3827 - val_mse: 289.3827 - val_mae: 10.5037\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 206.0409 - mse: 206.0409 - mae: 7.5436 - val_loss: 410.3156 - val_mse: 410.3156 - val_mae: 13.1650\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 108.5065 - mse: 108.5065 - mae: 6.8024 - val_loss: 531.0182 - val_mse: 531.0182 - val_mae: 14.1874\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 102.0593 - mse: 102.0593 - mae: 5.6626 - val_loss: 317.6537 - val_mse: 317.6537 - val_mae: 10.1784\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 194.6328 - mse: 194.6328 - mae: 7.8827 - val_loss: 520.3688 - val_mse: 520.3688 - val_mae: 12.3042\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 126.1226 - mse: 126.1226 - mae: 7.0940 - val_loss: 944.0361 - val_mse: 944.0361 - val_mae: 16.5015\n",
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 215.8727 - mse: 215.8727 - mae: 7.6298 - val_loss: 357.5692 - val_mse: 357.5692 - val_mae: 9.9507\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.7828 - mse: 209.7828 - mae: 8.8952 - val_loss: 378.1385 - val_mse: 378.1385 - val_mae: 10.7609\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.0509 - mse: 238.0509 - mae: 9.5597 - val_loss: 897.5874 - val_mse: 897.5873 - val_mae: 19.1778\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 149.2300 - mse: 149.2300 - mae: 7.8420 - val_loss: 458.7513 - val_mse: 458.7513 - val_mae: 11.7335\n",
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.3445 - mse: 233.3445 - mae: 7.8251 - val_loss: 296.4839 - val_mse: 296.4839 - val_mae: 9.7801\n",
      "Epoch 813/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 124.2751 - mse: 124.2751 - mae: 6.9470 - val_loss: 639.3765 - val_mse: 639.3765 - val_mae: 14.9342\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 222.6505 - mse: 222.6505 - mae: 7.9877 - val_loss: 283.8287 - val_mse: 283.8287 - val_mae: 10.1726\n",
      "Epoch 815/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 201.4021 - mse: 201.4021 - mae: 7.7831 - val_loss: 309.0888 - val_mse: 309.0888 - val_mae: 10.5042\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 199.6964 - mse: 199.6964 - mae: 8.0399 - val_loss: 644.4401 - val_mse: 644.4401 - val_mae: 14.1763\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 105.2108 - mse: 105.2108 - mae: 5.9057 - val_loss: 462.1609 - val_mse: 462.1609 - val_mae: 11.0169\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 116.9614 - mse: 116.9614 - mae: 6.6857 - val_loss: 502.0384 - val_mse: 502.0384 - val_mae: 11.2362\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 279.5926 - mse: 279.5926 - mae: 8.1151 - val_loss: 278.8335 - val_mse: 278.8335 - val_mae: 9.5601\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165.2078 - mse: 165.2078 - mae: 7.7034 - val_loss: 926.0714 - val_mse: 926.0714 - val_mae: 18.7408\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 156.7064 - mse: 156.7064 - mae: 7.3734 - val_loss: 329.0140 - val_mse: 329.0140 - val_mae: 9.9118\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 156.2601 - mse: 156.2601 - mae: 7.6199 - val_loss: 283.9109 - val_mse: 283.9109 - val_mae: 9.2745\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 130.1993 - mse: 130.1993 - mae: 6.8339 - val_loss: 607.9388 - val_mse: 607.9388 - val_mae: 14.5516\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.7560 - mse: 209.7560 - mae: 7.2966 - val_loss: 423.4152 - val_mse: 423.4152 - val_mae: 11.2705\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 71.6325 - mse: 71.6325 - mae: 5.7745 - val_loss: 416.3336 - val_mse: 416.3336 - val_mae: 10.8607\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 158.5035 - mse: 158.5035 - mae: 7.3889 - val_loss: 373.5641 - val_mse: 373.5641 - val_mae: 10.0883\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 111.7562 - mse: 111.7562 - mae: 6.3932 - val_loss: 418.2421 - val_mse: 418.2421 - val_mae: 11.0567\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 115.2576 - mse: 115.2576 - mae: 6.0931 - val_loss: 261.7324 - val_mse: 261.7324 - val_mae: 9.2429\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 111.1679 - mse: 111.1679 - mae: 6.0584 - val_loss: 344.4180 - val_mse: 344.4180 - val_mae: 10.8912\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 157.0978 - mse: 157.0978 - mae: 6.8582 - val_loss: 334.1169 - val_mse: 334.1169 - val_mae: 10.8323\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 101.5718 - mse: 101.5718 - mae: 5.8001 - val_loss: 401.8109 - val_mse: 401.8109 - val_mae: 11.5753\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 172.4305 - mse: 172.4305 - mae: 7.1055 - val_loss: 300.4840 - val_mse: 300.4840 - val_mae: 9.8791\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 102.8023 - mse: 102.8023 - mae: 6.2965 - val_loss: 311.7435 - val_mse: 311.7435 - val_mae: 10.2974\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 139.1568 - mse: 139.1568 - mae: 6.9007 - val_loss: 259.0155 - val_mse: 259.0155 - val_mae: 9.9640\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 135.5240 - mse: 135.5240 - mae: 6.6261 - val_loss: 296.9558 - val_mse: 296.9558 - val_mae: 11.0121\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 115.1983 - mse: 115.1983 - mae: 6.9000 - val_loss: 316.0921 - val_mse: 316.0921 - val_mae: 11.3969\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 195.8377 - mse: 195.8377 - mae: 7.7900 - val_loss: 316.3716 - val_mse: 316.3716 - val_mae: 11.6262\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 155.8769 - mse: 155.8769 - mae: 7.4246 - val_loss: 655.2247 - val_mse: 655.2247 - val_mae: 14.9561\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 206.3097 - mse: 206.3097 - mae: 7.4474 - val_loss: 244.2622 - val_mse: 244.2622 - val_mae: 9.3916\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 159.3304 - mse: 159.3304 - mae: 7.4902 - val_loss: 406.8144 - val_mse: 406.8144 - val_mae: 12.7974\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 140.4869 - mse: 140.4869 - mae: 7.1746 - val_loss: 538.0507 - val_mse: 538.0507 - val_mae: 14.6693\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 174.8005 - mse: 174.8005 - mae: 8.3208 - val_loss: 438.1440 - val_mse: 438.1440 - val_mae: 11.5761\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 165.9300 - mse: 165.9300 - mae: 7.2319 - val_loss: 372.1430 - val_mse: 372.1430 - val_mae: 10.3518\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 145.8508 - mse: 145.8508 - mae: 5.9202 - val_loss: 610.2089 - val_mse: 610.2089 - val_mae: 12.5818\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 254.6125 - mse: 254.6125 - mae: 8.0499 - val_loss: 413.3376 - val_mse: 413.3376 - val_mae: 9.9584\n",
      "Epoch 846/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 8ms/step - loss: 316.3690 - mse: 316.3690 - mae: 8.9550 - val_loss: 415.7433 - val_mse: 415.7433 - val_mae: 11.0890\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 270.3705 - mse: 270.3705 - mae: 8.3139 - val_loss: 592.5405 - val_mse: 592.5405 - val_mae: 14.2426\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 169.6057 - mse: 169.6057 - mae: 7.5034 - val_loss: 318.8577 - val_mse: 318.8577 - val_mae: 10.5606\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 104.6966 - mse: 104.6966 - mae: 6.0724 - val_loss: 275.9025 - val_mse: 275.9025 - val_mae: 9.8959\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.7737 - mse: 120.7737 - mae: 6.3862 - val_loss: 378.2871 - val_mse: 378.2872 - val_mae: 11.3559\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.8659 - mse: 233.8659 - mae: 7.4234 - val_loss: 299.9637 - val_mse: 299.9637 - val_mae: 10.5085\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 196.8084 - mse: 196.8084 - mae: 6.5681 - val_loss: 330.0208 - val_mse: 330.0208 - val_mae: 11.4570\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 81.2189 - mse: 81.2189 - mae: 5.8903 - val_loss: 354.8840 - val_mse: 354.8840 - val_mae: 11.8120\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 117.5213 - mse: 117.5213 - mae: 6.4554 - val_loss: 415.0672 - val_mse: 415.0672 - val_mae: 11.8282\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 83.9038 - mse: 83.9038 - mae: 5.6214 - val_loss: 291.0935 - val_mse: 291.0935 - val_mae: 10.0697\n",
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 146.4275 - mse: 146.4275 - mae: 6.6967 - val_loss: 293.2752 - val_mse: 293.2752 - val_mae: 10.4966\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 100.8335 - mse: 100.8335 - mae: 6.5301 - val_loss: 568.6033 - val_mse: 568.6033 - val_mae: 14.4618\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 131.3000 - mse: 131.3000 - mae: 6.8494 - val_loss: 460.2742 - val_mse: 460.2742 - val_mae: 11.6119\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 132.3487 - mse: 132.3487 - mae: 7.0210 - val_loss: 240.4311 - val_mse: 240.4311 - val_mae: 9.2132\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 188.2911 - mse: 188.2911 - mae: 7.3294 - val_loss: 383.9230 - val_mse: 383.9230 - val_mae: 11.5855\n",
      "Epoch 861/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 206.4300 - mse: 206.4300 - mae: 7.9706 - val_loss: 352.8984 - val_mse: 352.8984 - val_mae: 12.0949\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 169.4345 - mse: 169.4345 - mae: 7.0933 - val_loss: 292.4411 - val_mse: 292.4411 - val_mae: 11.1691\n",
      "Epoch 863/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 181.4048 - mse: 181.4048 - mae: 7.1746 - val_loss: 495.1373 - val_mse: 495.1373 - val_mae: 14.0808\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 123.4325 - mse: 123.4325 - mae: 6.7639 - val_loss: 643.4118 - val_mse: 643.4118 - val_mae: 14.4130\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 124.6026 - mse: 124.6026 - mae: 6.4107 - val_loss: 258.4303 - val_mse: 258.4303 - val_mae: 9.5056\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117.4457 - mse: 117.4457 - mae: 6.6278 - val_loss: 347.9635 - val_mse: 347.9635 - val_mae: 10.5701\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 126.1930 - mse: 126.1930 - mae: 6.7269 - val_loss: 404.6241 - val_mse: 404.6241 - val_mae: 12.0996\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 131.8208 - mse: 131.8208 - mae: 6.3164 - val_loss: 271.1055 - val_mse: 271.1055 - val_mae: 10.3697\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 123.2124 - mse: 123.2124 - mae: 6.1573 - val_loss: 281.6388 - val_mse: 281.6388 - val_mae: 10.7353\n",
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 120.9759 - mse: 120.9759 - mae: 6.4277 - val_loss: 375.8491 - val_mse: 375.8491 - val_mae: 12.5470\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 134.0874 - mse: 134.0874 - mae: 7.0713 - val_loss: 364.7504 - val_mse: 364.7504 - val_mae: 11.6363\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 162.3672 - mse: 162.3672 - mae: 6.8989 - val_loss: 779.6367 - val_mse: 779.6367 - val_mae: 15.6609\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 178.6921 - mse: 178.6921 - mae: 7.0888 - val_loss: 480.5042 - val_mse: 480.5042 - val_mae: 10.8303\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 135.6664 - mse: 135.6664 - mae: 7.2096 - val_loss: 277.2881 - val_mse: 277.2881 - val_mae: 9.2046\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 167.5593 - mse: 167.5593 - mae: 7.3050 - val_loss: 481.8716 - val_mse: 481.8716 - val_mae: 12.0789\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 153.1573 - mse: 153.1573 - mae: 6.8597 - val_loss: 424.1016 - val_mse: 424.1016 - val_mae: 12.9011\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 101.6007 - mse: 101.6007 - mae: 6.7971 - val_loss: 391.2260 - val_mse: 391.2260 - val_mae: 12.3157\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 199.3241 - mse: 199.3241 - mae: 7.8543 - val_loss: 537.7311 - val_mse: 537.7311 - val_mae: 12.7582\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 132.7197 - mse: 132.7197 - mae: 6.7815 - val_loss: 414.5079 - val_mse: 414.5079 - val_mae: 10.5309\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 186.9650 - mse: 186.9650 - mae: 7.1750 - val_loss: 491.3402 - val_mse: 491.3402 - val_mae: 10.8757\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 125.6993 - mse: 125.6993 - mae: 6.5031 - val_loss: 374.9881 - val_mse: 374.9881 - val_mae: 10.0024\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 153.6207 - mse: 153.6207 - mae: 7.6818 - val_loss: 501.6450 - val_mse: 501.6450 - val_mae: 12.8878\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 121.7736 - mse: 121.7736 - mae: 6.6245 - val_loss: 517.9269 - val_mse: 517.9269 - val_mae: 13.1128\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 115.4223 - mse: 115.4223 - mae: 6.9389 - val_loss: 311.3986 - val_mse: 311.3986 - val_mae: 10.1687\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 174.4765 - mse: 174.4765 - mae: 7.7353 - val_loss: 338.5836 - val_mse: 338.5836 - val_mae: 10.5523\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 64.2626 - mse: 64.2626 - mae: 5.3350 - val_loss: 433.7973 - val_mse: 433.7973 - val_mae: 12.1375\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114.5255 - mse: 114.5255 - mae: 6.6525 - val_loss: 419.1956 - val_mse: 419.1956 - val_mae: 11.4123\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 94.9718 - mse: 94.9718 - mae: 6.3255 - val_loss: 327.9724 - val_mse: 327.9724 - val_mae: 10.3094\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 78.0973 - mse: 78.0973 - mae: 5.8746 - val_loss: 349.1712 - val_mse: 349.1712 - val_mae: 10.8727\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 126.9908 - mse: 126.9908 - mae: 6.7005 - val_loss: 517.4390 - val_mse: 517.4390 - val_mae: 13.1148\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 85.7248 - mse: 85.7248 - mae: 5.5839 - val_loss: 325.8521 - val_mse: 325.8521 - val_mae: 10.0218\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 185.4619 - mse: 185.4619 - mae: 7.3785 - val_loss: 329.2693 - val_mse: 329.2693 - val_mae: 10.1952\n",
      "Epoch 893/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 167.9051 - mse: 167.9051 - mae: 7.3583 - val_loss: 245.2227 - val_mse: 245.2227 - val_mae: 9.9066\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 70.9811 - mse: 70.9811 - mae: 5.7949 - val_loss: 469.9829 - val_mse: 469.9829 - val_mae: 12.9192\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 163.0156 - mse: 163.0156 - mae: 6.5038 - val_loss: 249.5044 - val_mse: 249.5044 - val_mae: 9.8766\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 139.2742 - mse: 139.2742 - mae: 6.7595 - val_loss: 333.2796 - val_mse: 333.2796 - val_mae: 11.4795\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 97.0137 - mse: 97.0137 - mae: 6.1849 - val_loss: 410.1002 - val_mse: 410.1002 - val_mae: 12.4529\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 154.2289 - mse: 154.2289 - mae: 6.7841 - val_loss: 235.2646 - val_mse: 235.2646 - val_mae: 10.0181\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 233.6207 - mse: 233.6207 - mae: 7.3008 - val_loss: 357.4305 - val_mse: 357.4305 - val_mae: 11.1968\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.8722 - mse: 147.8722 - mae: 6.5441 - val_loss: 343.0282 - val_mse: 343.0282 - val_mae: 11.3890\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 153.0585 - mse: 153.0585 - mae: 6.5248 - val_loss: 500.4201 - val_mse: 500.4201 - val_mae: 12.8697\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 201.2191 - mse: 201.2191 - mae: 7.2816 - val_loss: 237.5413 - val_mse: 237.5413 - val_mae: 9.7505\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114.7964 - mse: 114.7964 - mae: 6.2797 - val_loss: 278.0831 - val_mse: 278.0831 - val_mae: 11.0566\n",
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 117.9839 - mse: 117.9839 - mae: 6.4165 - val_loss: 313.6577 - val_mse: 313.6577 - val_mae: 11.7931\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 173.7128 - mse: 173.7128 - mae: 6.5377 - val_loss: 548.3351 - val_mse: 548.3351 - val_mae: 13.4272\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 150.7905 - mse: 150.7905 - mae: 7.0825 - val_loss: 367.8330 - val_mse: 367.8330 - val_mae: 10.3768\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 195.5579 - mse: 195.5579 - mae: 8.2895 - val_loss: 299.6794 - val_mse: 299.6794 - val_mae: 10.2188\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 175.7489 - mse: 175.7489 - mae: 7.5436 - val_loss: 492.2080 - val_mse: 492.2080 - val_mae: 13.9555\n",
      "Epoch 909/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 203.2967 - mse: 203.2967 - mae: 7.1738 - val_loss: 719.9639 - val_mse: 719.9639 - val_mae: 15.6478\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 109.7820 - mse: 109.7820 - mae: 6.3651 - val_loss: 474.7134 - val_mse: 474.7134 - val_mae: 11.6047\n",
      "Epoch 911/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 182.5589 - mse: 182.5589 - mae: 7.4537 - val_loss: 518.2623 - val_mse: 518.2623 - val_mae: 11.9642\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 113.6432 - mse: 113.6432 - mae: 6.7537 - val_loss: 723.2987 - val_mse: 723.2987 - val_mae: 14.2442\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 119.4721 - mse: 119.4721 - mae: 6.8110 - val_loss: 440.4816 - val_mse: 440.4816 - val_mae: 10.9175\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 144.6010 - mse: 144.6010 - mae: 6.8553 - val_loss: 532.1766 - val_mse: 532.1766 - val_mae: 12.5705\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 104.9443 - mse: 104.9443 - mae: 5.9478 - val_loss: 548.8090 - val_mse: 548.8090 - val_mae: 13.4135\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 113.5204 - mse: 113.5204 - mae: 6.2339 - val_loss: 453.5589 - val_mse: 453.5589 - val_mae: 12.2689\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 132.9010 - mse: 132.9010 - mae: 6.8793 - val_loss: 414.9093 - val_mse: 414.9093 - val_mae: 11.8056\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 107.7120 - mse: 107.7120 - mae: 6.3418 - val_loss: 456.3591 - val_mse: 456.3591 - val_mae: 12.4353\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 227.7478 - mse: 227.7478 - mae: 7.1207 - val_loss: 313.7950 - val_mse: 313.7950 - val_mae: 10.4708\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 59.0145 - mse: 59.0145 - mae: 5.5004 - val_loss: 520.2514 - val_mse: 520.2514 - val_mae: 12.4321\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.8177 - mse: 98.8177 - mae: 6.2850 - val_loss: 513.5773 - val_mse: 513.5773 - val_mae: 12.3230\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 118.9685 - mse: 118.9685 - mae: 5.8495 - val_loss: 422.3656 - val_mse: 422.3656 - val_mae: 11.5703\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 104.8439 - mse: 104.8439 - mae: 6.1280 - val_loss: 238.7902 - val_mse: 238.7902 - val_mae: 10.2566\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 114.9043 - mse: 114.9043 - mae: 6.2933 - val_loss: 621.7759 - val_mse: 621.7759 - val_mae: 15.8548\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 164.3350 - mse: 164.3350 - mae: 7.6328 - val_loss: 306.4319 - val_mse: 306.4319 - val_mae: 10.4998\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 106.7502 - mse: 106.7502 - mae: 5.9489 - val_loss: 281.5212 - val_mse: 281.5212 - val_mae: 9.5836\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 121.0721 - mse: 121.0721 - mae: 7.0100 - val_loss: 547.4078 - val_mse: 547.4078 - val_mae: 12.7815\n",
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 158.8398 - mse: 158.8398 - mae: 6.7793 - val_loss: 254.1497 - val_mse: 254.1497 - val_mae: 9.6910\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 161.9868 - mse: 161.9868 - mae: 7.0810 - val_loss: 459.7687 - val_mse: 459.7687 - val_mae: 13.2615\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 273.2397 - mse: 273.2397 - mae: 8.0078 - val_loss: 401.5733 - val_mse: 401.5733 - val_mae: 12.4809\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 107.0751 - mse: 107.0751 - mae: 6.0853 - val_loss: 339.1265 - val_mse: 339.1265 - val_mae: 11.1311\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 138.9835 - mse: 138.9835 - mae: 6.2258 - val_loss: 334.6807 - val_mse: 334.6807 - val_mae: 11.0949\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 169.1705 - mse: 169.1705 - mae: 7.3940 - val_loss: 692.1099 - val_mse: 692.1099 - val_mae: 14.7916\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 220.2548 - mse: 220.2548 - mae: 7.8473 - val_loss: 282.2349 - val_mse: 282.2349 - val_mae: 9.5902\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 170.4880 - mse: 170.4880 - mae: 6.8595 - val_loss: 491.9246 - val_mse: 491.9246 - val_mae: 13.1188\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 137.6131 - mse: 137.6131 - mae: 6.5948 - val_loss: 264.5281 - val_mse: 264.5281 - val_mae: 10.7155\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 109.8371 - mse: 109.8371 - mae: 6.2927 - val_loss: 392.6164 - val_mse: 392.6164 - val_mae: 13.0107\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 94.8035 - mse: 94.8035 - mae: 5.8061 - val_loss: 292.7986 - val_mse: 292.7986 - val_mae: 10.9805\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 128.2794 - mse: 128.2794 - mae: 6.9836 - val_loss: 230.8278 - val_mse: 230.8278 - val_mae: 10.1338\n",
      "Epoch 940/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 167.3376 - mse: 167.3376 - mae: 7.0951 - val_loss: 680.2584 - val_mse: 680.2584 - val_mae: 16.1370\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 174.8411 - mse: 174.8411 - mae: 7.3901 - val_loss: 285.0394 - val_mse: 285.0394 - val_mae: 9.8852\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 208.5712 - mse: 208.5712 - mae: 8.0164 - val_loss: 337.3907 - val_mse: 337.3907 - val_mae: 10.6258\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 175.7790 - mse: 175.7790 - mae: 7.5267 - val_loss: 808.8049 - val_mse: 808.8049 - val_mae: 16.3752\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 141.8860 - mse: 141.8860 - mae: 6.8371 - val_loss: 218.8093 - val_mse: 218.8093 - val_mae: 8.6643\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 223.7639 - mse: 223.7639 - mae: 8.5390 - val_loss: 414.0069 - val_mse: 414.0069 - val_mae: 12.7139\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 202.1484 - mse: 202.1484 - mae: 8.2460 - val_loss: 624.3382 - val_mse: 624.3382 - val_mae: 15.8461\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 119.1067 - mse: 119.1067 - mae: 6.1163 - val_loss: 229.3401 - val_mse: 229.3401 - val_mae: 9.1905\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 170.8010 - mse: 170.8010 - mae: 6.9662 - val_loss: 402.1488 - val_mse: 402.1488 - val_mae: 11.7003\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 182.4581 - mse: 182.4581 - mae: 7.1940 - val_loss: 459.4440 - val_mse: 459.4440 - val_mae: 12.8631\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 148.1718 - mse: 148.1718 - mae: 6.3444 - val_loss: 209.8632 - val_mse: 209.8632 - val_mae: 9.2959\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 167.0093 - mse: 167.0093 - mae: 6.3294 - val_loss: 435.0378 - val_mse: 435.0378 - val_mae: 12.7834\n",
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 158.8690 - mse: 158.8690 - mae: 7.1060 - val_loss: 639.3659 - val_mse: 639.3659 - val_mae: 14.1994\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 119.9268 - mse: 119.9268 - mae: 6.5318 - val_loss: 400.1025 - val_mse: 400.1025 - val_mae: 10.7048\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 213.9968 - mse: 213.9968 - mae: 7.6226 - val_loss: 367.4995 - val_mse: 367.4995 - val_mae: 10.4244\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 215.5840 - mse: 215.5840 - mae: 7.0588 - val_loss: 1208.4426 - val_mse: 1208.4426 - val_mae: 19.3978\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 289.3700 - mse: 289.3700 - mae: 9.0914 - val_loss: 423.2878 - val_mse: 423.2878 - val_mae: 9.6528\n",
      "Epoch 957/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 139.5379 - mse: 139.5379 - mae: 7.2282 - val_loss: 338.2105 - val_mse: 338.2105 - val_mae: 9.6887\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 152.5718 - mse: 152.5718 - mae: 7.3255 - val_loss: 751.5506 - val_mse: 751.5506 - val_mae: 15.9828\n",
      "Epoch 959/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 110.8591 - mse: 110.8591 - mae: 6.2998 - val_loss: 504.4449 - val_mse: 504.4449 - val_mae: 12.1680\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 144.4856 - mse: 144.4856 - mae: 6.4145 - val_loss: 470.4593 - val_mse: 470.4593 - val_mae: 11.3104\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 110.5003 - mse: 110.5003 - mae: 6.2636 - val_loss: 476.6786 - val_mse: 476.6786 - val_mae: 11.3402\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85.3560 - mse: 85.3560 - mae: 5.9241 - val_loss: 295.4879 - val_mse: 295.4879 - val_mae: 9.6979\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 189.4562 - mse: 189.4562 - mae: 8.1088 - val_loss: 340.1110 - val_mse: 340.1110 - val_mae: 11.1377\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 119.7701 - mse: 119.7701 - mae: 7.0028 - val_loss: 900.5354 - val_mse: 900.5354 - val_mae: 18.1836\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 171.4151 - mse: 171.4151 - mae: 7.2291 - val_loss: 249.6766 - val_mse: 249.6766 - val_mae: 9.3488\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 275.9066 - mse: 275.9066 - mae: 8.3290 - val_loss: 491.6813 - val_mse: 491.6813 - val_mae: 12.3617\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 196.3393 - mse: 196.3393 - mae: 8.0690 - val_loss: 786.2825 - val_mse: 786.2825 - val_mae: 14.7036\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.5460 - mse: 223.5460 - mae: 8.2305 - val_loss: 268.4848 - val_mse: 268.4848 - val_mae: 9.6216\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 175.5827 - mse: 175.5827 - mae: 7.8268 - val_loss: 842.5325 - val_mse: 842.5325 - val_mae: 16.6903\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 237.6937 - mse: 237.6937 - mae: 8.8158 - val_loss: 529.0138 - val_mse: 529.0138 - val_mae: 11.8402\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 143.6544 - mse: 143.6544 - mae: 7.4026 - val_loss: 265.4007 - val_mse: 265.4007 - val_mae: 9.2702\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 124.0293 - mse: 124.0293 - mae: 6.3026 - val_loss: 837.1234 - val_mse: 837.1234 - val_mae: 16.4085\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 142.5654 - mse: 142.5654 - mae: 6.6677 - val_loss: 595.7328 - val_mse: 595.7328 - val_mae: 12.8871\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 103.0690 - mse: 103.0690 - mae: 6.5058 - val_loss: 336.4081 - val_mse: 336.4081 - val_mae: 10.0336\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 147.9579 - mse: 147.9579 - mae: 6.9429 - val_loss: 676.5876 - val_mse: 676.5876 - val_mae: 14.1578\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 193.0330 - mse: 193.0330 - mae: 6.6290 - val_loss: 349.0401 - val_mse: 349.0401 - val_mae: 10.1984\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 144.1308 - mse: 144.1308 - mae: 6.9888 - val_loss: 543.7217 - val_mse: 543.7217 - val_mae: 12.3351\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 115.6329 - mse: 115.6329 - mae: 5.6311 - val_loss: 464.7815 - val_mse: 464.7815 - val_mae: 12.0464\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74.2980 - mse: 74.2980 - mae: 5.7444 - val_loss: 507.9977 - val_mse: 507.9977 - val_mae: 12.4306\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 148.3553 - mse: 148.3553 - mae: 6.5243 - val_loss: 404.1359 - val_mse: 404.1359 - val_mae: 11.0444\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 183.1201 - mse: 183.1201 - mae: 6.4427 - val_loss: 483.8789 - val_mse: 483.8789 - val_mae: 12.1370\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.9223 - mse: 241.9223 - mae: 8.0597 - val_loss: 315.6863 - val_mse: 315.6863 - val_mae: 10.2566\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 145.6829 - mse: 145.6829 - mae: 6.9728 - val_loss: 286.3328 - val_mse: 286.3328 - val_mae: 10.0721\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 142.1577 - mse: 142.1577 - mae: 6.4416 - val_loss: 389.1691 - val_mse: 389.1691 - val_mae: 12.3275\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 86.4379 - mse: 86.4379 - mae: 6.0497 - val_loss: 435.9894 - val_mse: 435.9894 - val_mae: 13.2279\n",
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 94.9160 - mse: 94.9160 - mae: 6.0443 - val_loss: 412.3100 - val_mse: 412.3100 - val_mae: 11.9220\n",
      "Epoch 987/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 72.1640 - mse: 72.1640 - mae: 5.2063 - val_loss: 330.3612 - val_mse: 330.3612 - val_mae: 10.2983\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 162.5773 - mse: 162.5773 - mae: 7.1398 - val_loss: 392.2987 - val_mse: 392.2987 - val_mae: 11.0773\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 95.8082 - mse: 95.8082 - mae: 5.9008 - val_loss: 324.6844 - val_mse: 324.6844 - val_mae: 10.9411\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 98.8608 - mse: 98.8608 - mae: 6.1583 - val_loss: 296.1925 - val_mse: 296.1925 - val_mae: 10.5495\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 152.2805 - mse: 152.2805 - mae: 6.5699 - val_loss: 374.1718 - val_mse: 374.1718 - val_mae: 11.1204\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 198.1864 - mse: 198.1864 - mae: 6.6851 - val_loss: 238.7795 - val_mse: 238.7795 - val_mae: 9.6009\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 170.3804 - mse: 170.3804 - mae: 6.5016 - val_loss: 480.7720 - val_mse: 480.7721 - val_mae: 13.7517\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 155.6389 - mse: 155.6389 - mae: 6.9253 - val_loss: 418.3367 - val_mse: 418.3367 - val_mae: 12.9534\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 228.2651 - mse: 228.2651 - mae: 7.1913 - val_loss: 346.7012 - val_mse: 346.7012 - val_mae: 11.0242\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 139.4110 - mse: 139.4110 - mae: 6.9738 - val_loss: 735.5615 - val_mse: 735.5615 - val_mae: 14.6614\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 200.1902 - mse: 200.1902 - mae: 7.5830 - val_loss: 263.7173 - val_mse: 263.7173 - val_mae: 8.9969\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 147.8636 - mse: 147.8636 - mae: 7.1653 - val_loss: 500.0623 - val_mse: 500.0623 - val_mae: 13.2812\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 92.7136 - mse: 92.7136 - mae: 6.3123 - val_loss: 599.5312 - val_mse: 599.5312 - val_mae: 14.0167\n",
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 92.7413 - mse: 92.7413 - mae: 5.9815 - val_loss: 296.3257 - val_mse: 296.3257 - val_mae: 9.8325\n"
     ]
    }
   ],
   "source": [
    "hist4 = model4.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpwjRXwNdUrv",
    "outputId": "dbac2177-0c1d-4601-82bd-ec5ae8be83a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n",
      "예측값 :  [165.25957]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "preds = model4.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9835012996166043\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QtPARErBQW4"
   },
   "source": [
    "### 1.5 Weight constraints 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5wcrjZpfmuY"
   },
   "source": [
    "Layer weight constraints 특정 파라미터가 지나치게 영향력이 커지는 것을 방지하여 오버피팅을 완화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "oWWi28bNftM-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xz91__Tlf09o"
   },
   "outputs": [],
   "source": [
    "model5 = tf.keras.Sequential()\n",
    "\n",
    "model5.add(tf.keras.Input(shape = 3))\n",
    "model5.add(Dense(100, kernel_constraint=max_norm(2.))) # 모델의 weight는 2.0 초과로 커질 수 없습니다.\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(200, kernel_constraint=max_norm(2.)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(300, kernel_constraint=max_norm(2.)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(200, kernel_constraint=max_norm(2.)))\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dense(100, kernel_constraint=max_norm(2.)))\n",
    "\n",
    "model5.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5oKzoh_f07R",
    "outputId": "95601731-e2fc-4ed5-d8cc-dca4f8b3a684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_24 (Dense)            (None, 100)               400       \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 300)               60300     \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 300)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 200)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 47ms/step - loss: 22290.4219 - mse: 22290.4219 - mae: 98.7847 - val_loss: 27873.3926 - val_mse: 27873.3887 - val_mae: 105.8955\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12336.0713 - mse: 12336.0713 - mae: 78.9754 - val_loss: 8132.9819 - val_mse: 8132.9819 - val_mae: 63.8108\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3605.4382 - mse: 3605.4382 - mae: 39.9315 - val_loss: 4548.3262 - val_mse: 4548.3262 - val_mae: 30.6307\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1995.1492 - mse: 1995.1492 - mae: 27.6408 - val_loss: 2778.8604 - val_mse: 2778.8604 - val_mae: 28.9295\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1561.8003 - mse: 1561.8003 - mae: 26.6672 - val_loss: 2530.3550 - val_mse: 2530.3550 - val_mae: 31.6944\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1378.1855 - mse: 1378.1855 - mae: 26.7752 - val_loss: 2786.2788 - val_mse: 2786.2788 - val_mae: 30.9292\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1422.9119 - mse: 1422.9119 - mae: 26.0563 - val_loss: 2483.4175 - val_mse: 2483.4175 - val_mae: 30.0138\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1338.3081 - mse: 1338.3081 - mae: 27.3030 - val_loss: 2438.0645 - val_mse: 2438.0645 - val_mae: 27.9868\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1089.1617 - mse: 1089.1617 - mae: 22.5776 - val_loss: 2410.7275 - val_mse: 2410.7275 - val_mae: 25.8355\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1101.4086 - mse: 1101.4086 - mae: 22.2798 - val_loss: 2339.7769 - val_mse: 2339.7769 - val_mae: 24.0248\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 992.3365 - mse: 992.3365 - mae: 19.3722 - val_loss: 2111.2922 - val_mse: 2111.2922 - val_mae: 22.9504\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 932.1378 - mse: 932.1378 - mae: 19.1686 - val_loss: 2005.3817 - val_mse: 2005.3817 - val_mae: 22.2328\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 866.6266 - mse: 866.6266 - mae: 16.9087 - val_loss: 1890.4741 - val_mse: 1890.4741 - val_mae: 21.6588\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 790.5651 - mse: 790.5651 - mae: 17.1472 - val_loss: 1821.3427 - val_mse: 1821.3427 - val_mae: 20.9832\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 819.5472 - mse: 819.5472 - mae: 17.6338 - val_loss: 1920.0531 - val_mse: 1920.0531 - val_mae: 20.4933\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 774.4632 - mse: 774.4632 - mae: 16.3210 - val_loss: 1607.4500 - val_mse: 1607.4502 - val_mae: 20.1052\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 737.5245 - mse: 737.5245 - mae: 16.2477 - val_loss: 1767.1973 - val_mse: 1767.1973 - val_mae: 19.7880\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 804.2344 - mse: 804.2344 - mae: 16.9695 - val_loss: 1955.9913 - val_mse: 1955.9913 - val_mae: 20.5499\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 818.8337 - mse: 818.8337 - mae: 16.1420 - val_loss: 1544.0347 - val_mse: 1544.0349 - val_mae: 19.0762\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 890.3226 - mse: 890.3226 - mae: 18.7582 - val_loss: 1643.3228 - val_mse: 1643.3228 - val_mae: 19.3243\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 632.3293 - mse: 632.3293 - mae: 15.2011 - val_loss: 1752.5267 - val_mse: 1752.5267 - val_mae: 20.5982\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 631.8781 - mse: 631.8781 - mae: 15.0423 - val_loss: 1364.9506 - val_mse: 1364.9506 - val_mae: 18.4357\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 659.1611 - mse: 659.1611 - mae: 15.4684 - val_loss: 1514.1689 - val_mse: 1514.1689 - val_mae: 18.2582\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 741.0485 - mse: 741.0485 - mae: 15.2453 - val_loss: 1574.4313 - val_mse: 1574.4310 - val_mae: 18.2483\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 590.2017 - mse: 590.2017 - mae: 14.1890 - val_loss: 1354.7900 - val_mse: 1354.7900 - val_mae: 17.4991\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 570.7499 - mse: 570.7499 - mae: 13.6266 - val_loss: 1577.7126 - val_mse: 1577.7125 - val_mae: 18.1726\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 561.9749 - mse: 561.9749 - mae: 13.4911 - val_loss: 1355.6296 - val_mse: 1355.6296 - val_mae: 17.0392\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 564.8532 - mse: 564.8532 - mae: 13.4160 - val_loss: 1544.3628 - val_mse: 1544.3628 - val_mae: 17.2695\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 571.8156 - mse: 571.8156 - mae: 13.1825 - val_loss: 1262.3771 - val_mse: 1262.3771 - val_mae: 16.5223\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 570.0825 - mse: 570.0825 - mae: 13.4939 - val_loss: 1686.0250 - val_mse: 1686.0250 - val_mae: 17.6416\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.8804 - mse: 509.8804 - mae: 12.1886 - val_loss: 1187.2145 - val_mse: 1187.2145 - val_mae: 16.3170\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 539.1148 - mse: 539.1148 - mae: 12.6680 - val_loss: 1427.4779 - val_mse: 1427.4779 - val_mae: 17.0624\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 526.8745 - mse: 526.8745 - mae: 12.0422 - val_loss: 1423.3080 - val_mse: 1423.3080 - val_mae: 16.7631\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 489.1380 - mse: 489.1380 - mae: 12.4911 - val_loss: 1164.7958 - val_mse: 1164.7958 - val_mae: 15.9896\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 514.3956 - mse: 514.3956 - mae: 12.1816 - val_loss: 1363.6140 - val_mse: 1363.6139 - val_mae: 16.6008\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 475.1051 - mse: 475.1051 - mae: 11.4386 - val_loss: 1096.6373 - val_mse: 1096.6373 - val_mae: 15.5290\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 454.7108 - mse: 454.7108 - mae: 11.1385 - val_loss: 1384.0906 - val_mse: 1384.0906 - val_mae: 16.1727\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 446.3140 - mse: 446.3140 - mae: 11.2907 - val_loss: 1120.9630 - val_mse: 1120.9630 - val_mae: 15.5266\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 428.0740 - mse: 428.0740 - mae: 11.0178 - val_loss: 1454.4161 - val_mse: 1454.4161 - val_mae: 16.3107\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 445.3560 - mse: 445.3560 - mae: 11.1288 - val_loss: 1164.8507 - val_mse: 1164.8507 - val_mae: 15.5472\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 438.7445 - mse: 438.7445 - mae: 11.0736 - val_loss: 1167.9312 - val_mse: 1167.9312 - val_mae: 15.3989\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 440.7711 - mse: 440.7711 - mae: 11.4654 - val_loss: 1354.9199 - val_mse: 1354.9199 - val_mae: 15.5849\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 401.8058 - mse: 401.8058 - mae: 10.9137 - val_loss: 1098.7234 - val_mse: 1098.7234 - val_mae: 15.0766\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 417.8785 - mse: 417.8785 - mae: 10.7219 - val_loss: 1205.3217 - val_mse: 1205.3217 - val_mae: 15.4229\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 392.6398 - mse: 392.6398 - mae: 10.3146 - val_loss: 1101.4259 - val_mse: 1101.4259 - val_mae: 14.8132\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 366.8902 - mse: 366.8902 - mae: 9.8909 - val_loss: 1252.3636 - val_mse: 1252.3636 - val_mae: 15.0986\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 379.1097 - mse: 379.1097 - mae: 9.9386 - val_loss: 1142.8282 - val_mse: 1142.8282 - val_mae: 14.7430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 383.0526 - mse: 383.0526 - mae: 10.0856 - val_loss: 1172.1973 - val_mse: 1172.1973 - val_mae: 15.0219\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 381.2849 - mse: 381.2849 - mae: 9.9175 - val_loss: 1072.5397 - val_mse: 1072.5397 - val_mae: 14.4382\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 370.0210 - mse: 370.0210 - mae: 9.6563 - val_loss: 1099.8610 - val_mse: 1099.8610 - val_mae: 14.4292\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 351.4739 - mse: 351.4739 - mae: 9.8331 - val_loss: 992.1875 - val_mse: 992.1875 - val_mae: 14.1966\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 350.3808 - mse: 350.3808 - mae: 9.7993 - val_loss: 1089.2465 - val_mse: 1089.2465 - val_mae: 14.2407\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 332.4214 - mse: 332.4214 - mae: 9.5084 - val_loss: 989.1188 - val_mse: 989.1188 - val_mae: 13.9728\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 304.6544 - mse: 304.6544 - mae: 9.3642 - val_loss: 1169.6892 - val_mse: 1169.6892 - val_mae: 15.1966\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 305.5247 - mse: 305.5247 - mae: 9.3665 - val_loss: 919.8616 - val_mse: 919.8616 - val_mae: 13.5463\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 320.0289 - mse: 320.0289 - mae: 9.6167 - val_loss: 1111.3810 - val_mse: 1111.3810 - val_mae: 15.1670\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 304.5888 - mse: 304.5888 - mae: 9.7282 - val_loss: 977.3133 - val_mse: 977.3133 - val_mae: 13.5488\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 293.1499 - mse: 293.1499 - mae: 9.2829 - val_loss: 922.6966 - val_mse: 922.6966 - val_mae: 13.0328\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 270.5352 - mse: 270.5352 - mae: 8.4165 - val_loss: 977.8441 - val_mse: 977.8441 - val_mae: 13.6840\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 268.5685 - mse: 268.5685 - mae: 8.3421 - val_loss: 910.0201 - val_mse: 910.0201 - val_mae: 12.8109\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 280.8132 - mse: 280.8132 - mae: 8.8548 - val_loss: 921.3214 - val_mse: 921.3214 - val_mae: 13.1053\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.8429 - mse: 306.8429 - mae: 8.7205 - val_loss: 1017.3217 - val_mse: 1017.3217 - val_mae: 13.1617\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6324 - mse: 224.6324 - mae: 8.4577 - val_loss: 767.7673 - val_mse: 767.7673 - val_mae: 12.2910\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.3013 - mse: 261.3013 - mae: 8.4841 - val_loss: 935.0904 - val_mse: 935.0904 - val_mae: 14.1544\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.0428 - mse: 254.0428 - mae: 8.4365 - val_loss: 834.3509 - val_mse: 834.3509 - val_mae: 12.4153\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.6824 - mse: 239.6824 - mae: 8.2474 - val_loss: 983.9846 - val_mse: 983.9846 - val_mae: 13.0636\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.2485 - mse: 215.2485 - mae: 7.9046 - val_loss: 856.6702 - val_mse: 856.6702 - val_mae: 12.6582\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.5618 - mse: 211.5618 - mae: 8.0044 - val_loss: 857.1526 - val_mse: 857.1526 - val_mae: 12.3782\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 206.3300 - mse: 206.3300 - mae: 7.6126 - val_loss: 874.8674 - val_mse: 874.8674 - val_mae: 12.1217\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 195.4708 - mse: 195.4708 - mae: 7.7079 - val_loss: 812.0306 - val_mse: 812.0306 - val_mae: 12.5645\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 192.7312 - mse: 192.7312 - mae: 7.4702 - val_loss: 792.5127 - val_mse: 792.5127 - val_mae: 12.2211\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 179.7157 - mse: 179.7157 - mae: 7.3634 - val_loss: 802.1361 - val_mse: 802.1361 - val_mae: 11.9729\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 172.4122 - mse: 172.4122 - mae: 7.1870 - val_loss: 858.4992 - val_mse: 858.4992 - val_mae: 12.7866\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 204.1692 - mse: 204.1692 - mae: 7.4514 - val_loss: 744.5860 - val_mse: 744.5860 - val_mae: 11.6269\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 199.4058 - mse: 199.4058 - mae: 7.8638 - val_loss: 811.3956 - val_mse: 811.3956 - val_mae: 12.9470\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.2734 - mse: 248.2734 - mae: 8.8151 - val_loss: 921.7804 - val_mse: 921.7804 - val_mae: 13.0906\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 165.3858 - mse: 165.3858 - mae: 7.9788 - val_loss: 633.8847 - val_mse: 633.8847 - val_mae: 11.9252\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 182.4929 - mse: 182.4929 - mae: 7.5930 - val_loss: 959.9729 - val_mse: 959.9729 - val_mae: 14.5656\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 156.7222 - mse: 156.7222 - mae: 7.3493 - val_loss: 732.8173 - val_mse: 732.8173 - val_mae: 11.8682\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 141.4420 - mse: 141.4420 - mae: 6.9520 - val_loss: 974.5950 - val_mse: 974.5950 - val_mae: 14.1133\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 145.4307 - mse: 145.4307 - mae: 6.7765 - val_loss: 639.9447 - val_mse: 639.9447 - val_mae: 11.8549\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 140.0676 - mse: 140.0676 - mae: 6.7482 - val_loss: 760.7141 - val_mse: 760.7141 - val_mae: 12.7291\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 129.7704 - mse: 129.7704 - mae: 6.5118 - val_loss: 752.6640 - val_mse: 752.6640 - val_mae: 12.2185\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 128.8028 - mse: 128.8028 - mae: 6.3571 - val_loss: 682.7035 - val_mse: 682.7035 - val_mae: 11.9753\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 119.6839 - mse: 119.6839 - mae: 6.3006 - val_loss: 752.1429 - val_mse: 752.1429 - val_mae: 12.7957\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 121.2766 - mse: 121.2766 - mae: 6.2522 - val_loss: 658.4626 - val_mse: 658.4626 - val_mae: 11.9053\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 118.6611 - mse: 118.6611 - mae: 6.3157 - val_loss: 768.9305 - val_mse: 768.9304 - val_mae: 12.6983\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 116.9189 - mse: 116.9189 - mae: 6.1609 - val_loss: 606.3510 - val_mse: 606.3510 - val_mae: 11.7681\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 139.9575 - mse: 139.9575 - mae: 6.6273 - val_loss: 716.6714 - val_mse: 716.6714 - val_mae: 12.5471\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117.5907 - mse: 117.5907 - mae: 6.3425 - val_loss: 667.4534 - val_mse: 667.4534 - val_mae: 12.2985\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 114.5245 - mse: 114.5245 - mae: 6.2326 - val_loss: 641.5270 - val_mse: 641.5270 - val_mae: 12.0047\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 110.3175 - mse: 110.3175 - mae: 5.9971 - val_loss: 628.0999 - val_mse: 628.0999 - val_mae: 12.2498\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 119.9448 - mse: 119.9448 - mae: 6.1031 - val_loss: 608.6826 - val_mse: 608.6826 - val_mae: 11.3922\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.6241 - mse: 98.6241 - mae: 5.9552 - val_loss: 668.8965 - val_mse: 668.8965 - val_mae: 12.3036\n",
      "Epoch 95/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 94.8051 - mse: 94.8051 - mae: 5.7425 - val_loss: 601.6069 - val_mse: 601.6069 - val_mae: 11.8228\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87.1623 - mse: 87.1623 - mae: 5.4579 - val_loss: 672.4236 - val_mse: 672.4236 - val_mae: 12.2760\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 88.6186 - mse: 88.6186 - mae: 5.4628 - val_loss: 564.6701 - val_mse: 564.6701 - val_mae: 11.6484\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 93.5791 - mse: 93.5791 - mae: 5.6755 - val_loss: 722.3646 - val_mse: 722.3646 - val_mae: 12.9477\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 97.0068 - mse: 97.0068 - mae: 5.7651 - val_loss: 550.3746 - val_mse: 550.3746 - val_mae: 11.2309\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 121.2816 - mse: 121.2816 - mae: 6.5106 - val_loss: 745.3749 - val_mse: 745.3749 - val_mae: 13.7393\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 102.7368 - mse: 102.7368 - mae: 6.1142 - val_loss: 535.6903 - val_mse: 535.6903 - val_mae: 11.1025\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 96.5205 - mse: 96.5205 - mae: 6.0822 - val_loss: 665.3199 - val_mse: 665.3199 - val_mae: 12.4649\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 78.9825 - mse: 78.9825 - mae: 5.2226 - val_loss: 544.7654 - val_mse: 544.7654 - val_mae: 11.4750\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 92.5866 - mse: 92.5866 - mae: 5.6550 - val_loss: 610.2295 - val_mse: 610.2295 - val_mae: 12.4178\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 91.6117 - mse: 91.6117 - mae: 5.3518 - val_loss: 559.7938 - val_mse: 559.7938 - val_mae: 11.4842\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 165.3426 - mse: 165.3426 - mae: 7.3581 - val_loss: 674.4613 - val_mse: 674.4613 - val_mae: 13.4433\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 143.7772 - mse: 143.7772 - mae: 7.4474 - val_loss: 601.4953 - val_mse: 601.4953 - val_mae: 12.2953\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 133.8973 - mse: 133.8973 - mae: 7.3136 - val_loss: 605.0901 - val_mse: 605.0901 - val_mae: 12.0706\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 118.3782 - mse: 118.3782 - mae: 6.6558 - val_loss: 694.8669 - val_mse: 694.8669 - val_mae: 13.3082\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 156.2021 - mse: 156.2021 - mae: 7.0801 - val_loss: 548.4304 - val_mse: 548.4304 - val_mae: 11.7074\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 179.3302 - mse: 179.3302 - mae: 6.7295 - val_loss: 642.9074 - val_mse: 642.9074 - val_mae: 12.3758\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 159.3933 - mse: 159.3933 - mae: 6.8195 - val_loss: 663.1032 - val_mse: 663.1032 - val_mae: 13.5528\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 150.6609 - mse: 150.6609 - mae: 7.1992 - val_loss: 524.5311 - val_mse: 524.5311 - val_mae: 11.2688\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 150.8238 - mse: 150.8238 - mae: 7.5573 - val_loss: 596.6451 - val_mse: 596.6451 - val_mae: 12.2830\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 157.1034 - mse: 157.1034 - mae: 7.2192 - val_loss: 550.2324 - val_mse: 550.2324 - val_mae: 11.8111\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 116.9621 - mse: 116.9621 - mae: 6.5771 - val_loss: 604.3665 - val_mse: 604.3665 - val_mae: 11.7217\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 121.3569 - mse: 121.3569 - mae: 6.1158 - val_loss: 635.6342 - val_mse: 635.6342 - val_mae: 12.2055\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 103.9812 - mse: 103.9812 - mae: 6.0409 - val_loss: 540.6859 - val_mse: 540.6859 - val_mae: 11.9206\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 78.6759 - mse: 78.6759 - mae: 5.5579 - val_loss: 630.1376 - val_mse: 630.1376 - val_mae: 12.8904\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.3788 - mse: 65.3788 - mae: 4.8373 - val_loss: 536.4311 - val_mse: 536.4311 - val_mae: 11.2833\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 61.5697 - mse: 61.5697 - mae: 4.6200 - val_loss: 640.4763 - val_mse: 640.4763 - val_mae: 12.4085\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 53.5426 - mse: 53.5426 - mae: 4.4035 - val_loss: 509.7580 - val_mse: 509.7580 - val_mae: 11.1319\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 51.4791 - mse: 51.4791 - mae: 4.2446 - val_loss: 602.9173 - val_mse: 602.9173 - val_mae: 12.5502\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 57.5090 - mse: 57.5090 - mae: 4.4599 - val_loss: 483.8005 - val_mse: 483.8005 - val_mae: 11.5152\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 53.3175 - mse: 53.3175 - mae: 4.3586 - val_loss: 624.5301 - val_mse: 624.5301 - val_mae: 12.8573\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60.9011 - mse: 60.9011 - mae: 4.5684 - val_loss: 524.4808 - val_mse: 524.4808 - val_mae: 11.3316\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49.9325 - mse: 49.9325 - mae: 4.2104 - val_loss: 659.6600 - val_mse: 659.6600 - val_mae: 13.0193\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 54.9051 - mse: 54.9051 - mae: 4.3888 - val_loss: 503.2654 - val_mse: 503.2654 - val_mae: 11.2825\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52.4731 - mse: 52.4731 - mae: 4.3475 - val_loss: 608.3555 - val_mse: 608.3555 - val_mae: 12.8493\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.9114 - mse: 48.9114 - mae: 4.1676 - val_loss: 494.3582 - val_mse: 494.3582 - val_mae: 11.2786\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52.1875 - mse: 52.1875 - mae: 4.3651 - val_loss: 639.0674 - val_mse: 639.0674 - val_mae: 12.8675\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 61.2187 - mse: 61.2187 - mae: 4.6424 - val_loss: 513.3358 - val_mse: 513.3358 - val_mae: 11.6626\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60.7304 - mse: 60.7304 - mae: 4.7486 - val_loss: 627.8928 - val_mse: 627.8928 - val_mae: 12.5477\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 59.8763 - mse: 59.8763 - mae: 4.4932 - val_loss: 474.6428 - val_mse: 474.6428 - val_mae: 11.0670\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.7643 - mse: 48.7643 - mae: 4.1415 - val_loss: 579.8987 - val_mse: 579.8987 - val_mae: 12.4515\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.3227 - mse: 43.3227 - mae: 3.9501 - val_loss: 498.7547 - val_mse: 498.7547 - val_mae: 11.2381\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 47.2191 - mse: 47.2191 - mae: 4.1861 - val_loss: 585.7861 - val_mse: 585.7861 - val_mae: 12.4641\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 50.2678 - mse: 50.2678 - mae: 4.2375 - val_loss: 527.8173 - val_mse: 527.8173 - val_mae: 11.4464\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 56.2862 - mse: 56.2862 - mae: 4.5258 - val_loss: 584.3987 - val_mse: 584.3987 - val_mae: 11.9723\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.8945 - mse: 41.8945 - mae: 3.8942 - val_loss: 558.5735 - val_mse: 558.5735 - val_mae: 12.0084\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.6805 - mse: 37.6805 - mae: 3.6437 - val_loss: 514.3896 - val_mse: 514.3896 - val_mae: 11.4708\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.5935 - mse: 35.5935 - mae: 3.5274 - val_loss: 539.6761 - val_mse: 539.6761 - val_mae: 11.7938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.0634 - mse: 38.0634 - mae: 3.6530 - val_loss: 447.6402 - val_mse: 447.6402 - val_mae: 10.8848\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.1607 - mse: 43.1607 - mae: 3.9008 - val_loss: 606.1542 - val_mse: 606.1542 - val_mae: 12.3900\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.9514 - mse: 38.9514 - mae: 3.7694 - val_loss: 499.5532 - val_mse: 499.5532 - val_mae: 11.2457\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.9857 - mse: 36.9857 - mae: 3.6274 - val_loss: 563.0886 - val_mse: 563.0886 - val_mae: 12.1371\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.3082 - mse: 34.3082 - mae: 3.4737 - val_loss: 507.6019 - val_mse: 507.6019 - val_mae: 11.3851\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.4121 - mse: 32.4121 - mae: 3.3346 - val_loss: 580.2809 - val_mse: 580.2809 - val_mae: 12.2580\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.2369 - mse: 34.2369 - mae: 3.5081 - val_loss: 500.5508 - val_mse: 500.5508 - val_mae: 11.1105\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.9627 - mse: 33.9627 - mae: 3.4451 - val_loss: 541.4824 - val_mse: 541.4824 - val_mae: 11.8407\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.2904 - mse: 32.2904 - mae: 3.2936 - val_loss: 515.5970 - val_mse: 515.5970 - val_mae: 11.5561\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.3191 - mse: 31.3191 - mae: 3.2819 - val_loss: 536.3099 - val_mse: 536.3099 - val_mae: 11.7794\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.7251 - mse: 30.7251 - mae: 3.2471 - val_loss: 501.5235 - val_mse: 501.5235 - val_mae: 11.5026\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.9644 - mse: 30.9644 - mae: 3.2874 - val_loss: 524.7662 - val_mse: 524.7662 - val_mae: 11.5930\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.7881 - mse: 29.7881 - mae: 3.2521 - val_loss: 503.8695 - val_mse: 503.8695 - val_mae: 11.4338\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.3441 - mse: 28.3441 - mae: 3.1581 - val_loss: 549.8809 - val_mse: 549.8809 - val_mae: 11.9461\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.4121 - mse: 30.4121 - mae: 3.1818 - val_loss: 479.6263 - val_mse: 479.6263 - val_mae: 11.2255\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.7863 - mse: 29.7863 - mae: 3.2806 - val_loss: 546.9417 - val_mse: 546.9417 - val_mae: 11.9747\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.8882 - mse: 27.8882 - mae: 3.0757 - val_loss: 490.4499 - val_mse: 490.4499 - val_mae: 11.2992\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.9423 - mse: 31.9423 - mae: 3.3562 - val_loss: 515.3509 - val_mse: 515.3509 - val_mae: 11.6993\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.8953 - mse: 27.8953 - mae: 3.1890 - val_loss: 506.1638 - val_mse: 506.1638 - val_mae: 11.3219\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.6503 - mse: 27.6503 - mae: 3.1393 - val_loss: 524.9039 - val_mse: 524.9039 - val_mae: 11.7280\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.0103 - mse: 26.0103 - mae: 3.0381 - val_loss: 496.7746 - val_mse: 496.7746 - val_mae: 11.3532\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.1834 - mse: 26.1834 - mae: 3.1467 - val_loss: 542.3405 - val_mse: 542.3405 - val_mae: 11.7573\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.2119 - mse: 27.2119 - mae: 3.0447 - val_loss: 496.4082 - val_mse: 496.4082 - val_mae: 11.4451\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.6112 - mse: 25.6112 - mae: 2.9791 - val_loss: 535.3094 - val_mse: 535.3094 - val_mae: 11.8989\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.8861 - mse: 24.8861 - mae: 2.9597 - val_loss: 510.5570 - val_mse: 510.5570 - val_mae: 11.6248\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.4352 - mse: 24.4352 - mae: 2.9299 - val_loss: 514.7557 - val_mse: 514.7557 - val_mae: 11.6590\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.0020 - mse: 24.0020 - mae: 2.8983 - val_loss: 487.9326 - val_mse: 487.9326 - val_mae: 11.2877\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 26.2061 - mse: 26.2061 - mae: 3.1475 - val_loss: 596.2545 - val_mse: 596.2545 - val_mae: 12.4226\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.6696 - mse: 35.6696 - mae: 3.4607 - val_loss: 482.5118 - val_mse: 482.5118 - val_mae: 11.2580\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 24.4548 - mse: 24.4548 - mae: 2.9093 - val_loss: 592.3857 - val_mse: 592.3857 - val_mae: 11.9240\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.0371 - mse: 27.0371 - mae: 3.0944 - val_loss: 457.5981 - val_mse: 457.5981 - val_mae: 10.9464\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 26.0744 - mse: 26.0744 - mae: 3.1390 - val_loss: 603.8859 - val_mse: 603.8859 - val_mae: 12.4574\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.0484 - mse: 28.0484 - mae: 3.2499 - val_loss: 444.0134 - val_mse: 444.0134 - val_mae: 10.9090\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45.1456 - mse: 45.1456 - mae: 3.8398 - val_loss: 610.7119 - val_mse: 610.7119 - val_mae: 12.1153\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.4336 - mse: 36.4336 - mae: 3.6388 - val_loss: 525.0923 - val_mse: 525.0923 - val_mae: 11.2609\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.5072 - mse: 33.5072 - mae: 3.6101 - val_loss: 490.4635 - val_mse: 490.4635 - val_mae: 11.8178\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.5125 - mse: 32.5125 - mae: 3.6349 - val_loss: 573.8613 - val_mse: 573.8613 - val_mae: 12.2841\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.0687 - mse: 24.0687 - mae: 2.9231 - val_loss: 487.4304 - val_mse: 487.4304 - val_mae: 11.3570\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.0372 - mse: 26.0372 - mae: 3.1166 - val_loss: 600.9216 - val_mse: 600.9216 - val_mae: 12.2694\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.4967 - mse: 24.4967 - mae: 3.0089 - val_loss: 469.3695 - val_mse: 469.3695 - val_mae: 11.0508\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.3689 - mse: 23.3689 - mae: 3.0602 - val_loss: 608.5685 - val_mse: 608.5685 - val_mae: 12.6575\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.1040 - mse: 23.1040 - mae: 2.8421 - val_loss: 438.2237 - val_mse: 438.2237 - val_mae: 10.8938\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.7952 - mse: 27.7952 - mae: 3.1238 - val_loss: 650.1089 - val_mse: 650.1089 - val_mae: 12.7098\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.3261 - mse: 24.3261 - mae: 3.0407 - val_loss: 462.6736 - val_mse: 462.6736 - val_mae: 10.9938\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.1342 - mse: 29.1342 - mae: 3.2180 - val_loss: 623.7324 - val_mse: 623.7324 - val_mae: 12.5764\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.4385 - mse: 21.4385 - mae: 2.9509 - val_loss: 505.2951 - val_mse: 505.2951 - val_mae: 11.2226\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.1294 - mse: 20.1294 - mae: 2.7099 - val_loss: 616.2413 - val_mse: 616.2413 - val_mae: 12.5166\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.9662 - mse: 21.9662 - mae: 2.8749 - val_loss: 511.4207 - val_mse: 511.4207 - val_mae: 11.3364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.5212 - mse: 22.5212 - mae: 2.7875 - val_loss: 534.3828 - val_mse: 534.3828 - val_mae: 11.7653\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.2557 - mse: 19.2557 - mae: 2.6404 - val_loss: 527.3383 - val_mse: 527.3383 - val_mae: 11.5810\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.1912 - mse: 20.1912 - mae: 2.6448 - val_loss: 500.2214 - val_mse: 500.2214 - val_mae: 11.2927\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.6435 - mse: 19.6435 - mae: 2.6736 - val_loss: 526.8422 - val_mse: 526.8422 - val_mae: 11.8016\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.7710 - mse: 18.7710 - mae: 2.5982 - val_loss: 482.1844 - val_mse: 482.1844 - val_mae: 11.2649\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.1172 - mse: 20.1172 - mae: 2.6828 - val_loss: 563.0303 - val_mse: 563.0303 - val_mae: 12.2212\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.2045 - mse: 19.2045 - mae: 2.6045 - val_loss: 469.6542 - val_mse: 469.6542 - val_mae: 11.3076\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.0538 - mse: 21.0538 - mae: 2.6702 - val_loss: 560.5012 - val_mse: 560.5012 - val_mae: 12.1214\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.3831 - mse: 18.3831 - mae: 2.5341 - val_loss: 494.9195 - val_mse: 494.9195 - val_mae: 11.4211\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.4512 - mse: 18.4512 - mae: 2.5423 - val_loss: 526.1920 - val_mse: 526.1920 - val_mae: 11.5763\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.0856 - mse: 17.0856 - mae: 2.4008 - val_loss: 514.9835 - val_mse: 514.9835 - val_mae: 11.6635\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.1383 - mse: 17.1383 - mae: 2.4443 - val_loss: 514.2534 - val_mse: 514.2534 - val_mae: 11.4503\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.5740 - mse: 16.5740 - mae: 2.3817 - val_loss: 513.9828 - val_mse: 513.9828 - val_mae: 11.5666\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.2883 - mse: 20.2883 - mae: 2.6855 - val_loss: 463.5935 - val_mse: 463.5935 - val_mae: 11.3420\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.6510 - mse: 24.6510 - mae: 2.8851 - val_loss: 620.1078 - val_mse: 620.1078 - val_mae: 12.3408\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.9275 - mse: 28.9275 - mae: 3.0628 - val_loss: 462.0776 - val_mse: 462.0776 - val_mae: 10.7944\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.0404 - mse: 37.0404 - mae: 3.5905 - val_loss: 625.3337 - val_mse: 625.3337 - val_mae: 13.3889\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.0363 - mse: 29.0363 - mae: 3.3523 - val_loss: 452.6046 - val_mse: 452.6046 - val_mae: 11.1149\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.1126 - mse: 30.1126 - mae: 3.5777 - val_loss: 656.7126 - val_mse: 656.7126 - val_mae: 12.9545\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.7393 - mse: 33.7393 - mae: 3.3497 - val_loss: 386.5390 - val_mse: 386.5389 - val_mae: 10.3897\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.1611 - mse: 30.1611 - mae: 3.2524 - val_loss: 693.0618 - val_mse: 693.0618 - val_mae: 13.3007\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.4852 - mse: 27.4852 - mae: 3.4112 - val_loss: 451.7691 - val_mse: 451.7691 - val_mae: 10.7211\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.3553 - mse: 65.3553 - mae: 4.4331 - val_loss: 720.3859 - val_mse: 720.3859 - val_mae: 12.8947\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.1733 - mse: 28.1733 - mae: 3.2615 - val_loss: 460.0427 - val_mse: 460.0427 - val_mae: 11.2840\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.6237 - mse: 39.6237 - mae: 3.5560 - val_loss: 374.6446 - val_mse: 374.6446 - val_mae: 10.6944\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 50.0304 - mse: 50.0304 - mae: 3.8065 - val_loss: 654.6147 - val_mse: 654.6147 - val_mae: 12.9167\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 35.6418 - mse: 35.6418 - mae: 3.3525 - val_loss: 429.8178 - val_mse: 429.8178 - val_mae: 10.6745\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 33.0437 - mse: 33.0437 - mae: 3.2382 - val_loss: 561.8130 - val_mse: 561.8130 - val_mae: 12.1253\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.4114 - mse: 26.4114 - mae: 3.0284 - val_loss: 534.2584 - val_mse: 534.2584 - val_mae: 11.8144\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.3413 - mse: 25.3413 - mae: 2.9596 - val_loss: 518.9612 - val_mse: 518.9612 - val_mae: 11.5005\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.4394 - mse: 20.4394 - mae: 2.7870 - val_loss: 531.1116 - val_mse: 531.1116 - val_mae: 12.0102\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.1956 - mse: 16.1956 - mae: 2.4419 - val_loss: 504.5663 - val_mse: 504.5663 - val_mae: 11.3484\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.0497 - mse: 16.0497 - mae: 2.3894 - val_loss: 507.4277 - val_mse: 507.4277 - val_mae: 11.5606\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.8549 - mse: 14.8549 - mae: 2.2891 - val_loss: 493.4225 - val_mse: 493.4225 - val_mae: 11.6152\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 15.2789 - mse: 15.2789 - mae: 2.2291 - val_loss: 530.4388 - val_mse: 530.4388 - val_mae: 11.7091\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.5454 - mse: 15.5454 - mae: 2.2733 - val_loss: 489.0217 - val_mse: 489.0217 - val_mae: 11.5084\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.9461 - mse: 14.9461 - mae: 2.2070 - val_loss: 540.8527 - val_mse: 540.8527 - val_mae: 11.8697\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0920 - mse: 15.0920 - mae: 2.2297 - val_loss: 490.3115 - val_mse: 490.3115 - val_mae: 11.3524\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.6817 - mse: 13.6817 - mae: 2.1342 - val_loss: 522.5958 - val_mse: 522.5958 - val_mae: 11.7751\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.0006 - mse: 15.0006 - mae: 2.2125 - val_loss: 469.2189 - val_mse: 469.2189 - val_mae: 11.1792\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.6140 - mse: 16.6140 - mae: 2.4704 - val_loss: 566.0221 - val_mse: 566.0221 - val_mae: 12.0927\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.3943 - mse: 23.3943 - mae: 2.8731 - val_loss: 469.2435 - val_mse: 469.2435 - val_mae: 11.3891\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.0488 - mse: 17.0488 - mae: 2.4033 - val_loss: 525.0586 - val_mse: 525.0586 - val_mae: 11.7383\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.3818 - mse: 17.3818 - mae: 2.4575 - val_loss: 520.1124 - val_mse: 520.1124 - val_mae: 11.9336\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.1859 - mse: 19.1859 - mae: 2.5613 - val_loss: 455.6257 - val_mse: 455.6257 - val_mae: 11.0383\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.4327 - mse: 18.4327 - mae: 2.3564 - val_loss: 626.4448 - val_mse: 626.4448 - val_mae: 12.5142\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.8106 - mse: 18.8106 - mae: 2.3334 - val_loss: 472.7756 - val_mse: 472.7756 - val_mae: 10.9489\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.4829 - mse: 15.4829 - mae: 2.3983 - val_loss: 539.6611 - val_mse: 539.6611 - val_mae: 11.8760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.8612 - mse: 13.8612 - mae: 2.1746 - val_loss: 502.3944 - val_mse: 502.3944 - val_mae: 11.7621\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7204 - mse: 13.7204 - mae: 2.1892 - val_loss: 528.2935 - val_mse: 528.2935 - val_mae: 11.9031\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7070 - mse: 13.7070 - mae: 2.0628 - val_loss: 507.0776 - val_mse: 507.0776 - val_mae: 11.6140\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4432 - mse: 13.4432 - mae: 2.1142 - val_loss: 543.5862 - val_mse: 543.5862 - val_mae: 11.6870\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.2513 - mse: 14.2513 - mae: 2.1763 - val_loss: 479.7292 - val_mse: 479.7292 - val_mae: 11.4346\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1779 - mse: 13.1779 - mae: 2.0342 - val_loss: 561.3898 - val_mse: 561.3898 - val_mae: 12.0504\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.1248 - mse: 13.1248 - mae: 2.1186 - val_loss: 486.4986 - val_mse: 486.4986 - val_mae: 11.4391\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.9323 - mse: 15.9323 - mae: 2.2931 - val_loss: 496.0151 - val_mse: 496.0151 - val_mae: 11.4187\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.5310 - mse: 14.5310 - mae: 2.2312 - val_loss: 544.2896 - val_mse: 544.2896 - val_mae: 11.9114\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3210 - mse: 13.3210 - mae: 2.0675 - val_loss: 500.6516 - val_mse: 500.6516 - val_mae: 11.3774\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.4970 - mse: 12.4970 - mae: 2.0169 - val_loss: 515.7530 - val_mse: 515.7530 - val_mae: 11.4128\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.2466 - mse: 12.2466 - mae: 1.9635 - val_loss: 486.6351 - val_mse: 486.6351 - val_mae: 11.4216\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.2661 - mse: 12.2661 - mae: 1.9737 - val_loss: 516.4998 - val_mse: 516.4998 - val_mae: 11.6596\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.5935 - mse: 12.5935 - mae: 1.9351 - val_loss: 502.4233 - val_mse: 502.4233 - val_mae: 11.5170\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.7344 - mse: 12.7344 - mae: 2.0662 - val_loss: 569.0155 - val_mse: 569.0155 - val_mae: 11.9619\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.5328 - mse: 13.5328 - mae: 2.1562 - val_loss: 495.0232 - val_mse: 495.0232 - val_mae: 11.3083\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.1013 - mse: 12.1013 - mae: 2.0284 - val_loss: 526.0441 - val_mse: 526.0441 - val_mae: 11.9780\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8211 - mse: 11.8211 - mae: 1.9586 - val_loss: 528.2558 - val_mse: 528.2558 - val_mae: 11.6445\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.8357 - mse: 13.8357 - mae: 2.1430 - val_loss: 462.9753 - val_mse: 462.9753 - val_mae: 11.1771\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.2262 - mse: 15.2262 - mae: 2.2808 - val_loss: 577.3075 - val_mse: 577.3075 - val_mae: 12.4434\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.8596 - mse: 17.8596 - mae: 2.4540 - val_loss: 424.7036 - val_mse: 424.7036 - val_mae: 10.5915\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.1112 - mse: 16.1112 - mae: 2.3656 - val_loss: 650.1406 - val_mse: 650.1406 - val_mae: 12.8296\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.5843 - mse: 20.5843 - mae: 2.6932 - val_loss: 413.4171 - val_mse: 413.4171 - val_mae: 10.5314\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.6368 - mse: 43.6368 - mae: 4.0197 - val_loss: 584.3282 - val_mse: 584.3282 - val_mae: 12.8024\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.7331 - mse: 19.7331 - mae: 2.5469 - val_loss: 473.9835 - val_mse: 473.9835 - val_mae: 11.2127\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.8109 - mse: 16.8109 - mae: 2.4117 - val_loss: 502.5384 - val_mse: 502.5384 - val_mae: 11.9604\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.9868 - mse: 13.9868 - mae: 2.1964 - val_loss: 579.5948 - val_mse: 579.5948 - val_mae: 12.0624\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9061 - mse: 11.9061 - mae: 2.0512 - val_loss: 513.8716 - val_mse: 513.8716 - val_mae: 11.4613\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.4297 - mse: 14.4297 - mae: 2.3951 - val_loss: 654.4499 - val_mse: 654.4499 - val_mae: 12.6197\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.7217 - mse: 15.7217 - mae: 2.4522 - val_loss: 518.3056 - val_mse: 518.3056 - val_mae: 11.0136\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8254 - mse: 11.8254 - mae: 2.0245 - val_loss: 589.8871 - val_mse: 589.8871 - val_mae: 12.4166\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7322 - mse: 13.7322 - mae: 2.1898 - val_loss: 560.4602 - val_mse: 560.4602 - val_mae: 11.6399\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.2724 - mse: 12.2724 - mae: 2.0410 - val_loss: 499.7323 - val_mse: 499.7323 - val_mae: 11.3146\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.7444 - mse: 11.7444 - mae: 1.9791 - val_loss: 543.8997 - val_mse: 543.8997 - val_mae: 11.9121\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.9725 - mse: 10.9725 - mae: 1.9044 - val_loss: 507.6390 - val_mse: 507.6390 - val_mae: 11.4172\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9856 - mse: 11.9856 - mae: 1.9679 - val_loss: 512.9642 - val_mse: 512.9642 - val_mae: 11.6316\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6133 - mse: 10.6133 - mae: 1.7420 - val_loss: 529.3478 - val_mse: 529.3478 - val_mae: 11.6324\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7917 - mse: 10.7917 - mae: 1.7312 - val_loss: 546.3174 - val_mse: 546.3174 - val_mae: 11.8708\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1791 - mse: 11.1791 - mae: 1.8060 - val_loss: 478.0389 - val_mse: 478.0389 - val_mae: 11.1558\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.8867 - mse: 13.8867 - mae: 2.0424 - val_loss: 581.1329 - val_mse: 581.1329 - val_mae: 11.9605\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.4375 - mse: 11.4375 - mae: 1.9325 - val_loss: 515.9060 - val_mse: 515.9060 - val_mae: 11.5535\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0171 - mse: 12.0171 - mae: 1.9788 - val_loss: 499.1777 - val_mse: 499.1777 - val_mae: 11.3320\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0757 - mse: 11.0757 - mae: 1.8661 - val_loss: 547.9070 - val_mse: 547.9070 - val_mae: 11.6660\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7003 - mse: 9.7003 - mae: 1.7614 - val_loss: 499.2975 - val_mse: 499.2975 - val_mae: 11.3828\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0520 - mse: 10.0520 - mae: 1.7238 - val_loss: 530.9138 - val_mse: 530.9138 - val_mae: 11.6712\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5037 - mse: 9.5037 - mae: 1.6579 - val_loss: 526.2546 - val_mse: 526.2546 - val_mae: 11.5967\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5420 - mse: 9.5420 - mae: 1.6359 - val_loss: 544.7985 - val_mse: 544.7985 - val_mae: 11.6993\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.4878 - mse: 10.4878 - mae: 1.8317 - val_loss: 492.3593 - val_mse: 492.3593 - val_mae: 11.3096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1359 - mse: 11.1359 - mae: 1.8093 - val_loss: 517.8301 - val_mse: 517.8301 - val_mae: 11.5751\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.5351 - mse: 9.5351 - mae: 1.6366 - val_loss: 530.2731 - val_mse: 530.2731 - val_mae: 11.6278\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0930 - mse: 9.0930 - mae: 1.5996 - val_loss: 483.3334 - val_mse: 483.3334 - val_mae: 11.2488\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.7843 - mse: 11.7843 - mae: 1.7892 - val_loss: 556.1858 - val_mse: 556.1858 - val_mae: 11.8351\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.9679 - mse: 13.9679 - mae: 2.0426 - val_loss: 561.1293 - val_mse: 561.1293 - val_mae: 12.0086\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 11.1653 - mse: 11.1653 - mae: 1.8966 - val_loss: 501.8444 - val_mse: 501.8444 - val_mae: 11.4305\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.8310 - mse: 9.8310 - mae: 1.8156 - val_loss: 609.2447 - val_mse: 609.2447 - val_mae: 12.2842\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.0987 - mse: 16.0987 - mae: 2.1637 - val_loss: 472.5857 - val_mse: 472.5857 - val_mae: 11.2692\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.0342 - mse: 19.0342 - mae: 2.4328 - val_loss: 539.9317 - val_mse: 539.9317 - val_mae: 11.6182\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.9768 - mse: 14.9768 - mae: 2.2224 - val_loss: 616.3456 - val_mse: 616.3456 - val_mae: 12.2263\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.4104 - mse: 25.4104 - mae: 2.6727 - val_loss: 496.4688 - val_mse: 496.4688 - val_mae: 11.5503\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.9806 - mse: 21.9806 - mae: 2.7451 - val_loss: 483.7624 - val_mse: 483.7624 - val_mae: 11.0863\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.3929 - mse: 24.3929 - mae: 2.8811 - val_loss: 659.6224 - val_mse: 659.6224 - val_mae: 12.7945\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.2778 - mse: 16.2778 - mae: 2.6757 - val_loss: 504.2245 - val_mse: 504.2245 - val_mae: 10.6623\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 23.7787 - mse: 23.7787 - mae: 2.8303 - val_loss: 582.7584 - val_mse: 582.7584 - val_mae: 12.4287\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3645 - mse: 13.3645 - mae: 2.3144 - val_loss: 567.1097 - val_mse: 567.1097 - val_mae: 11.9171\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7263 - mse: 10.7263 - mae: 2.0370 - val_loss: 530.2823 - val_mse: 530.2823 - val_mae: 11.5611\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2914 - mse: 10.2914 - mae: 2.0563 - val_loss: 528.7747 - val_mse: 528.7747 - val_mae: 11.5389\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.2622 - mse: 10.2622 - mae: 1.9256 - val_loss: 558.9612 - val_mse: 558.9612 - val_mae: 12.0322\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3620 - mse: 10.3620 - mae: 1.8620 - val_loss: 503.7993 - val_mse: 503.7993 - val_mae: 11.4624\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3822 - mse: 9.3822 - mae: 1.6839 - val_loss: 529.6712 - val_mse: 529.6712 - val_mae: 11.3805\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7533 - mse: 8.7533 - mae: 1.6071 - val_loss: 553.5172 - val_mse: 553.5172 - val_mae: 11.9158\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5535 - mse: 8.5535 - mae: 1.5690 - val_loss: 502.2129 - val_mse: 502.2129 - val_mae: 11.3794\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0351 - mse: 10.0351 - mae: 1.7225 - val_loss: 573.6987 - val_mse: 573.6987 - val_mae: 12.0471\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.4290 - mse: 8.4290 - mae: 1.5145 - val_loss: 505.8736 - val_mse: 505.8736 - val_mae: 11.1691\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7745 - mse: 8.7745 - mae: 1.6045 - val_loss: 567.3771 - val_mse: 567.3771 - val_mae: 11.9286\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.0942 - mse: 9.0942 - mae: 1.5801 - val_loss: 529.9100 - val_mse: 529.9100 - val_mae: 11.4775\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5910 - mse: 8.5910 - mae: 1.5510 - val_loss: 531.4645 - val_mse: 531.4645 - val_mae: 11.5066\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3099 - mse: 8.3099 - mae: 1.5793 - val_loss: 569.6795 - val_mse: 569.6795 - val_mae: 12.0058\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.1179 - mse: 8.1179 - mae: 1.5132 - val_loss: 533.4139 - val_mse: 533.4139 - val_mae: 11.4767\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.6703 - mse: 8.6703 - mae: 1.5009 - val_loss: 572.8347 - val_mse: 572.8347 - val_mae: 12.0791\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3882 - mse: 9.3882 - mae: 1.6197 - val_loss: 530.6113 - val_mse: 530.6113 - val_mae: 11.5563\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9571 - mse: 8.9571 - mae: 1.5106 - val_loss: 494.2574 - val_mse: 494.2574 - val_mae: 11.2817\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4214 - mse: 9.4214 - mae: 1.6624 - val_loss: 562.4955 - val_mse: 562.4955 - val_mae: 11.8439\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8360 - mse: 7.8360 - mae: 1.4775 - val_loss: 519.7275 - val_mse: 519.7275 - val_mae: 11.4514\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7850 - mse: 7.7850 - mae: 1.4438 - val_loss: 543.3450 - val_mse: 543.3450 - val_mae: 11.5720\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.7992 - mse: 7.7992 - mae: 1.4483 - val_loss: 548.5360 - val_mse: 548.5360 - val_mae: 11.6921\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3467 - mse: 7.3467 - mae: 1.4139 - val_loss: 530.2161 - val_mse: 530.2161 - val_mae: 11.4509\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.3979 - mse: 8.3979 - mae: 1.5068 - val_loss: 516.7833 - val_mse: 516.7833 - val_mae: 11.4456\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.9889 - mse: 8.9889 - mae: 1.6149 - val_loss: 566.9918 - val_mse: 566.9918 - val_mae: 11.9506\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.7931 - mse: 9.7931 - mae: 1.6226 - val_loss: 502.0276 - val_mse: 502.0276 - val_mae: 11.1773\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5388 - mse: 8.5388 - mae: 1.5027 - val_loss: 523.7596 - val_mse: 523.7596 - val_mae: 11.5812\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7925 - mse: 8.7925 - mae: 1.5690 - val_loss: 561.5011 - val_mse: 561.5011 - val_mae: 11.7889\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.1345 - mse: 10.1345 - mae: 1.7344 - val_loss: 569.4473 - val_mse: 569.4473 - val_mae: 12.0095\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0193 - mse: 10.0193 - mae: 1.8640 - val_loss: 495.2482 - val_mse: 495.2482 - val_mae: 11.1837\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.5645 - mse: 14.5645 - mae: 2.1075 - val_loss: 423.0996 - val_mse: 423.0996 - val_mae: 10.7262\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 19.6237 - mse: 19.6237 - mae: 2.4877 - val_loss: 703.1324 - val_mse: 703.1324 - val_mae: 13.2333\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 28.8031 - mse: 28.8031 - mae: 2.9682 - val_loss: 534.8992 - val_mse: 534.8992 - val_mae: 11.0437\n",
      "Epoch 335/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 17.5098 - mse: 17.5098 - mae: 2.3801 - val_loss: 534.9797 - val_mse: 534.9797 - val_mae: 11.4670\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.7943 - mse: 14.7943 - mae: 2.1990 - val_loss: 620.1072 - val_mse: 620.1072 - val_mae: 12.5645\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.9003 - mse: 14.9003 - mae: 2.5300 - val_loss: 441.4703 - val_mse: 441.4703 - val_mae: 10.6090\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0842 - mse: 14.0842 - mae: 2.5283 - val_loss: 588.4858 - val_mse: 588.4858 - val_mae: 12.5273\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.7204 - mse: 12.7204 - mae: 2.0744 - val_loss: 489.9358 - val_mse: 489.9358 - val_mae: 11.3869\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5109 - mse: 7.5109 - mae: 1.5739 - val_loss: 513.0196 - val_mse: 513.0196 - val_mae: 11.3126\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4546 - mse: 7.4546 - mae: 1.5962 - val_loss: 499.6498 - val_mse: 499.6498 - val_mae: 11.5794\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.4994 - mse: 8.4994 - mae: 1.6518 - val_loss: 508.5148 - val_mse: 508.5148 - val_mae: 11.1919\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7729 - mse: 7.7729 - mae: 1.5436 - val_loss: 606.0617 - val_mse: 606.0617 - val_mae: 12.1601\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6617 - mse: 11.6617 - mae: 1.9447 - val_loss: 477.8770 - val_mse: 477.8770 - val_mae: 11.0801\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.6915 - mse: 8.6915 - mae: 1.7208 - val_loss: 542.6288 - val_mse: 542.6288 - val_mae: 11.8285\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8314 - mse: 7.8314 - mae: 1.4850 - val_loss: 520.8264 - val_mse: 520.8264 - val_mae: 11.3131\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.3432 - mse: 7.3432 - mae: 1.4438 - val_loss: 493.2038 - val_mse: 493.2038 - val_mae: 11.0556\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9219 - mse: 8.9219 - mae: 1.7016 - val_loss: 610.0952 - val_mse: 610.0952 - val_mae: 12.2989\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3861 - mse: 10.3861 - mae: 1.9178 - val_loss: 457.1325 - val_mse: 457.1325 - val_mae: 10.8926\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3173 - mse: 13.3173 - mae: 2.0643 - val_loss: 541.5096 - val_mse: 541.5096 - val_mae: 11.4308\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 13.8498 - mse: 13.8498 - mae: 2.1898 - val_loss: 572.0825 - val_mse: 572.0825 - val_mae: 12.3252\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.8619 - mse: 24.8619 - mae: 2.9388 - val_loss: 433.6350 - val_mse: 433.6350 - val_mae: 10.8194\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.1853 - mse: 18.1853 - mae: 2.3550 - val_loss: 640.9010 - val_mse: 640.9010 - val_mae: 12.3230\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0852 - mse: 10.0852 - mae: 1.8503 - val_loss: 494.0855 - val_mse: 494.0855 - val_mae: 10.6608\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.6290 - mse: 14.6290 - mae: 2.1987 - val_loss: 543.2529 - val_mse: 543.2529 - val_mae: 11.4557\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3649 - mse: 10.3649 - mae: 1.8426 - val_loss: 560.5717 - val_mse: 560.5717 - val_mae: 11.7935\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3626 - mse: 9.3626 - mae: 1.7496 - val_loss: 500.1777 - val_mse: 500.1777 - val_mae: 11.1248\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.9033 - mse: 6.9033 - mae: 1.5698 - val_loss: 522.4410 - val_mse: 522.4410 - val_mae: 11.3993\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8778 - mse: 5.8778 - mae: 1.2958 - val_loss: 538.5628 - val_mse: 538.5628 - val_mae: 11.5254\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4124 - mse: 6.4124 - mae: 1.3691 - val_loss: 509.8899 - val_mse: 509.8899 - val_mae: 11.2979\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3327 - mse: 6.3327 - mae: 1.3529 - val_loss: 488.0296 - val_mse: 488.0296 - val_mae: 11.0484\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7231 - mse: 5.7231 - mae: 1.2918 - val_loss: 559.8010 - val_mse: 559.8010 - val_mae: 11.7299\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0641 - mse: 7.0641 - mae: 1.4009 - val_loss: 453.5060 - val_mse: 453.5060 - val_mae: 10.8635\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.6032 - mse: 8.6032 - mae: 1.5491 - val_loss: 549.3942 - val_mse: 549.3942 - val_mae: 11.7994\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5302 - mse: 6.5302 - mae: 1.4284 - val_loss: 499.2200 - val_mse: 499.2200 - val_mae: 11.0310\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5956 - mse: 5.5956 - mae: 1.2548 - val_loss: 521.0315 - val_mse: 521.0315 - val_mae: 11.3287\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7555 - mse: 5.7555 - mae: 1.3682 - val_loss: 557.3499 - val_mse: 557.3499 - val_mae: 11.7377\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9425 - mse: 5.9425 - mae: 1.2751 - val_loss: 483.1582 - val_mse: 483.1582 - val_mae: 11.1066\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0318 - mse: 7.0318 - mae: 1.4826 - val_loss: 527.9855 - val_mse: 527.9855 - val_mae: 11.3987\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6383 - mse: 6.6383 - mae: 1.3627 - val_loss: 562.7308 - val_mse: 562.7308 - val_mae: 11.7367\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1379 - mse: 7.1379 - mae: 1.5502 - val_loss: 528.9389 - val_mse: 528.9389 - val_mae: 11.3632\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7941 - mse: 6.7941 - mae: 1.4226 - val_loss: 512.1243 - val_mse: 512.1243 - val_mae: 11.0633\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7772 - mse: 6.7772 - mae: 1.4630 - val_loss: 554.1575 - val_mse: 554.1575 - val_mae: 11.4382\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4315 - mse: 5.4315 - mae: 1.2532 - val_loss: 519.8354 - val_mse: 519.8354 - val_mae: 11.2955\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8112 - mse: 4.8112 - mae: 1.2046 - val_loss: 508.1742 - val_mse: 508.1742 - val_mae: 11.1130\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3218 - mse: 5.3218 - mae: 1.1713 - val_loss: 544.6120 - val_mse: 544.6120 - val_mae: 11.6976\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.3297 - mse: 5.3297 - mae: 1.2822 - val_loss: 511.4699 - val_mse: 511.4699 - val_mae: 11.2110\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7500 - mse: 5.7500 - mae: 1.2651 - val_loss: 525.6299 - val_mse: 525.6299 - val_mae: 11.5041\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6370 - mse: 4.6370 - mae: 1.1441 - val_loss: 545.4154 - val_mse: 545.4154 - val_mae: 11.5155\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7309 - mse: 4.7309 - mae: 1.1745 - val_loss: 522.5366 - val_mse: 522.5366 - val_mae: 11.3801\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.8609 - mse: 4.8609 - mae: 1.1530 - val_loss: 525.4749 - val_mse: 525.4749 - val_mae: 11.2543\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5413 - mse: 4.5413 - mae: 1.1759 - val_loss: 538.5789 - val_mse: 538.5789 - val_mae: 11.4730\n",
      "Epoch 383/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7003 - mse: 4.7003 - mae: 1.1997 - val_loss: 488.1017 - val_mse: 488.1017 - val_mae: 10.9817\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5695 - mse: 6.5695 - mae: 1.5334 - val_loss: 571.6179 - val_mse: 571.6179 - val_mae: 12.0167\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3436 - mse: 7.3436 - mae: 1.5096 - val_loss: 573.5150 - val_mse: 573.5150 - val_mae: 11.7599\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5383 - mse: 7.5383 - mae: 1.6002 - val_loss: 502.8918 - val_mse: 502.8918 - val_mae: 11.3300\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0808 - mse: 5.0808 - mae: 1.2928 - val_loss: 500.1236 - val_mse: 500.1236 - val_mae: 11.3084\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4655 - mse: 4.4655 - mae: 1.0853 - val_loss: 519.2709 - val_mse: 519.2709 - val_mae: 11.3703\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5997 - mse: 4.5997 - mae: 1.0751 - val_loss: 530.5096 - val_mse: 530.5096 - val_mae: 11.4912\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9113 - mse: 3.9113 - mae: 1.0683 - val_loss: 528.5042 - val_mse: 528.5042 - val_mae: 11.4065\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0476 - mse: 4.0476 - mae: 1.0910 - val_loss: 537.2693 - val_mse: 537.2693 - val_mae: 11.3917\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9433 - mse: 3.9433 - mae: 1.0940 - val_loss: 521.3052 - val_mse: 521.3052 - val_mae: 11.2424\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0939 - mse: 4.0939 - mae: 1.0459 - val_loss: 556.7367 - val_mse: 556.7367 - val_mae: 11.5318\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0435 - mse: 4.0435 - mae: 1.0973 - val_loss: 529.7645 - val_mse: 529.7645 - val_mae: 11.4610\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8856 - mse: 3.8856 - mae: 1.0072 - val_loss: 521.1482 - val_mse: 521.1482 - val_mae: 11.1611\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7731 - mse: 3.7731 - mae: 1.0282 - val_loss: 537.1130 - val_mse: 537.1130 - val_mae: 11.4615\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6723 - mse: 3.6723 - mae: 1.0376 - val_loss: 548.7426 - val_mse: 548.7426 - val_mae: 11.5467\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1266 - mse: 4.1266 - mae: 1.0956 - val_loss: 527.5081 - val_mse: 527.5081 - val_mae: 11.3944\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4421 - mse: 3.4421 - mae: 0.9992 - val_loss: 535.6722 - val_mse: 535.6722 - val_mae: 11.4928\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5720 - mse: 3.5720 - mae: 0.9690 - val_loss: 514.1667 - val_mse: 514.1667 - val_mae: 11.2347\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5836 - mse: 3.5836 - mae: 0.9933 - val_loss: 510.7679 - val_mse: 510.7679 - val_mae: 11.2651\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5006 - mse: 3.5006 - mae: 0.9590 - val_loss: 550.7792 - val_mse: 550.7792 - val_mae: 11.6332\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7638 - mse: 4.7638 - mae: 1.1443 - val_loss: 558.8333 - val_mse: 558.8333 - val_mae: 11.6971\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5900 - mse: 5.5900 - mae: 1.2510 - val_loss: 493.9959 - val_mse: 493.9959 - val_mae: 11.0408\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5033 - mse: 6.5033 - mae: 1.4203 - val_loss: 527.5384 - val_mse: 527.5384 - val_mae: 11.4928\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.9659 - mse: 8.9659 - mae: 1.5155 - val_loss: 676.5133 - val_mse: 676.5133 - val_mae: 12.6157\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6429 - mse: 11.6429 - mae: 2.0072 - val_loss: 479.6003 - val_mse: 479.6003 - val_mae: 10.2094\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.4370 - mse: 40.4370 - mae: 3.3986 - val_loss: 556.2050 - val_mse: 556.2050 - val_mae: 10.7590\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 84.8690 - mse: 84.8690 - mae: 5.1335 - val_loss: 1470.3224 - val_mse: 1470.3224 - val_mae: 20.8675\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.5593 - mse: 239.5593 - mae: 9.1594 - val_loss: 569.5225 - val_mse: 569.5225 - val_mae: 11.6384\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 395.3405 - mse: 395.3405 - mae: 11.6281 - val_loss: 1455.4966 - val_mse: 1455.4966 - val_mae: 17.3420\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 388.9987 - mse: 388.9987 - mae: 12.0091 - val_loss: 356.1241 - val_mse: 356.1241 - val_mae: 11.2399\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 490.8679 - mse: 490.8679 - mae: 14.9416 - val_loss: 757.1978 - val_mse: 757.1978 - val_mae: 18.6719\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.2317 - mse: 264.2317 - mae: 9.7891 - val_loss: 400.5875 - val_mse: 400.5875 - val_mae: 13.5565\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 179.3284 - mse: 179.3284 - mae: 7.5961 - val_loss: 828.3959 - val_mse: 828.3959 - val_mae: 13.4869\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 177.5788 - mse: 177.5788 - mae: 7.4337 - val_loss: 543.6697 - val_mse: 543.6697 - val_mae: 11.1409\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 69.8371 - mse: 69.8371 - mae: 5.3043 - val_loss: 520.5739 - val_mse: 520.5739 - val_mae: 10.9622\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 53.3595 - mse: 53.3595 - mae: 4.9498 - val_loss: 644.1068 - val_mse: 644.1068 - val_mae: 12.2835\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.7646 - mse: 34.7646 - mae: 3.6370 - val_loss: 575.0483 - val_mse: 575.0483 - val_mae: 11.6227\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.8789 - mse: 29.8789 - mae: 3.7223 - val_loss: 669.1540 - val_mse: 669.1540 - val_mae: 12.1599\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.5712 - mse: 19.5712 - mae: 2.7540 - val_loss: 567.1649 - val_mse: 567.1649 - val_mae: 11.4592\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3805 - mse: 13.3805 - mae: 2.4392 - val_loss: 599.4418 - val_mse: 599.4418 - val_mae: 11.4002\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1294 - mse: 11.1294 - mae: 2.2739 - val_loss: 537.7408 - val_mse: 537.7408 - val_mae: 11.0502\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3531 - mse: 10.3531 - mae: 2.1044 - val_loss: 580.6738 - val_mse: 580.6738 - val_mae: 11.6960\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6044 - mse: 10.6044 - mae: 2.1185 - val_loss: 525.3509 - val_mse: 525.3509 - val_mae: 11.1347\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3462 - mse: 9.3462 - mae: 2.0605 - val_loss: 556.3861 - val_mse: 556.3861 - val_mae: 11.3787\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3747 - mse: 8.3747 - mae: 1.8511 - val_loss: 518.3453 - val_mse: 518.3453 - val_mae: 11.0834\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8168 - mse: 7.8168 - mae: 1.7990 - val_loss: 527.8953 - val_mse: 527.8953 - val_mae: 11.0630\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.4052 - mse: 8.4052 - mae: 1.8503 - val_loss: 541.6906 - val_mse: 541.6906 - val_mae: 11.2779\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1631 - mse: 7.1631 - mae: 1.6990 - val_loss: 525.6077 - val_mse: 525.6077 - val_mae: 10.8947\n",
      "Epoch 431/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8387 - mse: 7.8387 - mae: 1.6985 - val_loss: 539.1460 - val_mse: 539.1460 - val_mae: 11.2792\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5603 - mse: 8.5603 - mae: 1.7521 - val_loss: 504.4203 - val_mse: 504.4203 - val_mae: 11.0934\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.2796 - mse: 8.2796 - mae: 1.9037 - val_loss: 567.5349 - val_mse: 567.5349 - val_mae: 11.2744\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6975 - mse: 7.6975 - mae: 1.8213 - val_loss: 500.1105 - val_mse: 500.1105 - val_mae: 10.9570\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0637 - mse: 7.0637 - mae: 1.7203 - val_loss: 558.1948 - val_mse: 558.1948 - val_mae: 11.5867\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5900 - mse: 6.5900 - mae: 1.5591 - val_loss: 527.0440 - val_mse: 527.0440 - val_mae: 11.0365\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8762 - mse: 5.8762 - mae: 1.5461 - val_loss: 529.6030 - val_mse: 529.6030 - val_mae: 11.1793\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8171 - mse: 5.8171 - mae: 1.4998 - val_loss: 547.7958 - val_mse: 547.7958 - val_mae: 11.3986\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4504 - mse: 6.4504 - mae: 1.5788 - val_loss: 491.6785 - val_mse: 491.6785 - val_mae: 10.7891\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1258 - mse: 7.1258 - mae: 1.6262 - val_loss: 576.7624 - val_mse: 576.7624 - val_mae: 11.6081\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.1935 - mse: 7.1935 - mae: 1.6317 - val_loss: 488.6455 - val_mse: 488.6455 - val_mae: 10.6756\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7793 - mse: 6.7793 - mae: 1.5277 - val_loss: 553.6749 - val_mse: 553.6749 - val_mae: 11.6613\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5818 - mse: 5.5818 - mae: 1.4503 - val_loss: 512.5878 - val_mse: 512.5878 - val_mae: 10.9164\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7008 - mse: 5.7008 - mae: 1.4955 - val_loss: 518.6011 - val_mse: 518.6011 - val_mae: 10.9895\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.1914 - mse: 6.1914 - mae: 1.5045 - val_loss: 546.8070 - val_mse: 546.8070 - val_mae: 11.4193\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.0342 - mse: 7.0342 - mae: 1.5666 - val_loss: 479.3918 - val_mse: 479.3918 - val_mae: 10.6105\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0426 - mse: 7.0426 - mae: 1.6373 - val_loss: 565.1452 - val_mse: 565.1452 - val_mae: 11.6943\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5196 - mse: 5.5196 - mae: 1.4066 - val_loss: 495.5859 - val_mse: 495.5859 - val_mae: 11.0318\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2602 - mse: 6.2602 - mae: 1.4967 - val_loss: 562.5993 - val_mse: 562.5993 - val_mae: 11.4463\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6341 - mse: 5.6341 - mae: 1.4722 - val_loss: 473.1655 - val_mse: 473.1655 - val_mae: 10.6957\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.5604 - mse: 5.5604 - mae: 1.3920 - val_loss: 568.2365 - val_mse: 568.2365 - val_mae: 11.8441\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3126 - mse: 7.3126 - mae: 1.6753 - val_loss: 465.9636 - val_mse: 465.9636 - val_mae: 10.5906\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6477 - mse: 6.6477 - mae: 1.4920 - val_loss: 547.4586 - val_mse: 547.4586 - val_mae: 11.3256\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7117 - mse: 5.7117 - mae: 1.4155 - val_loss: 529.0068 - val_mse: 529.0068 - val_mae: 11.0250\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3747 - mse: 5.3747 - mae: 1.3023 - val_loss: 509.5486 - val_mse: 509.5486 - val_mae: 11.0767\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7270 - mse: 4.7270 - mae: 1.2545 - val_loss: 510.0797 - val_mse: 510.0797 - val_mae: 11.2191\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0977 - mse: 5.0977 - mae: 1.3724 - val_loss: 538.2060 - val_mse: 538.2060 - val_mae: 11.1665\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6769 - mse: 4.6769 - mae: 1.2761 - val_loss: 493.0326 - val_mse: 493.0326 - val_mae: 10.8388\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.1918 - mse: 5.1918 - mae: 1.3168 - val_loss: 562.4124 - val_mse: 562.4124 - val_mae: 11.3568\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5537 - mse: 4.5537 - mae: 1.1808 - val_loss: 508.7832 - val_mse: 508.7832 - val_mae: 10.9246\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2563 - mse: 4.2563 - mae: 1.1879 - val_loss: 539.3716 - val_mse: 539.3716 - val_mae: 11.4943\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3103 - mse: 4.3103 - mae: 1.2139 - val_loss: 510.0649 - val_mse: 510.0649 - val_mae: 10.9156\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.1489 - mse: 4.1489 - mae: 1.1779 - val_loss: 514.6199 - val_mse: 514.6199 - val_mae: 11.1048\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9964 - mse: 3.9964 - mae: 1.2078 - val_loss: 543.3592 - val_mse: 543.3592 - val_mae: 11.2812\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.3561 - mse: 4.3561 - mae: 1.1784 - val_loss: 513.1744 - val_mse: 513.1744 - val_mae: 10.8562\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.7885 - mse: 3.7885 - mae: 1.1088 - val_loss: 526.0790 - val_mse: 526.0790 - val_mae: 11.2321\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.7519 - mse: 3.7519 - mae: 1.1175 - val_loss: 533.4639 - val_mse: 533.4639 - val_mae: 11.2702\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3272 - mse: 4.3272 - mae: 1.1981 - val_loss: 497.9512 - val_mse: 497.9512 - val_mae: 10.7277\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.8715 - mse: 4.8715 - mae: 1.3005 - val_loss: 588.9439 - val_mse: 588.9439 - val_mae: 11.6198\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4512 - mse: 5.4512 - mae: 1.3637 - val_loss: 525.3146 - val_mse: 525.3146 - val_mae: 10.9798\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1452 - mse: 4.1452 - mae: 1.1816 - val_loss: 500.7051 - val_mse: 500.7051 - val_mae: 11.0857\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2712 - mse: 4.2712 - mae: 1.1847 - val_loss: 507.4487 - val_mse: 507.4487 - val_mae: 10.9794\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.3970 - mse: 3.3970 - mae: 1.0814 - val_loss: 514.1521 - val_mse: 514.1521 - val_mae: 11.0575\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5591 - mse: 3.5591 - mae: 1.0726 - val_loss: 528.7848 - val_mse: 528.7848 - val_mae: 11.2602\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7432 - mse: 3.7432 - mae: 1.0741 - val_loss: 498.2947 - val_mse: 498.2947 - val_mae: 10.8124\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7747 - mse: 3.7747 - mae: 1.1598 - val_loss: 528.4029 - val_mse: 528.4029 - val_mae: 11.1887\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7161 - mse: 3.7161 - mae: 1.1108 - val_loss: 538.1367 - val_mse: 538.1367 - val_mae: 11.1282\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1720 - mse: 3.1720 - mae: 1.0249 - val_loss: 518.1075 - val_mse: 518.1075 - val_mae: 11.0268\n",
      "Epoch 479/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0213 - mse: 3.0213 - mae: 0.9674 - val_loss: 538.7006 - val_mse: 538.7006 - val_mae: 11.2036\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2774 - mse: 3.2774 - mae: 1.0165 - val_loss: 525.2826 - val_mse: 525.2826 - val_mae: 11.0274\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8964 - mse: 2.8964 - mae: 0.9618 - val_loss: 512.7863 - val_mse: 512.7863 - val_mae: 11.0381\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2132 - mse: 3.2132 - mae: 1.0266 - val_loss: 541.8201 - val_mse: 541.8201 - val_mae: 11.2400\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3556 - mse: 3.3556 - mae: 0.9754 - val_loss: 527.9493 - val_mse: 527.9493 - val_mae: 11.0956\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6424 - mse: 2.6424 - mae: 0.9019 - val_loss: 535.8837 - val_mse: 535.8837 - val_mae: 11.2603\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6533 - mse: 2.6533 - mae: 0.9202 - val_loss: 515.6393 - val_mse: 515.6393 - val_mae: 10.9995\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.8408 - mse: 2.8408 - mae: 0.9716 - val_loss: 535.7739 - val_mse: 535.7739 - val_mae: 11.1927\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3967 - mse: 2.3967 - mae: 0.8603 - val_loss: 512.7836 - val_mse: 512.7836 - val_mae: 10.8770\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6371 - mse: 2.6371 - mae: 0.9080 - val_loss: 537.3937 - val_mse: 537.3937 - val_mae: 11.2274\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4256 - mse: 2.4256 - mae: 0.8732 - val_loss: 513.1863 - val_mse: 513.1863 - val_mae: 10.9124\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6655 - mse: 2.6655 - mae: 0.9179 - val_loss: 538.8383 - val_mse: 538.8383 - val_mae: 11.2159\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7072 - mse: 2.7072 - mae: 0.9007 - val_loss: 540.1050 - val_mse: 540.1050 - val_mae: 11.1494\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8777 - mse: 2.8777 - mae: 0.9418 - val_loss: 490.1774 - val_mse: 490.1774 - val_mae: 10.8289\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4486 - mse: 4.4486 - mae: 1.1592 - val_loss: 543.0106 - val_mse: 543.0106 - val_mae: 11.1259\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3833 - mse: 3.3833 - mae: 1.0622 - val_loss: 539.0155 - val_mse: 539.0155 - val_mae: 11.1879\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9857 - mse: 2.9857 - mae: 0.9973 - val_loss: 531.3224 - val_mse: 531.3224 - val_mae: 11.1505\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3910 - mse: 2.3910 - mae: 0.8566 - val_loss: 532.4586 - val_mse: 532.4586 - val_mae: 11.1073\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3922 - mse: 2.3922 - mae: 0.8832 - val_loss: 546.5573 - val_mse: 546.5573 - val_mae: 11.2204\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6556 - mse: 2.6556 - mae: 0.8864 - val_loss: 518.2867 - val_mse: 518.2867 - val_mae: 10.9977\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2549 - mse: 2.2549 - mae: 0.8509 - val_loss: 546.1203 - val_mse: 546.1203 - val_mae: 11.2766\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.6310 - mse: 2.6310 - mae: 0.9729 - val_loss: 513.0348 - val_mse: 513.0348 - val_mae: 10.9801\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7162 - mse: 2.7162 - mae: 1.0061 - val_loss: 545.8600 - val_mse: 545.8600 - val_mae: 11.2888\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0008 - mse: 3.0008 - mae: 0.9633 - val_loss: 565.6282 - val_mse: 565.6282 - val_mae: 11.2867\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0442 - mse: 3.0442 - mae: 0.9925 - val_loss: 470.4015 - val_mse: 470.4015 - val_mae: 10.5590\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.5343 - mse: 4.5343 - mae: 1.2054 - val_loss: 627.1041 - val_mse: 627.1041 - val_mae: 11.9950\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.0893 - mse: 9.0893 - mae: 1.7157 - val_loss: 415.3241 - val_mse: 415.3241 - val_mae: 9.9045\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.9583 - mse: 14.9583 - mae: 2.1822 - val_loss: 665.6166 - val_mse: 665.6166 - val_mae: 12.1180\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 18.6268 - mse: 18.6268 - mae: 2.4628 - val_loss: 604.7601 - val_mse: 604.7601 - val_mae: 11.4036\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.6500 - mse: 8.6500 - mae: 1.6545 - val_loss: 497.1173 - val_mse: 497.1173 - val_mae: 10.7734\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.7520 - mse: 6.7520 - mae: 1.4347 - val_loss: 626.7131 - val_mse: 626.7131 - val_mae: 11.9148\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7387 - mse: 6.7387 - mae: 1.4984 - val_loss: 496.8445 - val_mse: 496.8445 - val_mae: 10.6401\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.1480 - mse: 9.1480 - mae: 1.5366 - val_loss: 564.5163 - val_mse: 564.5163 - val_mae: 11.0374\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9183 - mse: 5.9183 - mae: 1.4423 - val_loss: 599.1912 - val_mse: 599.1912 - val_mae: 11.8606\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7575 - mse: 8.7575 - mae: 1.7536 - val_loss: 444.9451 - val_mse: 444.9451 - val_mae: 10.0091\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.3801 - mse: 19.3801 - mae: 2.5580 - val_loss: 738.6783 - val_mse: 738.6783 - val_mae: 13.1723\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.3544 - mse: 27.3544 - mae: 2.5357 - val_loss: 402.9608 - val_mse: 402.9608 - val_mae: 10.0935\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.5668 - mse: 35.5668 - mae: 3.1161 - val_loss: 463.9754 - val_mse: 463.9754 - val_mae: 10.6957\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 30.2091 - mse: 30.2091 - mae: 2.9789 - val_loss: 594.6473 - val_mse: 594.6473 - val_mae: 12.8112\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 38.7884 - mse: 38.7884 - mae: 3.8341 - val_loss: 395.5169 - val_mse: 395.5169 - val_mae: 9.7102\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.9580 - mse: 26.9580 - mae: 3.3216 - val_loss: 778.5950 - val_mse: 778.5950 - val_mae: 13.2992\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.9820 - mse: 33.9820 - mae: 3.1285 - val_loss: 384.1183 - val_mse: 384.1183 - val_mae: 9.7672\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.4974 - mse: 15.4974 - mae: 2.2083 - val_loss: 727.3994 - val_mse: 727.3994 - val_mae: 12.4258\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.3928 - mse: 13.3928 - mae: 2.4918 - val_loss: 576.2405 - val_mse: 576.2405 - val_mae: 10.5136\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5862 - mse: 10.5862 - mae: 2.0024 - val_loss: 574.6630 - val_mse: 574.6630 - val_mae: 11.4694\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.1825 - mse: 6.1825 - mae: 1.5395 - val_loss: 618.2497 - val_mse: 618.2497 - val_mae: 11.5427\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.7318 - mse: 7.7318 - mae: 1.7757 - val_loss: 516.6042 - val_mse: 516.6042 - val_mae: 10.7277\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1659 - mse: 4.1659 - mae: 1.3892 - val_loss: 563.9311 - val_mse: 563.9311 - val_mae: 11.2241\n",
      "Epoch 527/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5381 - mse: 2.5381 - mae: 1.0061 - val_loss: 552.9592 - val_mse: 552.9592 - val_mae: 11.0479\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6974 - mse: 2.6974 - mae: 0.9806 - val_loss: 539.8049 - val_mse: 539.8049 - val_mae: 11.1116\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4972 - mse: 2.4972 - mae: 0.9595 - val_loss: 564.6247 - val_mse: 564.6247 - val_mae: 11.1142\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3628 - mse: 2.3628 - mae: 0.9998 - val_loss: 525.3503 - val_mse: 525.3503 - val_mae: 10.8407\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1396 - mse: 2.1396 - mae: 0.9441 - val_loss: 557.6505 - val_mse: 557.6505 - val_mae: 11.4299\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1528 - mse: 2.1528 - mae: 0.8459 - val_loss: 531.0343 - val_mse: 531.0343 - val_mae: 10.8778\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.0356 - mse: 2.0356 - mae: 0.8687 - val_loss: 554.5017 - val_mse: 554.5017 - val_mae: 11.3050\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8822 - mse: 1.8822 - mae: 0.8146 - val_loss: 527.5939 - val_mse: 527.5939 - val_mae: 11.0349\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8779 - mse: 1.8779 - mae: 0.7741 - val_loss: 554.1143 - val_mse: 554.1143 - val_mae: 11.1478\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7936 - mse: 1.7936 - mae: 0.7454 - val_loss: 539.4770 - val_mse: 539.4770 - val_mae: 11.1286\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8000 - mse: 1.8000 - mae: 0.7502 - val_loss: 551.3641 - val_mse: 551.3641 - val_mae: 11.1111\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6748 - mse: 1.6748 - mae: 0.7490 - val_loss: 543.5945 - val_mse: 543.5945 - val_mae: 11.0734\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7464 - mse: 1.7464 - mae: 0.7156 - val_loss: 534.7156 - val_mse: 534.7156 - val_mae: 11.1086\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6422 - mse: 1.6422 - mae: 0.6893 - val_loss: 550.6403 - val_mse: 550.6403 - val_mae: 11.1404\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7451 - mse: 1.7451 - mae: 0.7176 - val_loss: 541.3978 - val_mse: 541.3978 - val_mae: 11.1330\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5307 - mse: 1.5307 - mae: 0.6767 - val_loss: 553.9783 - val_mse: 553.9783 - val_mae: 11.1466\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8479 - mse: 1.8479 - mae: 0.7409 - val_loss: 546.7819 - val_mse: 546.7819 - val_mae: 11.1346\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6209 - mse: 1.6209 - mae: 0.7177 - val_loss: 535.5898 - val_mse: 535.5898 - val_mae: 11.1390\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5522 - mse: 1.5522 - mae: 0.6953 - val_loss: 544.1354 - val_mse: 544.1354 - val_mae: 11.0066\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5253 - mse: 1.5253 - mae: 0.7021 - val_loss: 563.9789 - val_mse: 563.9789 - val_mae: 11.3354\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7280 - mse: 1.7280 - mae: 0.7563 - val_loss: 541.2770 - val_mse: 541.2770 - val_mae: 11.1419\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5897 - mse: 1.5897 - mae: 0.7215 - val_loss: 544.6721 - val_mse: 544.6721 - val_mae: 11.0538\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4985 - mse: 1.4985 - mae: 0.6649 - val_loss: 547.0536 - val_mse: 547.0536 - val_mae: 11.1839\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5142 - mse: 1.5142 - mae: 0.6806 - val_loss: 545.0911 - val_mse: 545.0911 - val_mae: 11.0893\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4572 - mse: 1.4572 - mae: 0.6345 - val_loss: 540.9628 - val_mse: 540.9628 - val_mae: 11.0932\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3630 - mse: 1.3630 - mae: 0.6344 - val_loss: 538.8687 - val_mse: 538.8687 - val_mae: 11.0467\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3355 - mse: 1.3355 - mae: 0.5940 - val_loss: 541.7613 - val_mse: 541.7613 - val_mae: 11.0568\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3701 - mse: 1.3701 - mae: 0.6247 - val_loss: 533.3724 - val_mse: 533.3724 - val_mae: 11.0258\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4124 - mse: 1.4124 - mae: 0.6678 - val_loss: 557.7395 - val_mse: 557.7395 - val_mae: 11.1802\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5980 - mse: 1.5980 - mae: 0.7084 - val_loss: 556.9991 - val_mse: 556.9991 - val_mae: 11.1703\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3574 - mse: 1.3574 - mae: 0.6536 - val_loss: 522.4655 - val_mse: 522.4655 - val_mae: 10.8964\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0495 - mse: 2.0495 - mae: 0.7769 - val_loss: 575.3728 - val_mse: 575.3728 - val_mae: 11.1272\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9554 - mse: 1.9554 - mae: 0.7953 - val_loss: 533.2217 - val_mse: 533.2217 - val_mae: 11.0860\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5576 - mse: 1.5576 - mae: 0.7508 - val_loss: 534.1068 - val_mse: 534.1068 - val_mae: 11.0617\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5008 - mse: 1.5008 - mae: 0.7208 - val_loss: 570.5499 - val_mse: 570.5499 - val_mae: 11.2895\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9814 - mse: 1.9814 - mae: 0.7809 - val_loss: 516.7784 - val_mse: 516.7784 - val_mae: 10.8010\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9245 - mse: 1.9245 - mae: 0.8582 - val_loss: 554.7706 - val_mse: 554.7706 - val_mae: 11.1777\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.4462 - mse: 1.4462 - mae: 0.7122 - val_loss: 545.7330 - val_mse: 545.7330 - val_mae: 11.0272\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5464 - mse: 1.5464 - mae: 0.6520 - val_loss: 529.3082 - val_mse: 529.3082 - val_mae: 10.9969\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2810 - mse: 1.2810 - mae: 0.6297 - val_loss: 555.5309 - val_mse: 555.5309 - val_mae: 11.1568\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4633 - mse: 1.4633 - mae: 0.6369 - val_loss: 529.0093 - val_mse: 529.0093 - val_mae: 11.0118\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2786 - mse: 1.2786 - mae: 0.6268 - val_loss: 540.0831 - val_mse: 540.0831 - val_mae: 11.0430\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3386 - mse: 1.3386 - mae: 0.6124 - val_loss: 550.1378 - val_mse: 550.1378 - val_mae: 11.2207\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1766 - mse: 1.1766 - mae: 0.6093 - val_loss: 535.3248 - val_mse: 535.3248 - val_mae: 10.9674\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2556 - mse: 1.2556 - mae: 0.6161 - val_loss: 545.4828 - val_mse: 545.4828 - val_mae: 11.0763\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4044 - mse: 1.4044 - mae: 0.6345 - val_loss: 562.0956 - val_mse: 562.0956 - val_mae: 11.2545\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4404 - mse: 1.4404 - mae: 0.7156 - val_loss: 523.5897 - val_mse: 523.5897 - val_mae: 10.9058\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6322 - mse: 1.6322 - mae: 0.7114 - val_loss: 526.9887 - val_mse: 526.9887 - val_mae: 10.9897\n",
      "Epoch 575/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4285 - mse: 1.4285 - mae: 0.6638 - val_loss: 544.8019 - val_mse: 544.8019 - val_mae: 11.0791\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1790 - mse: 1.1790 - mae: 0.6014 - val_loss: 544.4825 - val_mse: 544.4825 - val_mae: 11.1048\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1984 - mse: 1.1984 - mae: 0.6062 - val_loss: 526.1607 - val_mse: 526.1607 - val_mae: 10.9122\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2267 - mse: 1.2267 - mae: 0.6575 - val_loss: 571.9437 - val_mse: 571.9437 - val_mae: 11.4303\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.2021 - mse: 2.2021 - mae: 0.8333 - val_loss: 533.1843 - val_mse: 533.1843 - val_mae: 10.9953\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4330 - mse: 1.4330 - mae: 0.6918 - val_loss: 511.2894 - val_mse: 511.2894 - val_mae: 10.8770\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5279 - mse: 1.5279 - mae: 0.7163 - val_loss: 561.4164 - val_mse: 561.4164 - val_mae: 11.4869\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7300 - mse: 1.7300 - mae: 0.8044 - val_loss: 493.9910 - val_mse: 493.9910 - val_mae: 10.5464\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0951 - mse: 2.0951 - mae: 0.9496 - val_loss: 577.8420 - val_mse: 577.8420 - val_mae: 11.5035\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9142 - mse: 1.9142 - mae: 0.8641 - val_loss: 544.2891 - val_mse: 544.2891 - val_mae: 10.9144\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5488 - mse: 1.5488 - mae: 0.6898 - val_loss: 545.2424 - val_mse: 545.2424 - val_mae: 11.1819\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8288 - mse: 1.8288 - mae: 0.7341 - val_loss: 585.1366 - val_mse: 585.1366 - val_mae: 11.4666\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1259 - mse: 3.1259 - mae: 1.0355 - val_loss: 488.1812 - val_mse: 488.1812 - val_mae: 10.4925\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7840 - mse: 3.7840 - mae: 1.2488 - val_loss: 550.9392 - val_mse: 550.9392 - val_mae: 11.4162\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7588 - mse: 2.7588 - mae: 1.0375 - val_loss: 561.9130 - val_mse: 561.9130 - val_mae: 11.1485\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8747 - mse: 1.8747 - mae: 0.8151 - val_loss: 522.2770 - val_mse: 522.2770 - val_mae: 11.1401\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2324 - mse: 1.2324 - mae: 0.6616 - val_loss: 544.1404 - val_mse: 544.1404 - val_mae: 11.1424\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3059 - mse: 1.3059 - mae: 0.6807 - val_loss: 537.4553 - val_mse: 537.4553 - val_mae: 11.1285\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5056 - mse: 1.5056 - mae: 0.7270 - val_loss: 499.3689 - val_mse: 499.3689 - val_mae: 10.6494\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0481 - mse: 2.0481 - mae: 0.8100 - val_loss: 551.9449 - val_mse: 551.9449 - val_mae: 11.2406\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2498 - mse: 3.2498 - mae: 0.9616 - val_loss: 540.1904 - val_mse: 540.1904 - val_mae: 11.4166\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6236 - mse: 6.6236 - mae: 1.4966 - val_loss: 429.6324 - val_mse: 429.6324 - val_mae: 9.9188\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.2158 - mse: 9.2158 - mae: 1.8339 - val_loss: 579.4825 - val_mse: 579.4825 - val_mae: 11.8674\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0854 - mse: 10.0854 - mae: 1.7173 - val_loss: 609.0988 - val_mse: 609.0988 - val_mae: 11.8027\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7071 - mse: 4.7071 - mae: 1.2271 - val_loss: 494.3340 - val_mse: 494.3340 - val_mae: 10.9567\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1888 - mse: 2.1888 - mae: 0.9706 - val_loss: 523.3113 - val_mse: 523.3113 - val_mae: 10.8068\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6975 - mse: 1.6975 - mae: 0.7996 - val_loss: 510.5027 - val_mse: 510.5027 - val_mae: 10.9876\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7225 - mse: 1.7225 - mae: 0.7586 - val_loss: 526.2933 - val_mse: 526.2933 - val_mae: 11.0415\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4101 - mse: 1.4101 - mae: 0.6618 - val_loss: 516.8515 - val_mse: 516.8515 - val_mae: 10.8841\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0820 - mse: 1.0820 - mae: 0.5853 - val_loss: 523.4285 - val_mse: 523.4285 - val_mae: 11.0180\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2461 - mse: 1.2461 - mae: 0.6202 - val_loss: 505.4627 - val_mse: 505.4627 - val_mae: 10.8177\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2113 - mse: 1.2113 - mae: 0.6131 - val_loss: 507.7166 - val_mse: 507.7166 - val_mae: 10.7764\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0976 - mse: 1.0976 - mae: 0.5776 - val_loss: 530.9311 - val_mse: 530.9311 - val_mae: 11.0708\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9132 - mse: 0.9132 - mae: 0.5073 - val_loss: 513.1973 - val_mse: 513.1973 - val_mae: 10.8396\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8854 - mse: 0.8854 - mae: 0.5098 - val_loss: 539.4175 - val_mse: 539.4175 - val_mae: 11.1243\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1805 - mse: 1.1805 - mae: 0.5868 - val_loss: 533.4604 - val_mse: 533.4604 - val_mae: 11.0656\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6196 - mse: 1.6196 - mae: 0.6983 - val_loss: 505.9503 - val_mse: 505.9503 - val_mae: 10.7875\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6604 - mse: 1.6604 - mae: 0.6717 - val_loss: 514.2506 - val_mse: 514.2506 - val_mae: 10.8157\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4740 - mse: 1.4740 - mae: 0.7075 - val_loss: 551.2253 - val_mse: 551.2253 - val_mae: 11.2585\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5026 - mse: 1.5026 - mae: 0.7038 - val_loss: 549.7900 - val_mse: 549.7900 - val_mae: 11.0821\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8931 - mse: 1.8931 - mae: 0.7180 - val_loss: 505.4786 - val_mse: 505.4786 - val_mae: 10.7707\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8082 - mse: 1.8082 - mae: 0.8407 - val_loss: 538.0190 - val_mse: 538.0190 - val_mae: 11.0855\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4930 - mse: 1.4930 - mae: 0.8355 - val_loss: 534.1682 - val_mse: 534.1682 - val_mae: 11.0658\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0480 - mse: 1.0480 - mae: 0.6507 - val_loss: 540.2003 - val_mse: 540.2003 - val_mae: 11.1991\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0939 - mse: 1.0939 - mae: 0.6283 - val_loss: 497.1979 - val_mse: 497.1979 - val_mae: 10.6278\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6715 - mse: 1.6715 - mae: 0.7399 - val_loss: 521.6965 - val_mse: 521.6965 - val_mae: 11.2110\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0923 - mse: 1.0923 - mae: 0.6543 - val_loss: 536.2649 - val_mse: 536.2649 - val_mae: 10.9357\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2356 - mse: 2.2356 - mae: 0.7875 - val_loss: 502.0984 - val_mse: 502.0984 - val_mae: 10.8096\n",
      "Epoch 623/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5700 - mse: 1.5700 - mae: 0.7519 - val_loss: 525.7310 - val_mse: 525.7310 - val_mae: 11.0107\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6860 - mse: 1.6860 - mae: 0.7735 - val_loss: 575.8679 - val_mse: 575.8679 - val_mae: 11.3131\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8685 - mse: 1.8685 - mae: 0.8332 - val_loss: 546.5325 - val_mse: 546.5325 - val_mae: 10.9697\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6028 - mse: 1.6028 - mae: 0.7884 - val_loss: 517.7276 - val_mse: 517.7276 - val_mae: 10.9159\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6803 - mse: 1.6803 - mae: 0.7625 - val_loss: 539.4909 - val_mse: 539.4909 - val_mae: 10.9325\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9413 - mse: 1.9413 - mae: 0.9038 - val_loss: 554.5341 - val_mse: 554.5341 - val_mae: 11.3016\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3105 - mse: 1.3105 - mae: 0.7573 - val_loss: 499.8445 - val_mse: 499.8445 - val_mae: 10.8324\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5112 - mse: 1.5112 - mae: 0.7249 - val_loss: 514.2695 - val_mse: 514.2695 - val_mae: 10.7426\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9800 - mse: 0.9800 - mae: 0.6064 - val_loss: 541.5334 - val_mse: 541.5334 - val_mae: 11.1079\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0168 - mse: 1.0168 - mae: 0.5823 - val_loss: 522.7526 - val_mse: 522.7526 - val_mae: 11.0213\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0618 - mse: 1.0618 - mae: 0.5656 - val_loss: 516.9484 - val_mse: 516.9484 - val_mae: 10.8697\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9972 - mse: 0.9972 - mae: 0.5805 - val_loss: 550.1014 - val_mse: 550.1014 - val_mae: 11.1554\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2378 - mse: 1.2378 - mae: 0.6142 - val_loss: 546.2869 - val_mse: 546.2869 - val_mae: 11.1017\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9642 - mse: 0.9642 - mae: 0.5589 - val_loss: 508.3681 - val_mse: 508.3681 - val_mae: 10.8552\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0820 - mse: 1.0820 - mae: 0.5619 - val_loss: 523.5870 - val_mse: 523.5870 - val_mae: 10.9135\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8465 - mse: 0.8465 - mae: 0.5223 - val_loss: 539.7747 - val_mse: 539.7747 - val_mae: 11.0793\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7396 - mse: 0.7396 - mae: 0.4570 - val_loss: 521.3588 - val_mse: 521.3588 - val_mae: 10.8867\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7809 - mse: 0.7809 - mae: 0.4828 - val_loss: 532.5261 - val_mse: 532.5261 - val_mae: 11.0230\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7186 - mse: 0.7186 - mae: 0.4268 - val_loss: 528.5787 - val_mse: 528.5787 - val_mae: 10.9655\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7909 - mse: 0.7909 - mae: 0.4637 - val_loss: 517.2714 - val_mse: 517.2714 - val_mae: 10.9089\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8456 - mse: 0.8456 - mae: 0.4940 - val_loss: 511.5698 - val_mse: 511.5698 - val_mae: 10.8253\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8148 - mse: 0.8148 - mae: 0.5249 - val_loss: 546.2502 - val_mse: 546.2502 - val_mae: 11.1245\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0074 - mse: 1.0074 - mae: 0.5131 - val_loss: 526.0386 - val_mse: 526.0386 - val_mae: 11.0029\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9107 - mse: 0.9107 - mae: 0.4946 - val_loss: 516.9050 - val_mse: 516.9050 - val_mae: 10.8980\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6685 - mse: 0.6685 - mae: 0.4296 - val_loss: 520.3361 - val_mse: 520.3361 - val_mae: 10.9467\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8143 - mse: 0.8143 - mae: 0.4558 - val_loss: 525.1327 - val_mse: 525.1327 - val_mae: 10.9164\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6722 - mse: 0.6722 - mae: 0.4217 - val_loss: 517.6207 - val_mse: 517.6207 - val_mae: 10.9352\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6810 - mse: 0.6810 - mae: 0.4201 - val_loss: 516.3549 - val_mse: 516.3549 - val_mae: 10.8494\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7004 - mse: 0.7004 - mae: 0.4202 - val_loss: 531.5018 - val_mse: 531.5018 - val_mae: 11.0200\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8050 - mse: 0.8050 - mae: 0.4411 - val_loss: 546.0599 - val_mse: 546.0599 - val_mae: 11.1507\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0007 - mse: 1.0007 - mae: 0.5465 - val_loss: 516.4449 - val_mse: 516.4449 - val_mae: 10.8109\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9655 - mse: 0.9655 - mae: 0.5120 - val_loss: 514.8718 - val_mse: 514.8718 - val_mae: 10.8231\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9835 - mse: 0.9835 - mae: 0.5626 - val_loss: 557.4125 - val_mse: 557.4125 - val_mae: 11.0940\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8386 - mse: 1.8386 - mae: 0.7937 - val_loss: 516.9182 - val_mse: 516.9182 - val_mae: 10.8949\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9591 - mse: 0.9591 - mae: 0.6627 - val_loss: 520.2636 - val_mse: 520.2636 - val_mae: 10.8473\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9911 - mse: 0.9911 - mae: 0.6267 - val_loss: 539.0893 - val_mse: 539.0893 - val_mae: 10.9823\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1260 - mse: 1.1260 - mae: 0.5925 - val_loss: 535.3127 - val_mse: 535.3127 - val_mae: 11.0400\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9460 - mse: 0.9460 - mae: 0.5533 - val_loss: 521.6740 - val_mse: 521.6740 - val_mae: 10.8381\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1570 - mse: 1.1570 - mae: 0.5657 - val_loss: 514.6376 - val_mse: 514.6376 - val_mae: 10.7617\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6635 - mse: 1.6635 - mae: 0.6925 - val_loss: 534.0475 - val_mse: 534.0475 - val_mae: 10.8938\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7218 - mse: 1.7218 - mae: 0.7474 - val_loss: 575.9509 - val_mse: 575.9509 - val_mae: 11.5352\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5203 - mse: 2.5203 - mae: 0.9388 - val_loss: 496.0352 - val_mse: 496.0352 - val_mae: 10.6804\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3837 - mse: 2.3837 - mae: 0.9554 - val_loss: 486.3271 - val_mse: 486.3271 - val_mae: 10.6063\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2218 - mse: 4.2218 - mae: 1.2462 - val_loss: 553.4433 - val_mse: 553.4433 - val_mae: 11.4325\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.2322 - mse: 5.2322 - mae: 1.2026 - val_loss: 619.2233 - val_mse: 619.2233 - val_mae: 12.0171\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.3078 - mse: 18.3078 - mae: 2.3430 - val_loss: 423.0907 - val_mse: 423.0907 - val_mae: 9.6921\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.3274 - mse: 23.3274 - mae: 2.5349 - val_loss: 484.6878 - val_mse: 484.6878 - val_mae: 9.9368\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0609 - mse: 6.0609 - mae: 1.6083 - val_loss: 658.6071 - val_mse: 658.6071 - val_mae: 12.8238\n",
      "Epoch 671/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5393 - mse: 8.5393 - mae: 1.9179 - val_loss: 519.9469 - val_mse: 519.9469 - val_mae: 10.6664\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0803 - mse: 6.0803 - mae: 1.4576 - val_loss: 438.7997 - val_mse: 438.7997 - val_mae: 10.1669\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1817 - mse: 7.1817 - mae: 1.3796 - val_loss: 568.0469 - val_mse: 568.0469 - val_mae: 11.0543\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3080 - mse: 2.3080 - mae: 1.0702 - val_loss: 601.4932 - val_mse: 601.4932 - val_mae: 11.5929\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0090 - mse: 3.0090 - mae: 0.9720 - val_loss: 541.9835 - val_mse: 541.9835 - val_mae: 11.0068\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2775 - mse: 1.2775 - mae: 0.6753 - val_loss: 515.7598 - val_mse: 515.7598 - val_mae: 10.7468\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2363 - mse: 1.2363 - mae: 0.6723 - val_loss: 540.6653 - val_mse: 540.6653 - val_mae: 11.0236\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6216 - mse: 1.6216 - mae: 0.7050 - val_loss: 521.9817 - val_mse: 521.9817 - val_mae: 11.0339\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0555 - mse: 1.0555 - mae: 0.6382 - val_loss: 511.2898 - val_mse: 511.2898 - val_mae: 10.8324\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1256 - mse: 1.1256 - mae: 0.6090 - val_loss: 539.0015 - val_mse: 539.0015 - val_mae: 11.0979\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9376 - mse: 0.9376 - mae: 0.5318 - val_loss: 518.3285 - val_mse: 518.3285 - val_mae: 10.8658\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7615 - mse: 0.7615 - mae: 0.4787 - val_loss: 510.9853 - val_mse: 510.9853 - val_mae: 10.7785\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9691 - mse: 0.9691 - mae: 0.5627 - val_loss: 517.5360 - val_mse: 517.5360 - val_mae: 10.7520\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0762 - mse: 1.0762 - mae: 0.6141 - val_loss: 559.4727 - val_mse: 559.4727 - val_mae: 11.3270\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4103 - mse: 1.4103 - mae: 0.5988 - val_loss: 530.3596 - val_mse: 530.3596 - val_mae: 11.0546\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5155 - mse: 1.5155 - mae: 0.6606 - val_loss: 502.3823 - val_mse: 502.3823 - val_mae: 10.7326\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9779 - mse: 1.9779 - mae: 0.7475 - val_loss: 486.9174 - val_mse: 486.9174 - val_mae: 10.5350\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4583 - mse: 3.4583 - mae: 0.8609 - val_loss: 509.7611 - val_mse: 509.7611 - val_mae: 10.5486\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6344 - mse: 1.6344 - mae: 0.8269 - val_loss: 587.0682 - val_mse: 587.0682 - val_mae: 11.7192\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1230 - mse: 2.1230 - mae: 0.9194 - val_loss: 542.9716 - val_mse: 542.9716 - val_mae: 10.7861\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1430 - mse: 1.1430 - mae: 0.6992 - val_loss: 567.0156 - val_mse: 567.0156 - val_mae: 11.2214\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2026 - mse: 1.2026 - mae: 0.6819 - val_loss: 540.8644 - val_mse: 540.8644 - val_mae: 10.9526\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8838 - mse: 0.8838 - mae: 0.5633 - val_loss: 508.8480 - val_mse: 508.8480 - val_mae: 10.7619\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6402 - mse: 1.6402 - mae: 0.7576 - val_loss: 537.1151 - val_mse: 537.1151 - val_mae: 11.1069\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8418 - mse: 0.8418 - mae: 0.5435 - val_loss: 546.6627 - val_mse: 546.6627 - val_mae: 11.0660\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2099 - mse: 1.2099 - mae: 0.6059 - val_loss: 504.5700 - val_mse: 504.5700 - val_mae: 10.7962\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9192 - mse: 0.9192 - mae: 0.5507 - val_loss: 529.4742 - val_mse: 529.4742 - val_mae: 10.9939\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8450 - mse: 0.8450 - mae: 0.5438 - val_loss: 542.2006 - val_mse: 542.2006 - val_mae: 11.0232\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6525 - mse: 0.6525 - mae: 0.4550 - val_loss: 528.2376 - val_mse: 528.2376 - val_mae: 10.9404\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6661 - mse: 0.6661 - mae: 0.4617 - val_loss: 530.3096 - val_mse: 530.3096 - val_mae: 10.8908\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6349 - mse: 0.6349 - mae: 0.4300 - val_loss: 515.1641 - val_mse: 515.1641 - val_mae: 10.8017\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6571 - mse: 0.6571 - mae: 0.4499 - val_loss: 505.6434 - val_mse: 505.6434 - val_mae: 10.7321\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9549 - mse: 0.9549 - mae: 0.5529 - val_loss: 555.1779 - val_mse: 555.1779 - val_mae: 11.2042\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3212 - mse: 1.3212 - mae: 0.6752 - val_loss: 574.7724 - val_mse: 574.7724 - val_mae: 11.4069\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9114 - mse: 1.9114 - mae: 0.7550 - val_loss: 511.8734 - val_mse: 511.8734 - val_mae: 10.7347\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4801 - mse: 1.4801 - mae: 0.6240 - val_loss: 476.3300 - val_mse: 476.3300 - val_mae: 10.6387\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1198 - mse: 3.1198 - mae: 0.9399 - val_loss: 571.6296 - val_mse: 571.6296 - val_mae: 11.0758\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4197 - mse: 3.4197 - mae: 1.1377 - val_loss: 644.2932 - val_mse: 644.2932 - val_mae: 12.2981\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 14.3616 - mse: 14.3616 - mae: 2.2073 - val_loss: 413.9010 - val_mse: 413.9010 - val_mae: 9.3722\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.3902 - mse: 34.3902 - mae: 3.5773 - val_loss: 495.7053 - val_mse: 495.7053 - val_mae: 10.5913\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.3836 - mse: 16.3836 - mae: 2.5980 - val_loss: 899.4570 - val_mse: 899.4570 - val_mae: 13.4776\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.5649 - mse: 37.5649 - mae: 3.3783 - val_loss: 750.0416 - val_mse: 750.0417 - val_mae: 11.8550\n",
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.6273 - mse: 65.6273 - mae: 3.6891 - val_loss: 460.1743 - val_mse: 460.1743 - val_mae: 9.4298\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49.1095 - mse: 49.1095 - mae: 3.6947 - val_loss: 325.6768 - val_mse: 325.6768 - val_mae: 9.2874\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 119.4096 - mse: 119.4096 - mae: 5.1379 - val_loss: 989.5941 - val_mse: 989.5941 - val_mae: 12.9536\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72.2977 - mse: 72.2977 - mae: 5.0107 - val_loss: 580.3480 - val_mse: 580.3480 - val_mae: 12.4717\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.7392 - mse: 64.7392 - mae: 5.4928 - val_loss: 414.5734 - val_mse: 414.5734 - val_mae: 10.5149\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 61.3354 - mse: 61.3354 - mae: 5.1215 - val_loss: 541.8099 - val_mse: 541.8099 - val_mae: 12.9615\n",
      "Epoch 719/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 59.6199 - mse: 59.6199 - mae: 4.3678 - val_loss: 474.2935 - val_mse: 474.2935 - val_mae: 11.3913\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.7432 - mse: 20.7432 - mae: 3.2781 - val_loss: 401.4785 - val_mse: 401.4785 - val_mae: 9.9641\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.0038 - mse: 27.0038 - mae: 3.1393 - val_loss: 533.8333 - val_mse: 533.8333 - val_mae: 11.3121\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.2495 - mse: 11.2495 - mae: 2.4320 - val_loss: 687.8480 - val_mse: 687.8480 - val_mae: 12.2591\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.7647 - mse: 17.7647 - mae: 2.2936 - val_loss: 410.4252 - val_mse: 410.4252 - val_mae: 10.0242\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.1445 - mse: 18.1445 - mae: 2.8692 - val_loss: 516.0358 - val_mse: 516.0358 - val_mae: 11.5887\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8880 - mse: 8.8880 - mae: 2.1088 - val_loss: 507.0631 - val_mse: 507.0631 - val_mae: 10.7141\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6155 - mse: 5.6155 - mae: 1.6949 - val_loss: 476.3130 - val_mse: 476.3130 - val_mae: 11.0317\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5188 - mse: 3.5188 - mae: 1.3075 - val_loss: 500.2596 - val_mse: 500.2596 - val_mae: 11.0002\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5929 - mse: 2.5929 - mae: 1.1418 - val_loss: 498.5411 - val_mse: 498.5411 - val_mae: 11.1451\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1564 - mse: 2.1564 - mae: 1.0054 - val_loss: 509.1289 - val_mse: 509.1289 - val_mae: 10.7541\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6666 - mse: 1.6666 - mae: 0.9074 - val_loss: 537.7584 - val_mse: 537.7584 - val_mae: 11.1573\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3117 - mse: 1.3117 - mae: 0.7865 - val_loss: 508.2603 - val_mse: 508.2603 - val_mae: 10.9741\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6311 - mse: 1.6311 - mae: 0.7618 - val_loss: 513.3888 - val_mse: 513.3888 - val_mae: 10.6287\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7273 - mse: 1.7273 - mae: 0.8392 - val_loss: 541.4486 - val_mse: 541.4486 - val_mae: 11.2444\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0848 - mse: 2.0848 - mae: 0.8514 - val_loss: 455.4149 - val_mse: 455.4149 - val_mae: 10.3621\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1491 - mse: 4.1491 - mae: 1.1361 - val_loss: 493.6641 - val_mse: 493.6641 - val_mae: 10.6469\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2833 - mse: 3.2833 - mae: 1.0356 - val_loss: 572.7310 - val_mse: 572.7310 - val_mae: 11.4672\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9450 - mse: 2.9450 - mae: 0.9844 - val_loss: 459.5328 - val_mse: 459.5328 - val_mae: 10.3995\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9239 - mse: 1.9239 - mae: 0.7789 - val_loss: 513.2968 - val_mse: 513.2968 - val_mae: 10.9515\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7704 - mse: 1.7704 - mae: 0.8417 - val_loss: 545.6610 - val_mse: 545.6610 - val_mae: 11.1564\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9335 - mse: 1.9335 - mae: 0.8236 - val_loss: 492.4934 - val_mse: 492.4934 - val_mae: 10.6202\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5076 - mse: 1.5076 - mae: 0.7036 - val_loss: 521.2548 - val_mse: 521.2548 - val_mae: 10.8450\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3581 - mse: 1.3581 - mae: 0.6759 - val_loss: 562.1765 - val_mse: 562.1765 - val_mae: 11.3345\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0077 - mse: 2.0077 - mae: 0.7709 - val_loss: 487.8371 - val_mse: 487.8371 - val_mae: 10.6680\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5113 - mse: 1.5113 - mae: 0.6923 - val_loss: 493.2131 - val_mse: 493.2131 - val_mae: 10.6405\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9747 - mse: 1.9747 - mae: 0.8525 - val_loss: 556.6596 - val_mse: 556.6596 - val_mae: 11.1376\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9498 - mse: 2.9498 - mae: 0.9534 - val_loss: 542.6982 - val_mse: 542.6982 - val_mae: 11.2317\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7346 - mse: 1.7346 - mae: 0.7725 - val_loss: 449.2579 - val_mse: 449.2579 - val_mae: 10.2494\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7824 - mse: 1.7824 - mae: 0.8128 - val_loss: 517.5719 - val_mse: 517.5719 - val_mae: 11.2460\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1826 - mse: 2.1826 - mae: 0.9396 - val_loss: 511.7079 - val_mse: 511.7079 - val_mae: 10.7365\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3756 - mse: 1.3756 - mae: 0.8002 - val_loss: 496.0474 - val_mse: 496.0474 - val_mae: 10.8314\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4358 - mse: 1.4358 - mae: 0.7741 - val_loss: 531.8484 - val_mse: 531.8484 - val_mae: 11.0709\n",
      "Epoch 752/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9223 - mse: 1.9223 - mae: 0.8015 - val_loss: 545.3030 - val_mse: 545.3030 - val_mae: 11.0766\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3601 - mse: 1.3601 - mae: 0.7549 - val_loss: 509.9717 - val_mse: 509.9717 - val_mae: 10.6996\n",
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8825 - mse: 0.8825 - mae: 0.5957 - val_loss: 523.5835 - val_mse: 523.5835 - val_mae: 10.8743\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0819 - mse: 1.0819 - mae: 0.6450 - val_loss: 525.7503 - val_mse: 525.7503 - val_mae: 11.0313\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9329 - mse: 0.9329 - mae: 0.5524 - val_loss: 508.1761 - val_mse: 508.1761 - val_mae: 10.7553\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7536 - mse: 0.7536 - mae: 0.4859 - val_loss: 517.3407 - val_mse: 517.3407 - val_mae: 10.8494\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9192 - mse: 0.9192 - mae: 0.5502 - val_loss: 531.7452 - val_mse: 531.7452 - val_mae: 10.9915\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7778 - mse: 0.7778 - mae: 0.5551 - val_loss: 491.0246 - val_mse: 491.0246 - val_mae: 10.6069\n",
      "Epoch 760/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1728 - mse: 1.1728 - mae: 0.6920 - val_loss: 526.9993 - val_mse: 526.9993 - val_mae: 11.0270\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1010 - mse: 1.1010 - mae: 0.6491 - val_loss: 520.8658 - val_mse: 520.8658 - val_mae: 10.8513\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4826 - mse: 1.4826 - mae: 0.7606 - val_loss: 481.7697 - val_mse: 481.7697 - val_mae: 10.5791\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8656 - mse: 1.8656 - mae: 0.7618 - val_loss: 485.0637 - val_mse: 485.0637 - val_mae: 10.4076\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5409 - mse: 1.5409 - mae: 0.7583 - val_loss: 524.7313 - val_mse: 524.7313 - val_mae: 11.2603\n",
      "Epoch 765/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.0741 - mse: 1.0741 - mae: 0.6331 - val_loss: 504.8434 - val_mse: 504.8434 - val_mae: 10.7368\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7838 - mse: 0.7838 - mae: 0.5057 - val_loss: 501.7636 - val_mse: 501.7636 - val_mae: 10.8378\n",
      "Epoch 767/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0205 - mse: 1.0205 - mae: 0.5366 - val_loss: 509.8052 - val_mse: 509.8052 - val_mae: 10.7071\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8694 - mse: 0.8694 - mae: 0.5663 - val_loss: 539.9404 - val_mse: 539.9404 - val_mae: 11.1114\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0417 - mse: 1.0417 - mae: 0.5758 - val_loss: 522.5085 - val_mse: 522.5085 - val_mae: 10.8575\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8157 - mse: 0.8157 - mae: 0.5191 - val_loss: 507.4763 - val_mse: 507.4763 - val_mae: 10.7633\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6641 - mse: 0.6641 - mae: 0.4715 - val_loss: 508.1042 - val_mse: 508.1042 - val_mae: 10.8032\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5620 - mse: 0.5620 - mae: 0.4136 - val_loss: 518.5046 - val_mse: 518.5046 - val_mae: 10.7997\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5130 - mse: 0.5130 - mae: 0.3701 - val_loss: 511.6288 - val_mse: 511.6288 - val_mae: 10.7969\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4979 - mse: 0.4979 - mae: 0.3724 - val_loss: 517.8935 - val_mse: 517.8935 - val_mae: 10.8627\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4505 - mse: 0.4505 - mae: 0.3528 - val_loss: 521.7974 - val_mse: 521.7974 - val_mae: 10.9030\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5221 - mse: 0.5221 - mae: 0.3610 - val_loss: 511.4986 - val_mse: 511.4986 - val_mae: 10.7764\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4712 - mse: 0.4712 - mae: 0.3383 - val_loss: 508.5081 - val_mse: 508.5081 - val_mae: 10.7769\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6295 - mse: 0.6295 - mae: 0.4133 - val_loss: 535.2109 - val_mse: 535.2109 - val_mae: 10.9123\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7432 - mse: 0.7432 - mae: 0.4650 - val_loss: 518.1785 - val_mse: 518.1785 - val_mae: 11.0142\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5424 - mse: 0.5424 - mae: 0.3904 - val_loss: 512.8363 - val_mse: 512.8363 - val_mae: 10.7263\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4519 - mse: 0.4519 - mae: 0.3482 - val_loss: 509.7232 - val_mse: 509.7232 - val_mae: 10.7949\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4633 - mse: 0.4633 - mae: 0.3744 - val_loss: 517.8275 - val_mse: 517.8275 - val_mae: 10.7817\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4562 - mse: 0.4562 - mae: 0.3580 - val_loss: 519.5967 - val_mse: 519.5967 - val_mae: 10.9219\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4735 - mse: 0.4735 - mae: 0.3531 - val_loss: 519.8845 - val_mse: 519.8845 - val_mae: 10.8774\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4437 - mse: 0.4437 - mae: 0.3275 - val_loss: 527.9125 - val_mse: 527.9125 - val_mae: 10.9274\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5012 - mse: 0.5012 - mae: 0.3459 - val_loss: 528.0050 - val_mse: 528.0050 - val_mae: 10.9699\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5077 - mse: 0.5077 - mae: 0.3793 - val_loss: 519.4355 - val_mse: 519.4355 - val_mae: 10.8168\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4203 - mse: 0.4203 - mae: 0.3136 - val_loss: 523.1447 - val_mse: 523.1447 - val_mae: 10.9414\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3967 - mse: 0.3967 - mae: 0.3003 - val_loss: 515.3425 - val_mse: 515.3425 - val_mae: 10.7371\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4686 - mse: 0.4686 - mae: 0.3552 - val_loss: 528.5937 - val_mse: 528.5937 - val_mae: 11.0527\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4715 - mse: 0.4715 - mae: 0.3614 - val_loss: 524.0252 - val_mse: 524.0252 - val_mae: 10.9121\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4269 - mse: 0.4269 - mae: 0.3154 - val_loss: 522.7609 - val_mse: 522.7609 - val_mae: 10.9261\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4300 - mse: 0.4300 - mae: 0.3205 - val_loss: 524.8625 - val_mse: 524.8625 - val_mae: 10.8534\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4201 - mse: 0.4201 - mae: 0.3246 - val_loss: 520.8686 - val_mse: 520.8686 - val_mae: 10.9637\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4630 - mse: 0.4630 - mae: 0.3329 - val_loss: 513.2786 - val_mse: 513.2786 - val_mae: 10.8115\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4490 - mse: 0.4490 - mae: 0.3193 - val_loss: 520.3637 - val_mse: 520.3637 - val_mae: 10.8494\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4880 - mse: 0.4880 - mae: 0.3361 - val_loss: 523.2305 - val_mse: 523.2305 - val_mae: 10.9483\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4246 - mse: 0.4246 - mae: 0.3580 - val_loss: 517.8994 - val_mse: 517.8994 - val_mae: 10.8599\n",
      "Epoch 799/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4665 - mse: 0.4665 - mae: 0.3548 - val_loss: 517.8910 - val_mse: 517.8910 - val_mae: 10.7769\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4837 - mse: 0.4837 - mae: 0.3778 - val_loss: 520.9158 - val_mse: 520.9158 - val_mae: 10.9589\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4158 - mse: 0.4158 - mae: 0.3223 - val_loss: 520.2302 - val_mse: 520.2302 - val_mae: 10.8381\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4113 - mse: 0.4113 - mae: 0.3244 - val_loss: 516.1061 - val_mse: 516.1061 - val_mae: 10.8928\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4170 - mse: 0.4170 - mae: 0.3209 - val_loss: 518.9384 - val_mse: 518.9384 - val_mae: 10.8550\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3802 - mse: 0.3802 - mae: 0.2886 - val_loss: 520.2119 - val_mse: 520.2119 - val_mae: 10.8610\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3818 - mse: 0.3818 - mae: 0.2985 - val_loss: 522.0282 - val_mse: 522.0282 - val_mae: 10.9361\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4120 - mse: 0.4120 - mae: 0.3156 - val_loss: 517.8351 - val_mse: 517.8351 - val_mae: 10.8146\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4482 - mse: 0.4482 - mae: 0.3217 - val_loss: 514.7743 - val_mse: 514.7743 - val_mae: 10.8735\n",
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5092 - mse: 0.5092 - mae: 0.3492 - val_loss: 515.2595 - val_mse: 515.2595 - val_mae: 10.7381\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4142 - mse: 0.4142 - mae: 0.3216 - val_loss: 509.6263 - val_mse: 509.6263 - val_mae: 10.8129\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4349 - mse: 0.4349 - mae: 0.3345 - val_loss: 518.4366 - val_mse: 518.4366 - val_mae: 10.9106\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4449 - mse: 0.4449 - mae: 0.3421 - val_loss: 524.2940 - val_mse: 524.2940 - val_mae: 10.8917\n",
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5783 - mse: 0.5783 - mae: 0.3815 - val_loss: 525.5303 - val_mse: 525.5303 - val_mae: 10.9818\n",
      "Epoch 813/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5352 - mse: 0.5352 - mae: 0.3958 - val_loss: 521.0521 - val_mse: 521.0521 - val_mae: 10.8387\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4597 - mse: 0.4597 - mae: 0.3635 - val_loss: 502.6777 - val_mse: 502.6777 - val_mae: 10.7396\n",
      "Epoch 815/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5960 - mse: 0.5960 - mae: 0.3820 - val_loss: 516.2555 - val_mse: 516.2555 - val_mae: 10.6997\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4366 - mse: 0.4366 - mae: 0.3573 - val_loss: 533.1760 - val_mse: 533.1760 - val_mae: 11.0667\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5435 - mse: 0.5435 - mae: 0.3741 - val_loss: 527.8101 - val_mse: 527.8101 - val_mae: 10.9850\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5877 - mse: 0.5877 - mae: 0.4275 - val_loss: 495.4685 - val_mse: 495.4685 - val_mae: 10.4891\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0930 - mse: 1.0930 - mae: 0.6244 - val_loss: 503.4416 - val_mse: 503.4416 - val_mae: 10.7825\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6096 - mse: 1.6096 - mae: 0.6671 - val_loss: 595.7502 - val_mse: 595.7502 - val_mae: 11.5277\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6008 - mse: 6.6008 - mae: 1.4284 - val_loss: 595.3728 - val_mse: 595.3728 - val_mae: 11.7720\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.1381 - mse: 22.1381 - mae: 2.2400 - val_loss: 562.5152 - val_mse: 562.5152 - val_mae: 11.7640\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.3150 - mse: 39.3150 - mae: 3.4744 - val_loss: 341.1425 - val_mse: 341.1425 - val_mae: 8.9958\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.9866 - mse: 39.9866 - mae: 3.8160 - val_loss: 356.4565 - val_mse: 356.4565 - val_mae: 10.5120\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 34.9483 - mse: 34.9483 - mae: 3.0346 - val_loss: 438.5606 - val_mse: 438.5606 - val_mae: 10.4618\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3556 - mse: 14.3556 - mae: 2.7928 - val_loss: 596.0119 - val_mse: 596.0119 - val_mae: 13.7414\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 45.1114 - mse: 45.1114 - mae: 4.5688 - val_loss: 356.7476 - val_mse: 356.7476 - val_mae: 9.0728\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 46.0743 - mse: 46.0743 - mae: 4.0927 - val_loss: 438.0522 - val_mse: 438.0522 - val_mae: 10.4539\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.7685 - mse: 16.7685 - mae: 2.6208 - val_loss: 581.6004 - val_mse: 581.6004 - val_mae: 11.8993\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.7007 - mse: 18.7007 - mae: 2.6579 - val_loss: 407.7664 - val_mse: 407.7664 - val_mae: 10.0785\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7287 - mse: 10.7287 - mae: 2.0800 - val_loss: 363.8678 - val_mse: 363.8678 - val_mae: 9.3475\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.8580 - mse: 14.8580 - mae: 2.2211 - val_loss: 504.6199 - val_mse: 504.6199 - val_mae: 11.0089\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8927 - mse: 8.8927 - mae: 1.7430 - val_loss: 624.4785 - val_mse: 624.4785 - val_mae: 11.6974\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.8651 - mse: 10.8651 - mae: 2.0064 - val_loss: 557.5051 - val_mse: 557.5051 - val_mae: 11.4123\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8941 - mse: 3.8941 - mae: 1.1891 - val_loss: 459.4777 - val_mse: 459.4777 - val_mae: 10.5785\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2830 - mse: 4.2830 - mae: 1.1853 - val_loss: 478.2448 - val_mse: 478.2448 - val_mae: 10.4733\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9345 - mse: 2.9345 - mae: 1.1159 - val_loss: 542.1008 - val_mse: 542.1008 - val_mae: 11.5316\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.4906 - mse: 2.4906 - mae: 0.9466 - val_loss: 439.9114 - val_mse: 439.9114 - val_mae: 10.4085\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6579 - mse: 5.6579 - mae: 1.3331 - val_loss: 412.8986 - val_mse: 412.8986 - val_mae: 9.7688\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9276 - mse: 4.9276 - mae: 1.2200 - val_loss: 529.9965 - val_mse: 529.9965 - val_mae: 10.8346\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1989 - mse: 4.1989 - mae: 1.0883 - val_loss: 571.5896 - val_mse: 571.5896 - val_mae: 11.5156\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1527 - mse: 4.1527 - mae: 1.2382 - val_loss: 444.3952 - val_mse: 444.3952 - val_mae: 9.8088\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4533 - mse: 7.4533 - mae: 1.4976 - val_loss: 460.0711 - val_mse: 460.0711 - val_mae: 10.2460\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.9561 - mse: 4.9561 - mae: 1.1676 - val_loss: 595.6614 - val_mse: 595.6614 - val_mae: 11.6421\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.6976 - mse: 7.6976 - mae: 1.5219 - val_loss: 498.8541 - val_mse: 498.8541 - val_mae: 11.0198\n",
      "Epoch 846/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9654 - mse: 3.9654 - mae: 1.2517 - val_loss: 442.3554 - val_mse: 442.3554 - val_mae: 9.9521\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4548 - mse: 4.4548 - mae: 1.2450 - val_loss: 537.1198 - val_mse: 537.1198 - val_mae: 11.3458\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0671 - mse: 3.0671 - mae: 1.0311 - val_loss: 608.0885 - val_mse: 608.0885 - val_mae: 11.3932\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3610 - mse: 3.3610 - mae: 0.9976 - val_loss: 541.9869 - val_mse: 541.9869 - val_mae: 11.2093\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1268 - mse: 2.1268 - mae: 0.8791 - val_loss: 477.8851 - val_mse: 477.8851 - val_mae: 10.5182\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5341 - mse: 1.5341 - mae: 0.7193 - val_loss: 494.4888 - val_mse: 494.4888 - val_mae: 10.9411\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0621 - mse: 1.0621 - mae: 0.5920 - val_loss: 529.5647 - val_mse: 529.5647 - val_mae: 10.9053\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7575 - mse: 1.7575 - mae: 0.6674 - val_loss: 486.9533 - val_mse: 486.9533 - val_mae: 10.6850\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4889 - mse: 1.4889 - mae: 0.6991 - val_loss: 470.9914 - val_mse: 470.9914 - val_mae: 10.3798\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3272 - mse: 1.3272 - mae: 0.7383 - val_loss: 520.4125 - val_mse: 520.4125 - val_mae: 11.1685\n",
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5750 - mse: 1.5750 - mae: 0.7246 - val_loss: 529.2271 - val_mse: 529.2271 - val_mae: 10.8345\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3292 - mse: 1.3292 - mae: 0.7153 - val_loss: 505.5614 - val_mse: 505.5614 - val_mae: 11.0398\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9435 - mse: 0.9435 - mae: 0.6420 - val_loss: 498.9957 - val_mse: 498.9957 - val_mae: 10.6586\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7689 - mse: 0.7689 - mae: 0.5842 - val_loss: 511.0110 - val_mse: 511.0110 - val_mae: 10.7222\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6028 - mse: 0.6028 - mae: 0.4834 - val_loss: 507.4520 - val_mse: 507.4520 - val_mae: 10.7821\n",
      "Epoch 861/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4528 - mse: 0.4528 - mae: 0.3676 - val_loss: 501.9263 - val_mse: 501.9263 - val_mae: 10.6912\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4453 - mse: 0.4453 - mae: 0.3926 - val_loss: 519.2221 - val_mse: 519.2221 - val_mae: 10.9873\n",
      "Epoch 863/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4720 - mse: 0.4720 - mae: 0.3741 - val_loss: 499.8686 - val_mse: 499.8686 - val_mae: 10.7264\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4895 - mse: 0.4895 - mae: 0.3742 - val_loss: 488.1430 - val_mse: 488.1430 - val_mae: 10.5587\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4314 - mse: 0.4314 - mae: 0.3581 - val_loss: 510.7172 - val_mse: 510.7172 - val_mae: 10.8970\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6009 - mse: 0.6009 - mae: 0.3907 - val_loss: 508.9812 - val_mse: 508.9812 - val_mae: 10.9527\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5239 - mse: 0.5239 - mae: 0.3805 - val_loss: 493.3459 - val_mse: 493.3459 - val_mae: 10.6514\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4553 - mse: 0.4553 - mae: 0.3309 - val_loss: 501.7829 - val_mse: 501.7829 - val_mae: 10.8110\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4191 - mse: 0.4191 - mae: 0.3627 - val_loss: 520.4680 - val_mse: 520.4680 - val_mae: 10.9262\n",
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5885 - mse: 0.5885 - mae: 0.4889 - val_loss: 522.5138 - val_mse: 522.5138 - val_mae: 11.0064\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0976 - mse: 1.0976 - mae: 0.5729 - val_loss: 496.5982 - val_mse: 496.5982 - val_mae: 10.6606\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1357 - mse: 1.1357 - mae: 0.5262 - val_loss: 501.1809 - val_mse: 501.1809 - val_mae: 10.6944\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0392 - mse: 1.0392 - mae: 0.5358 - val_loss: 464.5320 - val_mse: 464.5320 - val_mae: 10.3028\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3970 - mse: 1.3970 - mae: 0.6986 - val_loss: 504.4565 - val_mse: 504.4565 - val_mae: 10.8805\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3496 - mse: 1.3496 - mae: 0.6572 - val_loss: 525.1011 - val_mse: 525.1011 - val_mae: 10.8611\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8552 - mse: 0.8552 - mae: 0.5469 - val_loss: 531.8561 - val_mse: 531.8561 - val_mae: 10.9510\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0840 - mse: 1.0840 - mae: 0.6216 - val_loss: 499.9825 - val_mse: 499.9825 - val_mae: 10.6319\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8826 - mse: 0.8826 - mae: 0.5478 - val_loss: 471.1791 - val_mse: 471.1791 - val_mae: 10.4130\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4730 - mse: 1.4730 - mae: 0.6319 - val_loss: 515.4330 - val_mse: 515.4330 - val_mae: 10.7238\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7051 - mse: 0.7051 - mae: 0.4859 - val_loss: 529.9733 - val_mse: 529.9733 - val_mae: 11.0904\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6138 - mse: 0.6138 - mae: 0.4599 - val_loss: 518.9447 - val_mse: 518.9447 - val_mae: 10.7893\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6134 - mse: 0.6134 - mae: 0.4810 - val_loss: 497.4904 - val_mse: 497.4904 - val_mae: 10.8079\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6320 - mse: 0.6320 - mae: 0.4897 - val_loss: 506.4753 - val_mse: 506.4753 - val_mae: 10.7525\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5251 - mse: 0.5251 - mae: 0.4510 - val_loss: 512.3235 - val_mse: 512.3235 - val_mae: 10.8951\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5091 - mse: 0.5091 - mae: 0.4740 - val_loss: 495.5430 - val_mse: 495.5430 - val_mae: 10.6383\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5471 - mse: 0.5471 - mae: 0.4137 - val_loss: 496.4976 - val_mse: 496.4976 - val_mae: 10.6249\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5496 - mse: 0.5496 - mae: 0.4095 - val_loss: 505.1118 - val_mse: 505.1118 - val_mae: 10.8536\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4220 - mse: 0.4220 - mae: 0.3565 - val_loss: 504.9684 - val_mse: 504.9684 - val_mae: 10.6072\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4115 - mse: 0.4115 - mae: 0.3600 - val_loss: 503.8705 - val_mse: 503.8705 - val_mae: 10.8646\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4355 - mse: 0.4355 - mae: 0.3851 - val_loss: 503.5583 - val_mse: 503.5583 - val_mae: 10.7325\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5706 - mse: 0.5706 - mae: 0.5033 - val_loss: 523.2486 - val_mse: 523.2486 - val_mae: 11.0062\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6195 - mse: 0.6195 - mae: 0.4808 - val_loss: 521.4481 - val_mse: 521.4481 - val_mae: 10.8191\n",
      "Epoch 893/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5442 - mse: 0.5442 - mae: 0.3901 - val_loss: 518.7355 - val_mse: 518.7355 - val_mae: 11.0241\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5561 - mse: 0.5561 - mae: 0.4350 - val_loss: 515.7477 - val_mse: 515.7477 - val_mae: 10.7859\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6124 - mse: 0.6124 - mae: 0.4171 - val_loss: 515.7218 - val_mse: 515.7218 - val_mae: 10.9792\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4754 - mse: 0.4754 - mae: 0.4005 - val_loss: 508.5662 - val_mse: 508.5662 - val_mae: 10.6821\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4350 - mse: 0.4350 - mae: 0.3360 - val_loss: 505.9791 - val_mse: 505.9791 - val_mae: 10.7427\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4886 - mse: 0.4886 - mae: 0.3730 - val_loss: 535.5203 - val_mse: 535.5203 - val_mae: 10.8734\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7552 - mse: 1.7552 - mae: 0.7259 - val_loss: 566.1696 - val_mse: 566.1696 - val_mae: 11.6294\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8666 - mse: 1.8666 - mae: 0.7038 - val_loss: 512.5703 - val_mse: 512.5703 - val_mae: 10.7984\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7842 - mse: 0.7842 - mae: 0.4947 - val_loss: 496.1748 - val_mse: 496.1748 - val_mae: 10.7232\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7266 - mse: 0.7266 - mae: 0.4906 - val_loss: 501.1523 - val_mse: 501.1523 - val_mae: 10.7251\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6077 - mse: 0.6077 - mae: 0.4613 - val_loss: 493.4972 - val_mse: 493.4972 - val_mae: 10.6683\n",
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1846 - mse: 1.1846 - mae: 0.6191 - val_loss: 521.3393 - val_mse: 521.3393 - val_mae: 10.8677\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5839 - mse: 0.5839 - mae: 0.4619 - val_loss: 522.3715 - val_mse: 522.3715 - val_mae: 11.0798\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6497 - mse: 0.6497 - mae: 0.4839 - val_loss: 502.6277 - val_mse: 502.6277 - val_mae: 10.8734\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3777 - mse: 0.3777 - mae: 0.3667 - val_loss: 500.0380 - val_mse: 500.0380 - val_mae: 10.7105\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4590 - mse: 0.4590 - mae: 0.3561 - val_loss: 497.8406 - val_mse: 497.8406 - val_mae: 10.7485\n",
      "Epoch 909/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4997 - mse: 0.4997 - mae: 0.4012 - val_loss: 510.8866 - val_mse: 510.8866 - val_mae: 10.8849\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4510 - mse: 0.4510 - mae: 0.3350 - val_loss: 513.1003 - val_mse: 513.1003 - val_mae: 10.7490\n",
      "Epoch 911/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4186 - mse: 0.4186 - mae: 0.3496 - val_loss: 541.4384 - val_mse: 541.4384 - val_mae: 11.1724\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0182 - mse: 1.0182 - mae: 0.5440 - val_loss: 535.4173 - val_mse: 535.4173 - val_mae: 11.1030\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0927 - mse: 1.0927 - mae: 0.6157 - val_loss: 508.1142 - val_mse: 508.1142 - val_mae: 10.4324\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8785 - mse: 0.8785 - mae: 0.5531 - val_loss: 536.8259 - val_mse: 536.8259 - val_mae: 10.9513\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5015 - mse: 0.5015 - mae: 0.4066 - val_loss: 504.2234 - val_mse: 504.2234 - val_mae: 10.6413\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6919 - mse: 0.6919 - mae: 0.4659 - val_loss: 483.9002 - val_mse: 483.9002 - val_mae: 10.5023\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7059 - mse: 0.7059 - mae: 0.4953 - val_loss: 504.3998 - val_mse: 504.3998 - val_mae: 10.9068\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5698 - mse: 0.5698 - mae: 0.4188 - val_loss: 511.6812 - val_mse: 511.6812 - val_mae: 10.8839\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5795 - mse: 0.5795 - mae: 0.4128 - val_loss: 520.1866 - val_mse: 520.1866 - val_mae: 11.0417\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8918 - mse: 0.8918 - mae: 0.5029 - val_loss: 514.6564 - val_mse: 514.6564 - val_mae: 10.8693\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7935 - mse: 1.7935 - mae: 0.6854 - val_loss: 468.2732 - val_mse: 468.2732 - val_mae: 10.5712\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8170 - mse: 2.8170 - mae: 0.9322 - val_loss: 451.9668 - val_mse: 451.9668 - val_mae: 10.0267\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6118 - mse: 2.6118 - mae: 1.0623 - val_loss: 540.2922 - val_mse: 540.2922 - val_mae: 11.2894\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7360 - mse: 7.7360 - mae: 1.4599 - val_loss: 590.6714 - val_mse: 590.6714 - val_mae: 12.6487\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 9.2768 - mse: 9.2768 - mae: 1.8522 - val_loss: 594.7994 - val_mse: 594.7994 - val_mae: 11.2243\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9378 - mse: 9.9378 - mae: 1.7115 - val_loss: 460.1094 - val_mse: 460.1094 - val_mae: 10.0445\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6150 - mse: 5.6150 - mae: 1.4491 - val_loss: 420.0419 - val_mse: 420.0419 - val_mae: 9.6182\n",
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4726 - mse: 9.4726 - mae: 1.6757 - val_loss: 534.0868 - val_mse: 534.0868 - val_mae: 10.6089\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6092 - mse: 5.6092 - mae: 1.4640 - val_loss: 646.5590 - val_mse: 646.5590 - val_mae: 11.8581\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2217 - mse: 6.2217 - mae: 1.3799 - val_loss: 591.9451 - val_mse: 591.9451 - val_mae: 11.2265\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.0815 - mse: 8.0815 - mae: 1.5256 - val_loss: 518.0441 - val_mse: 518.0441 - val_mae: 11.0096\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.3587 - mse: 8.3587 - mae: 1.5615 - val_loss: 482.5603 - val_mse: 482.5603 - val_mae: 10.3705\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.4682 - mse: 8.4682 - mae: 1.6095 - val_loss: 461.8697 - val_mse: 461.8697 - val_mae: 10.0962\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3017 - mse: 4.3017 - mae: 1.1594 - val_loss: 569.7374 - val_mse: 569.7374 - val_mae: 10.9845\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5965 - mse: 3.5965 - mae: 1.1048 - val_loss: 474.6104 - val_mse: 474.6104 - val_mae: 10.1161\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.6549 - mse: 7.6549 - mae: 1.6006 - val_loss: 557.9014 - val_mse: 557.9014 - val_mae: 11.9446\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.7304 - mse: 5.7304 - mae: 1.4304 - val_loss: 538.2686 - val_mse: 538.2686 - val_mae: 10.7821\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5845 - mse: 3.5845 - mae: 1.3701 - val_loss: 531.1506 - val_mse: 531.1506 - val_mae: 11.4908\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3279 - mse: 2.3279 - mae: 1.0699 - val_loss: 513.4742 - val_mse: 513.4742 - val_mae: 10.6618\n",
      "Epoch 940/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2618 - mse: 1.2618 - mae: 0.7945 - val_loss: 486.6423 - val_mse: 486.6423 - val_mae: 10.7856\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5667 - mse: 2.5667 - mae: 0.9158 - val_loss: 473.7071 - val_mse: 473.7071 - val_mae: 10.1754\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8253 - mse: 1.8253 - mae: 0.9109 - val_loss: 525.8898 - val_mse: 525.8898 - val_mae: 11.4268\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6513 - mse: 1.6513 - mae: 0.8809 - val_loss: 502.7223 - val_mse: 502.7223 - val_mae: 10.9925\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6102 - mse: 0.6102 - mae: 0.5111 - val_loss: 477.4898 - val_mse: 477.4898 - val_mae: 10.6267\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8618 - mse: 0.8618 - mae: 0.4899 - val_loss: 490.3339 - val_mse: 490.3339 - val_mae: 10.5611\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6442 - mse: 0.6442 - mae: 0.5162 - val_loss: 513.9008 - val_mse: 513.9008 - val_mae: 11.1831\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4744 - mse: 0.4744 - mae: 0.4588 - val_loss: 505.1971 - val_mse: 505.1971 - val_mae: 10.6877\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4446 - mse: 0.4446 - mae: 0.4251 - val_loss: 511.2015 - val_mse: 511.2015 - val_mae: 11.0678\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4395 - mse: 0.4395 - mae: 0.4339 - val_loss: 506.0530 - val_mse: 506.0530 - val_mae: 10.6304\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4044 - mse: 0.4044 - mae: 0.3908 - val_loss: 501.5131 - val_mse: 501.5131 - val_mae: 10.9251\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4432 - mse: 0.4432 - mae: 0.3564 - val_loss: 498.7298 - val_mse: 498.7298 - val_mae: 10.5982\n",
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4010 - mse: 0.4010 - mae: 0.3963 - val_loss: 509.5585 - val_mse: 509.5585 - val_mae: 10.9863\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3102 - mse: 0.3102 - mae: 0.3378 - val_loss: 503.8717 - val_mse: 503.8717 - val_mae: 10.5786\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3777 - mse: 0.3777 - mae: 0.3695 - val_loss: 513.9885 - val_mse: 513.9885 - val_mae: 11.0782\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4734 - mse: 0.4734 - mae: 0.4314 - val_loss: 510.9251 - val_mse: 510.9251 - val_mae: 10.6496\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4428 - mse: 0.4428 - mae: 0.3893 - val_loss: 498.3839 - val_mse: 498.3839 - val_mae: 10.8854\n",
      "Epoch 957/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3659 - mse: 0.3659 - mae: 0.3532 - val_loss: 499.2764 - val_mse: 499.2764 - val_mae: 10.5687\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4197 - mse: 0.4197 - mae: 0.3807 - val_loss: 501.2361 - val_mse: 501.2361 - val_mae: 10.8278\n",
      "Epoch 959/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0320 - mse: 1.0320 - mae: 0.5112 - val_loss: 518.3601 - val_mse: 518.3601 - val_mae: 11.1077\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2836 - mse: 1.2836 - mae: 0.6334 - val_loss: 554.1804 - val_mse: 554.1804 - val_mae: 11.2735\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0617 - mse: 3.0617 - mae: 0.8695 - val_loss: 526.4203 - val_mse: 526.4203 - val_mae: 10.9692\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9955 - mse: 0.9955 - mae: 0.5662 - val_loss: 548.2888 - val_mse: 548.2888 - val_mae: 11.0313\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6135 - mse: 1.6135 - mae: 0.6856 - val_loss: 488.6500 - val_mse: 488.6500 - val_mae: 11.0669\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2394 - mse: 1.2394 - mae: 0.7170 - val_loss: 505.0236 - val_mse: 505.0236 - val_mae: 10.6182\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6487 - mse: 0.6487 - mae: 0.5443 - val_loss: 502.9674 - val_mse: 502.9674 - val_mae: 10.9508\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6934 - mse: 0.6934 - mae: 0.5845 - val_loss: 521.2079 - val_mse: 521.2079 - val_mae: 10.7460\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7818 - mse: 0.7818 - mae: 0.5820 - val_loss: 507.3040 - val_mse: 507.3040 - val_mae: 10.9281\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7280 - mse: 0.7280 - mae: 0.4792 - val_loss: 520.4214 - val_mse: 520.4214 - val_mae: 10.8530\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4755 - mse: 0.4755 - mae: 0.3747 - val_loss: 506.2894 - val_mse: 506.2894 - val_mae: 10.8349\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3703 - mse: 0.3703 - mae: 0.3365 - val_loss: 485.3903 - val_mse: 485.3903 - val_mae: 10.4490\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5009 - mse: 0.5009 - mae: 0.4076 - val_loss: 501.4635 - val_mse: 501.4635 - val_mae: 10.6484\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4632 - mse: 0.4632 - mae: 0.3588 - val_loss: 521.9266 - val_mse: 521.9266 - val_mae: 10.9132\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3218 - mse: 0.3218 - mae: 0.2772 - val_loss: 506.8474 - val_mse: 506.8474 - val_mae: 10.7306\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3039 - mse: 0.3039 - mae: 0.2726 - val_loss: 501.0605 - val_mse: 501.0605 - val_mae: 10.7559\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3045 - mse: 0.3045 - mae: 0.2976 - val_loss: 514.5336 - val_mse: 514.5336 - val_mae: 10.7715\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3677 - mse: 0.3677 - mae: 0.3553 - val_loss: 514.7128 - val_mse: 514.7128 - val_mae: 10.9375\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3066 - mse: 0.3066 - mae: 0.3102 - val_loss: 520.3608 - val_mse: 520.3608 - val_mae: 10.8361\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8436 - mse: 0.8436 - mae: 0.4663 - val_loss: 532.2651 - val_mse: 532.2651 - val_mae: 11.0766\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7556 - mse: 0.7556 - mae: 0.5714 - val_loss: 476.6485 - val_mse: 476.6485 - val_mae: 10.2408\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0618 - mse: 1.0618 - mae: 0.6095 - val_loss: 488.7154 - val_mse: 488.7154 - val_mae: 10.7813\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8344 - mse: 0.8344 - mae: 0.5387 - val_loss: 498.7543 - val_mse: 498.7543 - val_mae: 10.6932\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4641 - mse: 1.4641 - mae: 0.7506 - val_loss: 503.3922 - val_mse: 503.3922 - val_mae: 10.7592\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0822 - mse: 1.0822 - mae: 0.6253 - val_loss: 491.2866 - val_mse: 491.2866 - val_mae: 10.6068\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5120 - mse: 0.5120 - mae: 0.4326 - val_loss: 505.1842 - val_mse: 505.1842 - val_mae: 10.6923\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4637 - mse: 0.4637 - mae: 0.4238 - val_loss: 492.0141 - val_mse: 492.0141 - val_mae: 10.7552\n",
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4717 - mse: 0.4717 - mae: 0.3927 - val_loss: 526.5769 - val_mse: 526.5769 - val_mae: 10.9149\n",
      "Epoch 987/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7663 - mse: 0.7663 - mae: 0.5317 - val_loss: 541.0140 - val_mse: 541.0140 - val_mae: 11.1399\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.9110 - mse: 0.9110 - mae: 0.5554 - val_loss: 551.1058 - val_mse: 551.1058 - val_mae: 11.0556\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9304 - mse: 0.9304 - mae: 0.5485 - val_loss: 501.4933 - val_mse: 501.4933 - val_mae: 10.6551\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6772 - mse: 0.6772 - mae: 0.4616 - val_loss: 494.3977 - val_mse: 494.3977 - val_mae: 10.6974\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6777 - mse: 0.6777 - mae: 0.4526 - val_loss: 506.1472 - val_mse: 506.1472 - val_mae: 10.8089\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7041 - mse: 0.7041 - mae: 0.4915 - val_loss: 521.4288 - val_mse: 521.4288 - val_mae: 11.1520\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7720 - mse: 0.7720 - mae: 0.5448 - val_loss: 523.0130 - val_mse: 523.0130 - val_mae: 10.8647\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4629 - mse: 1.4629 - mae: 0.6731 - val_loss: 537.3931 - val_mse: 537.3931 - val_mae: 11.3609\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6856 - mse: 7.6856 - mae: 1.5102 - val_loss: 479.5635 - val_mse: 479.5635 - val_mae: 10.4882\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 26.3777 - mse: 26.3777 - mae: 2.5628 - val_loss: 366.5833 - val_mse: 366.5833 - val_mae: 8.9690\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 90.1468 - mse: 90.1468 - mae: 5.0662 - val_loss: 393.6520 - val_mse: 393.6519 - val_mae: 9.9542\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 180.7621 - mse: 180.7621 - mae: 8.0199 - val_loss: 1512.1299 - val_mse: 1512.1299 - val_mae: 21.8160\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 127.1429 - mse: 127.1429 - mae: 7.5871 - val_loss: 1054.0887 - val_mse: 1054.0887 - val_mae: 12.8472\n",
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 90.6068 - mse: 90.6068 - mae: 4.5243 - val_loss: 517.1928 - val_mse: 517.1928 - val_mae: 10.5097\n"
     ]
    }
   ],
   "source": [
    "hist5 = model5.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WYG7LYfgf047",
    "outputId": "744eeab9-23b5-4cfb-b640-a6bc74bdc309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3cbfdd45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "예측값 :  [155.4876]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "preds = model5.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712039542900389\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snEuBCBZI3Fn"
   },
   "source": [
    "### 1.6 weight initialization 기법 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "87ETIkgeKavw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v30sAAkwK7RT",
    "outputId": "417d0a8d-9134-4601-e992-8b6be36ebd2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 5e-4\n",
    "optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "loss = MeanSquaredError()\n",
    "epochs = 1000\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "8pfQqx-bI2ce"
   },
   "outputs": [],
   "source": [
    "model6 = tf.keras.Sequential()\n",
    "\n",
    "model6.add(tf.keras.Input(shape = 3))\n",
    "#Activation function을 내부에 넣어 줄 수도 있습니다.\n",
    "model6.add(Dense(100, activation='relu', kernel_initializer='he_normal')) \n",
    "model6.add(Dense(200, activation='relu', kernel_initializer='he_normal'))\n",
    "model6.add(Dense(300, activation='relu', kernel_initializer='he_normal')) \n",
    "model6.add(Dense(200, activation='relu', kernel_initializer='he_normal'))\n",
    "model6.add(Dense(100, activation='relu', kernel_initializer='he_normal'))\n",
    "model6.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHrGj7G4I90S",
    "outputId": "6ffc682f-569a-4a38-ebf9-d9def0dc8ca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 100)               400       \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 200)               20200     \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 300)               60300     \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,301\n",
      "Trainable params: 161,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 47ms/step - loss: 22351.1934 - mse: 22351.1934 - mae: 98.4794 - val_loss: 31620.9004 - val_mse: 31620.9004 - val_mae: 117.8865\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 20358.4492 - mse: 20358.4492 - mae: 89.7444 - val_loss: 28346.2168 - val_mse: 28346.2168 - val_mae: 106.8462\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17426.1211 - mse: 17426.1211 - mae: 79.9645 - val_loss: 23649.9551 - val_mse: 23649.9551 - val_mae: 91.2390\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14417.9639 - mse: 14417.9639 - mae: 72.2417 - val_loss: 17837.5840 - val_mse: 17837.5840 - val_mae: 74.8226\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9946.8496 - mse: 9946.8496 - mae: 64.6175 - val_loss: 12247.2949 - val_mse: 12247.2949 - val_mae: 65.4646\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 6419.8770 - mse: 6419.8770 - mae: 57.0717 - val_loss: 8153.4834 - val_mse: 8153.4834 - val_mae: 56.2883\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4141.3916 - mse: 4141.3916 - mae: 47.9189 - val_loss: 5434.2754 - val_mse: 5434.2754 - val_mae: 44.2267\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2332.1985 - mse: 2332.1985 - mae: 31.9629 - val_loss: 3874.4727 - val_mse: 3874.4727 - val_mae: 32.1824\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1641.2501 - mse: 1641.2501 - mae: 24.6809 - val_loss: 3166.1111 - val_mse: 3166.1111 - val_mae: 27.5276\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1422.9788 - mse: 1422.9788 - mae: 25.3950 - val_loss: 2389.2576 - val_mse: 2389.2576 - val_mae: 28.5914\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1260.5608 - mse: 1260.5608 - mae: 26.0487 - val_loss: 2279.4089 - val_mse: 2279.4089 - val_mae: 26.8572\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1170.5356 - mse: 1170.5356 - mae: 24.1944 - val_loss: 2277.5305 - val_mse: 2277.5305 - val_mae: 25.7501\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1109.5582 - mse: 1109.5582 - mae: 21.5654 - val_loss: 2320.1631 - val_mse: 2320.1631 - val_mae: 24.6473\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 983.9847 - mse: 983.9847 - mae: 20.0676 - val_loss: 1990.2196 - val_mse: 1990.2196 - val_mae: 23.8416\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 938.4720 - mse: 938.4720 - mae: 20.2545 - val_loss: 2084.8760 - val_mse: 2084.8760 - val_mae: 22.9955\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 935.1330 - mse: 935.1330 - mae: 18.7645 - val_loss: 2174.5730 - val_mse: 2174.5730 - val_mae: 22.7771\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 875.3568 - mse: 875.3568 - mae: 18.5097 - val_loss: 1913.2899 - val_mse: 1913.2899 - val_mae: 22.7490\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 860.4832 - mse: 860.4832 - mae: 18.9142 - val_loss: 1866.9620 - val_mse: 1866.9620 - val_mae: 22.6985\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 867.9652 - mse: 867.9652 - mae: 19.0648 - val_loss: 1783.6805 - val_mse: 1783.6805 - val_mae: 22.7787\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 829.3391 - mse: 829.3391 - mae: 18.5197 - val_loss: 1938.6086 - val_mse: 1938.6086 - val_mae: 23.1425\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 820.4700 - mse: 820.4700 - mae: 17.9207 - val_loss: 1989.2295 - val_mse: 1989.2295 - val_mae: 22.7462\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 860.4428 - mse: 860.4428 - mae: 18.9237 - val_loss: 1725.6246 - val_mse: 1725.6246 - val_mae: 22.5452\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 791.0128 - mse: 791.0128 - mae: 17.9202 - val_loss: 2026.2943 - val_mse: 2026.2943 - val_mae: 22.5327\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 815.7823 - mse: 815.7823 - mae: 17.6674 - val_loss: 1860.7936 - val_mse: 1860.7936 - val_mae: 22.1583\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 824.8693 - mse: 824.8693 - mae: 17.6105 - val_loss: 1995.4989 - val_mse: 1995.4989 - val_mae: 22.0014\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 937.5154 - mse: 937.5154 - mae: 18.9764 - val_loss: 1604.7306 - val_mse: 1604.7306 - val_mae: 22.2431\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 806.1074 - mse: 806.1074 - mae: 18.1159 - val_loss: 1919.8845 - val_mse: 1919.8845 - val_mae: 22.7828\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 782.2870 - mse: 782.2870 - mae: 17.5828 - val_loss: 1802.9951 - val_mse: 1802.9951 - val_mae: 21.8896\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 742.0997 - mse: 742.0997 - mae: 17.2765 - val_loss: 1665.6074 - val_mse: 1665.6074 - val_mae: 22.0148\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 779.6287 - mse: 779.6287 - mae: 18.2025 - val_loss: 1573.6974 - val_mse: 1573.6974 - val_mae: 21.7760\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 752.2056 - mse: 752.2056 - mae: 17.4355 - val_loss: 1911.7830 - val_mse: 1911.7830 - val_mae: 22.9180\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 777.2083 - mse: 777.2083 - mae: 17.4223 - val_loss: 1725.9368 - val_mse: 1725.9366 - val_mae: 21.6333\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 726.6765 - mse: 726.6765 - mae: 17.0940 - val_loss: 1671.0333 - val_mse: 1671.0333 - val_mae: 21.5991\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 716.0719 - mse: 716.0719 - mae: 16.9745 - val_loss: 1754.2665 - val_mse: 1754.2665 - val_mae: 21.6054\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 718.2495 - mse: 718.2495 - mae: 16.9064 - val_loss: 1679.6205 - val_mse: 1679.6205 - val_mae: 21.2896\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 708.4658 - mse: 708.4658 - mae: 16.7556 - val_loss: 1747.2830 - val_mse: 1747.2830 - val_mae: 21.5065\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 721.3749 - mse: 721.3749 - mae: 16.8734 - val_loss: 1725.1360 - val_mse: 1725.1360 - val_mae: 21.4620\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 763.4334 - mse: 763.4334 - mae: 17.9100 - val_loss: 1529.8757 - val_mse: 1529.8757 - val_mae: 21.6451\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 707.0530 - mse: 707.0530 - mae: 16.6879 - val_loss: 1977.5181 - val_mse: 1977.5181 - val_mae: 22.8698\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 740.5376 - mse: 740.5376 - mae: 17.1374 - val_loss: 1763.5538 - val_mse: 1763.5538 - val_mae: 21.6180\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 720.9626 - mse: 720.9626 - mae: 17.2699 - val_loss: 1614.6882 - val_mse: 1614.6882 - val_mae: 21.4896\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 683.2253 - mse: 683.2253 - mae: 16.5296 - val_loss: 1896.0653 - val_mse: 1896.0653 - val_mae: 22.5446\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 703.4609 - mse: 703.4609 - mae: 16.5655 - val_loss: 1732.8605 - val_mse: 1732.8605 - val_mae: 21.4496\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 675.5455 - mse: 675.5455 - mae: 16.4051 - val_loss: 1684.8103 - val_mse: 1684.8104 - val_mae: 21.5414\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 691.0115 - mse: 691.0115 - mae: 16.5997 - val_loss: 1703.9655 - val_mse: 1703.9655 - val_mae: 21.3097\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 659.8327 - mse: 659.8327 - mae: 16.0882 - val_loss: 1740.8046 - val_mse: 1740.8046 - val_mae: 21.4079\n",
      "Epoch 47/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 662.4670 - mse: 662.4670 - mae: 16.2165 - val_loss: 1682.1021 - val_mse: 1682.1021 - val_mae: 21.1556\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 719.4745 - mse: 719.4745 - mae: 17.1262 - val_loss: 1601.3696 - val_mse: 1601.3696 - val_mae: 21.0862\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 708.0571 - mse: 708.0571 - mae: 16.6310 - val_loss: 2035.1841 - val_mse: 2035.1841 - val_mae: 22.9973\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 670.6763 - mse: 670.6763 - mae: 16.1693 - val_loss: 1621.7108 - val_mse: 1621.7108 - val_mae: 21.5991\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 663.6181 - mse: 663.6181 - mae: 16.6580 - val_loss: 1635.0250 - val_mse: 1635.0250 - val_mae: 20.9875\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 662.9650 - mse: 662.9650 - mae: 16.0640 - val_loss: 1814.9524 - val_mse: 1814.9524 - val_mae: 21.9474\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 643.0673 - mse: 643.0673 - mae: 15.9241 - val_loss: 1558.1122 - val_mse: 1558.1119 - val_mae: 21.1925\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 663.7150 - mse: 663.7150 - mae: 16.4625 - val_loss: 1580.4510 - val_mse: 1580.4510 - val_mae: 20.8756\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 624.6114 - mse: 624.6114 - mae: 15.7077 - val_loss: 1879.8850 - val_mse: 1879.8850 - val_mae: 22.4232\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 633.0262 - mse: 633.0262 - mae: 15.5580 - val_loss: 1634.9473 - val_mse: 1634.9473 - val_mae: 21.3709\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 638.3030 - mse: 638.3030 - mae: 16.2628 - val_loss: 1641.6760 - val_mse: 1641.6760 - val_mae: 21.0204\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 618.4390 - mse: 618.4390 - mae: 15.6081 - val_loss: 1793.5256 - val_mse: 1793.5256 - val_mae: 21.6649\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.7366 - mse: 598.7366 - mae: 15.2071 - val_loss: 1620.0566 - val_mse: 1620.0566 - val_mae: 21.0784\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 608.9614 - mse: 608.9614 - mae: 15.7243 - val_loss: 1577.5670 - val_mse: 1577.5670 - val_mae: 20.8385\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 601.4000 - mse: 601.4000 - mae: 15.1785 - val_loss: 1632.0297 - val_mse: 1632.0295 - val_mae: 20.9105\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 605.3651 - mse: 605.3651 - mae: 15.0969 - val_loss: 1597.5974 - val_mse: 1597.5974 - val_mae: 20.7172\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 587.0883 - mse: 587.0883 - mae: 15.2872 - val_loss: 1628.5818 - val_mse: 1628.5815 - val_mae: 20.8673\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 585.2690 - mse: 585.2690 - mae: 14.8659 - val_loss: 1657.9641 - val_mse: 1657.9641 - val_mae: 20.7747\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 581.1804 - mse: 581.1804 - mae: 14.7546 - val_loss: 1647.4255 - val_mse: 1647.4255 - val_mae: 20.7583\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 561.1660 - mse: 561.1660 - mae: 14.7306 - val_loss: 1499.6534 - val_mse: 1499.6534 - val_mae: 20.5896\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 583.7809 - mse: 583.7809 - mae: 15.1689 - val_loss: 1620.5597 - val_mse: 1620.5597 - val_mae: 20.8237\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 564.9693 - mse: 564.9693 - mae: 14.6329 - val_loss: 1618.2649 - val_mse: 1618.2651 - val_mae: 20.9484\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.5165 - mse: 598.5165 - mae: 15.2156 - val_loss: 1515.0920 - val_mse: 1515.0920 - val_mae: 20.5316\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 553.3519 - mse: 553.3519 - mae: 14.3343 - val_loss: 1697.8003 - val_mse: 1697.8005 - val_mae: 21.3386\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 559.7812 - mse: 559.7812 - mae: 14.4254 - val_loss: 1506.5320 - val_mse: 1506.5320 - val_mae: 21.3022\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 574.8009 - mse: 574.8009 - mae: 15.1111 - val_loss: 1553.6327 - val_mse: 1553.6327 - val_mae: 20.5023\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 570.0215 - mse: 570.0215 - mae: 14.3059 - val_loss: 1592.5896 - val_mse: 1592.5896 - val_mae: 20.5964\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 548.5518 - mse: 548.5518 - mae: 14.3227 - val_loss: 1523.5190 - val_mse: 1523.5190 - val_mae: 20.4319\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 576.8538 - mse: 576.8538 - mae: 14.6229 - val_loss: 1577.1012 - val_mse: 1577.1012 - val_mae: 20.5271\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.2541 - mse: 509.2541 - mae: 13.9862 - val_loss: 1441.2123 - val_mse: 1441.2123 - val_mae: 20.4552\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 524.9542 - mse: 524.9542 - mae: 13.9799 - val_loss: 1633.1864 - val_mse: 1633.1864 - val_mae: 20.9023\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 510.1392 - mse: 510.1392 - mae: 13.7239 - val_loss: 1526.9062 - val_mse: 1526.9062 - val_mae: 20.5068\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.2072 - mse: 509.2072 - mae: 13.7892 - val_loss: 1518.5747 - val_mse: 1518.5747 - val_mae: 20.1115\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 499.0227 - mse: 499.0227 - mae: 13.4486 - val_loss: 1450.1434 - val_mse: 1450.1434 - val_mae: 19.9923\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 510.5941 - mse: 510.5941 - mae: 13.6913 - val_loss: 1469.2028 - val_mse: 1469.2028 - val_mae: 20.0499\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 525.5434 - mse: 525.5434 - mae: 13.9127 - val_loss: 1659.1576 - val_mse: 1659.1576 - val_mae: 20.9283\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 516.8218 - mse: 516.8218 - mae: 14.2147 - val_loss: 1516.9388 - val_mse: 1516.9388 - val_mae: 20.7324\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 515.4175 - mse: 515.4175 - mae: 14.0116 - val_loss: 1391.0247 - val_mse: 1391.0247 - val_mae: 19.8571\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.3760 - mse: 509.3760 - mae: 13.6977 - val_loss: 1604.2863 - val_mse: 1604.2863 - val_mae: 20.7669\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 480.0642 - mse: 480.0642 - mae: 13.9999 - val_loss: 1457.1315 - val_mse: 1457.1315 - val_mae: 20.0420\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 466.7029 - mse: 466.7029 - mae: 12.9017 - val_loss: 1430.3073 - val_mse: 1430.3073 - val_mae: 19.7290\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 462.2448 - mse: 462.2448 - mae: 12.6687 - val_loss: 1527.9497 - val_mse: 1527.9497 - val_mae: 19.8999\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 467.4203 - mse: 467.4203 - mae: 13.2293 - val_loss: 1524.1859 - val_mse: 1524.1859 - val_mae: 19.7245\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 463.3847 - mse: 463.3847 - mae: 12.8967 - val_loss: 1407.2604 - val_mse: 1407.2604 - val_mae: 19.4201\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 459.0889 - mse: 459.0889 - mae: 12.4311 - val_loss: 1428.0037 - val_mse: 1428.0038 - val_mae: 19.4624\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 495.9532 - mse: 495.9532 - mae: 13.3508 - val_loss: 1500.4948 - val_mse: 1500.4948 - val_mae: 20.0205\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 556.8461 - mse: 556.8461 - mae: 14.3599 - val_loss: 1333.2244 - val_mse: 1333.2244 - val_mae: 20.0254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 592.2530 - mse: 592.2530 - mae: 15.2938 - val_loss: 1692.2960 - val_mse: 1692.2960 - val_mae: 20.7265\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 476.0487 - mse: 476.0487 - mae: 15.2972 - val_loss: 1314.4304 - val_mse: 1314.4304 - val_mae: 20.0935\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 465.7708 - mse: 465.7708 - mae: 13.2021 - val_loss: 1617.0217 - val_mse: 1617.0217 - val_mae: 23.4037\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 477.4360 - mse: 477.4360 - mae: 14.3556 - val_loss: 1450.7463 - val_mse: 1450.7463 - val_mae: 19.6677\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 472.8438 - mse: 472.8438 - mae: 14.4607 - val_loss: 1335.8374 - val_mse: 1335.8374 - val_mae: 18.9941\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 529.7220 - mse: 529.7220 - mae: 14.5349 - val_loss: 1652.5209 - val_mse: 1652.5206 - val_mae: 20.5725\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 427.6349 - mse: 427.6349 - mae: 12.5979 - val_loss: 1371.5011 - val_mse: 1371.5011 - val_mae: 19.8862\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 424.8416 - mse: 424.8416 - mae: 12.1883 - val_loss: 1661.0637 - val_mse: 1661.0637 - val_mae: 20.4282\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 398.7818 - mse: 398.7818 - mae: 11.7269 - val_loss: 1426.9709 - val_mse: 1426.9712 - val_mae: 19.3141\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 497.0864 - mse: 497.0864 - mae: 12.9126 - val_loss: 1349.2990 - val_mse: 1349.2990 - val_mae: 19.0135\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 439.5041 - mse: 439.5041 - mae: 12.7293 - val_loss: 1439.7593 - val_mse: 1439.7593 - val_mae: 19.4073\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 376.9138 - mse: 376.9138 - mae: 12.2119 - val_loss: 1287.5347 - val_mse: 1287.5347 - val_mae: 18.9152\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 370.4074 - mse: 370.4074 - mae: 11.5700 - val_loss: 1410.7241 - val_mse: 1410.7242 - val_mae: 19.1335\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.8982 - mse: 367.8982 - mae: 11.0854 - val_loss: 1357.8334 - val_mse: 1357.8334 - val_mae: 18.7854\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 359.1835 - mse: 359.1835 - mae: 11.4992 - val_loss: 1384.6144 - val_mse: 1384.6144 - val_mae: 18.4093\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 338.0330 - mse: 338.0330 - mae: 10.4164 - val_loss: 1294.6567 - val_mse: 1294.6567 - val_mae: 17.9931\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 338.1082 - mse: 338.1082 - mae: 10.3348 - val_loss: 1335.7340 - val_mse: 1335.7340 - val_mae: 18.2001\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 350.4435 - mse: 350.4435 - mae: 11.2673 - val_loss: 1316.5404 - val_mse: 1316.5404 - val_mae: 18.0050\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 345.8544 - mse: 345.8544 - mae: 10.4753 - val_loss: 1217.8201 - val_mse: 1217.8201 - val_mae: 17.7552\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 322.0639 - mse: 322.0639 - mae: 10.5767 - val_loss: 1339.3125 - val_mse: 1339.3125 - val_mae: 18.2756\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 331.8631 - mse: 331.8631 - mae: 10.7689 - val_loss: 1297.3225 - val_mse: 1297.3225 - val_mae: 18.2657\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 329.2704 - mse: 329.2704 - mae: 10.6422 - val_loss: 1362.1179 - val_mse: 1362.1179 - val_mae: 18.6747\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 358.9697 - mse: 358.9697 - mae: 11.1860 - val_loss: 1452.3728 - val_mse: 1452.3728 - val_mae: 19.6450\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 377.6809 - mse: 377.6809 - mae: 11.6729 - val_loss: 1289.0215 - val_mse: 1289.0215 - val_mae: 19.3271\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 345.7779 - mse: 345.7779 - mae: 12.0417 - val_loss: 1285.6486 - val_mse: 1285.6486 - val_mae: 19.7226\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 315.6057 - mse: 315.6057 - mae: 11.4006 - val_loss: 1268.5618 - val_mse: 1268.5618 - val_mae: 18.6226\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 301.6426 - mse: 301.6426 - mae: 10.9163 - val_loss: 1281.9153 - val_mse: 1281.9153 - val_mae: 17.6252\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 278.4898 - mse: 278.4898 - mae: 9.5653 - val_loss: 1259.5641 - val_mse: 1259.5641 - val_mae: 17.8916\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.9932 - mse: 269.9932 - mae: 9.3522 - val_loss: 1251.0605 - val_mse: 1251.0605 - val_mae: 17.4753\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.6444 - mse: 269.6444 - mae: 9.5536 - val_loss: 1283.8658 - val_mse: 1283.8658 - val_mae: 17.6600\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 252.4458 - mse: 252.4458 - mae: 8.9035 - val_loss: 1138.8755 - val_mse: 1138.8755 - val_mae: 16.6830\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 276.9843 - mse: 276.9843 - mae: 9.1846 - val_loss: 1252.8289 - val_mse: 1252.8289 - val_mae: 17.4740\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.3214 - mse: 253.3214 - mae: 9.1546 - val_loss: 1217.1189 - val_mse: 1217.1189 - val_mae: 16.9469\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 248.4087 - mse: 248.4087 - mae: 8.7509 - val_loss: 1209.8307 - val_mse: 1209.8307 - val_mae: 16.8522\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 253.2396 - mse: 253.2396 - mae: 8.9829 - val_loss: 1228.9397 - val_mse: 1228.9397 - val_mae: 17.1058\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 267.6073 - mse: 267.6073 - mae: 9.1057 - val_loss: 1311.9449 - val_mse: 1311.9449 - val_mae: 18.3267\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 373.7514 - mse: 373.7514 - mae: 11.3799 - val_loss: 1284.8496 - val_mse: 1284.8496 - val_mae: 19.3833\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 289.5898 - mse: 289.5898 - mae: 11.0664 - val_loss: 1293.8220 - val_mse: 1293.8220 - val_mae: 20.7660\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.6347 - mse: 274.6347 - mae: 11.4102 - val_loss: 1322.2712 - val_mse: 1322.2712 - val_mae: 18.5929\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.5011 - mse: 253.5011 - mae: 10.2909 - val_loss: 1297.4532 - val_mse: 1297.4532 - val_mae: 17.9769\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 241.1999 - mse: 241.1999 - mae: 10.0724 - val_loss: 1197.5189 - val_mse: 1197.5189 - val_mae: 16.6775\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 242.5224 - mse: 242.5224 - mae: 8.9620 - val_loss: 1339.8555 - val_mse: 1339.8555 - val_mae: 18.1659\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 269.2992 - mse: 269.2992 - mae: 8.6873 - val_loss: 1074.9524 - val_mse: 1074.9524 - val_mae: 16.1204\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 198.3916 - mse: 198.3916 - mae: 7.8102 - val_loss: 1474.5374 - val_mse: 1474.5374 - val_mae: 17.8005\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220.0649 - mse: 220.0649 - mae: 8.4873 - val_loss: 1266.9137 - val_mse: 1266.9137 - val_mae: 16.5977\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.9702 - mse: 238.9702 - mae: 8.9489 - val_loss: 1173.9628 - val_mse: 1173.9628 - val_mae: 16.5234\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.6397 - mse: 208.6397 - mae: 8.1846 - val_loss: 1289.8552 - val_mse: 1289.8552 - val_mae: 18.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.6100 - mse: 274.6100 - mae: 9.8092 - val_loss: 1209.1731 - val_mse: 1209.1731 - val_mae: 18.0757\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 309.4959 - mse: 309.4959 - mae: 11.2764 - val_loss: 1493.9059 - val_mse: 1493.9059 - val_mae: 22.3018\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 285.1939 - mse: 285.1939 - mae: 12.1916 - val_loss: 1111.7720 - val_mse: 1111.7720 - val_mae: 17.3533\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 242.8343 - mse: 242.8343 - mae: 10.4221 - val_loss: 1434.8976 - val_mse: 1434.8976 - val_mae: 19.5687\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 219.6290 - mse: 219.6290 - mae: 9.2210 - val_loss: 1108.0543 - val_mse: 1108.0543 - val_mae: 16.7052\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 242.8277 - mse: 242.8277 - mae: 9.1768 - val_loss: 1494.5984 - val_mse: 1494.5984 - val_mae: 19.8820\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 300.8167 - mse: 300.8167 - mae: 11.1258 - val_loss: 1155.2290 - val_mse: 1155.2290 - val_mae: 18.8445\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 317.7715 - mse: 317.7715 - mae: 11.0694 - val_loss: 1725.5894 - val_mse: 1725.5894 - val_mae: 23.8321\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 282.9625 - mse: 282.9625 - mae: 11.4719 - val_loss: 1236.4241 - val_mse: 1236.4241 - val_mae: 18.2603\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 227.1354 - mse: 227.1354 - mae: 10.3693 - val_loss: 1384.1769 - val_mse: 1384.1768 - val_mae: 17.8626\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 259.4191 - mse: 259.4191 - mae: 11.2466 - val_loss: 1229.0800 - val_mse: 1229.0800 - val_mae: 16.0624\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 296.9008 - mse: 296.9008 - mae: 9.9390 - val_loss: 1343.8387 - val_mse: 1343.8387 - val_mae: 17.8885\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 225.1518 - mse: 225.1518 - mae: 9.0780 - val_loss: 1185.9827 - val_mse: 1185.9827 - val_mae: 16.2038\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 187.6696 - mse: 187.6696 - mae: 7.7383 - val_loss: 1223.7793 - val_mse: 1223.7793 - val_mae: 16.4333\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 151.8284 - mse: 151.8284 - mae: 7.1768 - val_loss: 1107.0538 - val_mse: 1107.0538 - val_mae: 15.4606\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 168.1811 - mse: 168.1811 - mae: 6.7581 - val_loss: 1259.3495 - val_mse: 1259.3495 - val_mae: 16.6870\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 141.3398 - mse: 141.3398 - mae: 6.3194 - val_loss: 1289.9286 - val_mse: 1289.9286 - val_mae: 16.9513\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 133.9547 - mse: 133.9547 - mae: 6.2696 - val_loss: 1206.6462 - val_mse: 1206.6462 - val_mae: 16.0241\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 130.9464 - mse: 130.9464 - mae: 6.1947 - val_loss: 1209.8831 - val_mse: 1209.8831 - val_mae: 16.1189\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 136.5789 - mse: 136.5789 - mae: 6.0746 - val_loss: 1300.3201 - val_mse: 1300.3201 - val_mae: 16.9093\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 131.0776 - mse: 131.0776 - mae: 6.5541 - val_loss: 1218.8113 - val_mse: 1218.8113 - val_mae: 16.1698\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 152.1741 - mse: 152.1741 - mae: 6.5500 - val_loss: 1288.7234 - val_mse: 1288.7234 - val_mae: 16.6617\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 153.6440 - mse: 153.6440 - mae: 6.6616 - val_loss: 1169.3342 - val_mse: 1169.3342 - val_mae: 16.2385\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 216.4862 - mse: 216.4862 - mae: 8.4913 - val_loss: 1312.4983 - val_mse: 1312.4983 - val_mae: 16.7604\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 203.7951 - mse: 203.7951 - mae: 9.2400 - val_loss: 1116.7336 - val_mse: 1116.7336 - val_mae: 16.8369\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 161.8746 - mse: 161.8746 - mae: 9.3857 - val_loss: 1262.9113 - val_mse: 1262.9113 - val_mae: 16.4231\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 138.5137 - mse: 138.5137 - mae: 7.6067 - val_loss: 1214.1615 - val_mse: 1214.1615 - val_mae: 15.6370\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 120.6945 - mse: 120.6945 - mae: 6.3799 - val_loss: 1194.1295 - val_mse: 1194.1295 - val_mae: 16.5413\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 110.7886 - mse: 110.7886 - mae: 6.2829 - val_loss: 1308.8704 - val_mse: 1308.8704 - val_mae: 16.0238\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 149.8142 - mse: 149.8142 - mae: 6.7590 - val_loss: 1083.6898 - val_mse: 1083.6898 - val_mae: 14.9629\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 128.3475 - mse: 128.3475 - mae: 6.1792 - val_loss: 1248.1436 - val_mse: 1248.1436 - val_mae: 16.8831\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 119.1671 - mse: 119.1671 - mae: 6.7388 - val_loss: 1209.0211 - val_mse: 1209.0211 - val_mae: 15.7995\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.7216 - mse: 98.7216 - mae: 5.7524 - val_loss: 1370.4929 - val_mse: 1370.4930 - val_mae: 16.8469\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 98.3726 - mse: 98.3726 - mae: 5.8041 - val_loss: 1304.2075 - val_mse: 1304.2075 - val_mae: 16.0819\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 91.7146 - mse: 91.7146 - mae: 5.4934 - val_loss: 1248.8514 - val_mse: 1248.8514 - val_mae: 15.5461\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85.2046 - mse: 85.2046 - mae: 4.9441 - val_loss: 1165.1669 - val_mse: 1165.1669 - val_mae: 16.1229\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 120.0590 - mse: 120.0590 - mae: 6.6034 - val_loss: 1149.0038 - val_mse: 1149.0038 - val_mae: 15.0265\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 113.3512 - mse: 113.3512 - mae: 6.6272 - val_loss: 1261.1698 - val_mse: 1261.1698 - val_mae: 17.1211\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 96.1530 - mse: 96.1530 - mae: 6.1695 - val_loss: 1262.7688 - val_mse: 1262.7688 - val_mae: 15.4433\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 81.6495 - mse: 81.6495 - mae: 5.3266 - val_loss: 1288.8436 - val_mse: 1288.8436 - val_mae: 15.9579\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74.8687 - mse: 74.8687 - mae: 4.8298 - val_loss: 1262.9775 - val_mse: 1262.9775 - val_mae: 15.4979\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 74.6652 - mse: 74.6652 - mae: 4.9997 - val_loss: 1151.9581 - val_mse: 1151.9581 - val_mae: 14.7614\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72.7906 - mse: 72.7906 - mae: 4.6544 - val_loss: 1266.1946 - val_mse: 1266.1946 - val_mae: 16.1997\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 66.7472 - mse: 66.7472 - mae: 4.6321 - val_loss: 1215.6635 - val_mse: 1215.6635 - val_mae: 15.0621\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 65.3595 - mse: 65.3595 - mae: 4.5532 - val_loss: 1319.3048 - val_mse: 1319.3048 - val_mae: 15.6806\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 60.9807 - mse: 60.9807 - mae: 4.4862 - val_loss: 1180.9092 - val_mse: 1180.9092 - val_mae: 14.5589\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 63.3422 - mse: 63.3422 - mae: 4.3340 - val_loss: 1253.3932 - val_mse: 1253.3932 - val_mae: 15.5497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 58.9077 - mse: 58.9077 - mae: 4.1499 - val_loss: 1325.6284 - val_mse: 1325.6285 - val_mae: 15.7013\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72.3660 - mse: 72.3660 - mae: 4.7497 - val_loss: 1194.0609 - val_mse: 1194.0609 - val_mae: 14.5183\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 69.0039 - mse: 69.0039 - mae: 4.8045 - val_loss: 1287.8329 - val_mse: 1287.8329 - val_mae: 15.0318\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 64.0378 - mse: 64.0378 - mae: 4.3310 - val_loss: 1468.0692 - val_mse: 1468.0692 - val_mae: 17.0682\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 87.4297 - mse: 87.4297 - mae: 5.8095 - val_loss: 1109.7913 - val_mse: 1109.7913 - val_mae: 14.6171\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 54.1080 - mse: 54.1080 - mae: 4.5322 - val_loss: 1485.3882 - val_mse: 1485.3882 - val_mae: 17.2763\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 78.0534 - mse: 78.0534 - mae: 5.8796 - val_loss: 1196.2974 - val_mse: 1196.2974 - val_mae: 14.7216\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 73.8517 - mse: 73.8517 - mae: 5.9279 - val_loss: 1483.8392 - val_mse: 1483.8392 - val_mae: 17.3616\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 67.0592 - mse: 67.0592 - mae: 5.4668 - val_loss: 1279.3258 - val_mse: 1279.3258 - val_mae: 15.1136\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49.1338 - mse: 49.1338 - mae: 4.5040 - val_loss: 1241.5988 - val_mse: 1241.5988 - val_mae: 14.9301\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 47.3509 - mse: 47.3509 - mae: 4.0115 - val_loss: 1315.9781 - val_mse: 1315.9781 - val_mae: 15.2516\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 45.0501 - mse: 45.0501 - mae: 3.8733 - val_loss: 1244.5371 - val_mse: 1244.5371 - val_mae: 14.8449\n",
      "Epoch 200/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.0546 - mse: 41.0546 - mae: 3.6975 - val_loss: 1331.6224 - val_mse: 1331.6223 - val_mae: 15.4119\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 42.8117 - mse: 42.8117 - mae: 3.8026 - val_loss: 1278.3064 - val_mse: 1278.3064 - val_mae: 14.8489\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 39.8968 - mse: 39.8968 - mae: 3.7108 - val_loss: 1161.2231 - val_mse: 1161.2231 - val_mae: 14.2872\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.4597 - mse: 41.4597 - mae: 3.5444 - val_loss: 1379.2804 - val_mse: 1379.2805 - val_mae: 15.5496\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.4412 - mse: 41.4412 - mae: 3.7934 - val_loss: 1199.9536 - val_mse: 1199.9536 - val_mae: 14.2474\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.2480 - mse: 38.2480 - mae: 3.7085 - val_loss: 1299.3926 - val_mse: 1299.3926 - val_mae: 14.9486\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.3329 - mse: 37.3329 - mae: 3.7436 - val_loss: 1366.2509 - val_mse: 1366.2509 - val_mae: 15.3725\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.0101 - mse: 34.0101 - mae: 3.4659 - val_loss: 1211.5684 - val_mse: 1211.5684 - val_mae: 14.3314\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 40.3568 - mse: 40.3568 - mae: 3.7008 - val_loss: 1385.9945 - val_mse: 1385.9944 - val_mae: 15.7685\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.2391 - mse: 41.2391 - mae: 4.0207 - val_loss: 1233.8795 - val_mse: 1233.8795 - val_mae: 14.0552\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.9798 - mse: 35.9798 - mae: 3.8854 - val_loss: 1284.2057 - val_mse: 1284.2057 - val_mae: 15.0100\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.0711 - mse: 32.0711 - mae: 3.5829 - val_loss: 1304.7175 - val_mse: 1304.7175 - val_mae: 14.8410\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.4143 - mse: 32.4143 - mae: 3.5570 - val_loss: 1187.0985 - val_mse: 1187.0985 - val_mae: 13.9467\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 36.4955 - mse: 36.4955 - mae: 3.4517 - val_loss: 1312.1072 - val_mse: 1312.1072 - val_mae: 14.9186\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.3731 - mse: 31.3731 - mae: 3.3014 - val_loss: 1254.3282 - val_mse: 1254.3282 - val_mae: 14.6223\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.7155 - mse: 30.7155 - mae: 3.3056 - val_loss: 1329.0790 - val_mse: 1329.0790 - val_mae: 14.8101\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.1763 - mse: 35.1763 - mae: 3.5431 - val_loss: 1065.8855 - val_mse: 1065.8855 - val_mae: 14.3770\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 59.8644 - mse: 59.8644 - mae: 4.9114 - val_loss: 1397.1809 - val_mse: 1397.1809 - val_mae: 16.2819\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 43.5347 - mse: 43.5347 - mae: 4.8184 - val_loss: 1256.8483 - val_mse: 1256.8483 - val_mae: 14.7856\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 49.2046 - mse: 49.2046 - mae: 5.0947 - val_loss: 1247.7845 - val_mse: 1247.7845 - val_mae: 14.4226\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.3203 - mse: 48.3203 - mae: 4.3747 - val_loss: 1384.1300 - val_mse: 1384.1300 - val_mae: 15.7461\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 55.9068 - mse: 55.9068 - mae: 4.8866 - val_loss: 1048.2743 - val_mse: 1048.2743 - val_mae: 14.6807\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 72.1477 - mse: 72.1477 - mae: 5.6352 - val_loss: 1328.0803 - val_mse: 1328.0803 - val_mae: 15.2146\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 63.6935 - mse: 63.6935 - mae: 5.3316 - val_loss: 1356.4365 - val_mse: 1356.4366 - val_mae: 15.6937\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 52.3367 - mse: 52.3367 - mae: 5.1618 - val_loss: 1171.2439 - val_mse: 1171.2439 - val_mae: 14.3846\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.5421 - mse: 38.5421 - mae: 4.2500 - val_loss: 1654.7784 - val_mse: 1654.7784 - val_mae: 16.7875\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 46.2272 - mse: 46.2272 - mae: 4.1780 - val_loss: 1076.2684 - val_mse: 1076.2684 - val_mae: 14.5967\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 59.5356 - mse: 59.5356 - mae: 4.9555 - val_loss: 1448.9742 - val_mse: 1448.9742 - val_mae: 15.6444\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 39.7793 - mse: 39.7793 - mae: 4.4193 - val_loss: 1201.7955 - val_mse: 1201.7955 - val_mae: 13.6974\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.1812 - mse: 29.1812 - mae: 3.9344 - val_loss: 1322.6135 - val_mse: 1322.6135 - val_mae: 14.7585\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.0909 - mse: 31.0909 - mae: 3.4733 - val_loss: 1281.4509 - val_mse: 1281.4509 - val_mae: 14.8721\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 31.1155 - mse: 31.1155 - mae: 3.5839 - val_loss: 1056.8505 - val_mse: 1056.8505 - val_mae: 13.6537\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 27.1089 - mse: 27.1089 - mae: 3.1239 - val_loss: 1349.4871 - val_mse: 1349.4871 - val_mae: 15.0300\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.1241 - mse: 25.1241 - mae: 3.1126 - val_loss: 1192.7089 - val_mse: 1192.7089 - val_mae: 13.7735\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.8548 - mse: 23.8548 - mae: 3.1752 - val_loss: 1209.4553 - val_mse: 1209.4553 - val_mae: 14.2819\n",
      "Epoch 235/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 21.3600 - mse: 21.3600 - mae: 2.8474 - val_loss: 1211.9736 - val_mse: 1211.9736 - val_mae: 13.9735\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 18.8075 - mse: 18.8075 - mae: 2.4997 - val_loss: 1205.1888 - val_mse: 1205.1888 - val_mae: 13.9366\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.4391 - mse: 17.4391 - mae: 2.4388 - val_loss: 1216.1156 - val_mse: 1216.1156 - val_mae: 14.0223\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.9103 - mse: 17.9103 - mae: 2.4657 - val_loss: 1263.0615 - val_mse: 1263.0615 - val_mae: 14.4132\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.4919 - mse: 18.4919 - mae: 2.5491 - val_loss: 1197.2389 - val_mse: 1197.2389 - val_mae: 13.8313\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.8194 - mse: 20.8194 - mae: 2.8355 - val_loss: 1151.1561 - val_mse: 1151.1561 - val_mae: 13.6346\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.7554 - mse: 19.7554 - mae: 2.6235 - val_loss: 1326.1406 - val_mse: 1326.1406 - val_mae: 14.9087\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.1589 - mse: 21.1589 - mae: 2.5118 - val_loss: 1232.7720 - val_mse: 1232.7720 - val_mae: 13.9353\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.9485 - mse: 21.9485 - mae: 2.7576 - val_loss: 1162.1177 - val_mse: 1162.1177 - val_mae: 13.9977\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.7310 - mse: 23.7310 - mae: 3.1492 - val_loss: 1330.7556 - val_mse: 1330.7556 - val_mae: 14.6222\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.9339 - mse: 21.9339 - mae: 3.0830 - val_loss: 1249.8784 - val_mse: 1249.8784 - val_mae: 14.1029\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.5027 - mse: 22.5027 - mae: 2.9651 - val_loss: 1120.4797 - val_mse: 1120.4797 - val_mae: 13.6968\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.0311 - mse: 25.0311 - mae: 2.9684 - val_loss: 1077.1262 - val_mse: 1077.1262 - val_mae: 13.2038\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.6247 - mse: 20.6247 - mae: 2.6088 - val_loss: 1197.9570 - val_mse: 1197.9570 - val_mae: 13.9619\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.0242 - mse: 18.0242 - mae: 2.5410 - val_loss: 1285.7588 - val_mse: 1285.7588 - val_mae: 14.1761\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.9613 - mse: 19.9613 - mae: 2.6907 - val_loss: 1032.1437 - val_mse: 1032.1437 - val_mae: 13.5448\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.7461 - mse: 20.7461 - mae: 2.8185 - val_loss: 1397.0209 - val_mse: 1397.0209 - val_mae: 15.2994\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.6696 - mse: 24.6696 - mae: 3.2117 - val_loss: 1161.9927 - val_mse: 1161.9927 - val_mae: 13.5724\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.2481 - mse: 20.2481 - mae: 2.9608 - val_loss: 1221.9797 - val_mse: 1221.9797 - val_mae: 13.7637\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.4864 - mse: 18.4864 - mae: 2.6645 - val_loss: 1210.7760 - val_mse: 1210.7760 - val_mae: 13.8142\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.0463 - mse: 21.0463 - mae: 2.7832 - val_loss: 1303.0165 - val_mse: 1303.0166 - val_mae: 14.9426\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 35.4592 - mse: 35.4592 - mae: 3.7721 - val_loss: 1045.6970 - val_mse: 1045.6970 - val_mae: 13.6843\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.4636 - mse: 21.4636 - mae: 3.3225 - val_loss: 1299.3950 - val_mse: 1299.3950 - val_mae: 15.1985\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.3145 - mse: 20.3145 - mae: 3.4717 - val_loss: 1182.5874 - val_mse: 1182.5874 - val_mae: 13.5240\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.4651 - mse: 17.4651 - mae: 2.9049 - val_loss: 1188.8568 - val_mse: 1188.8568 - val_mae: 13.8021\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.7869 - mse: 14.7869 - mae: 2.4757 - val_loss: 1261.6820 - val_mse: 1261.6820 - val_mae: 14.1436\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.9135 - mse: 12.9135 - mae: 2.2354 - val_loss: 1079.5223 - val_mse: 1079.5223 - val_mae: 13.6136\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.1082 - mse: 16.1082 - mae: 2.2714 - val_loss: 1232.0858 - val_mse: 1232.0858 - val_mae: 13.7902\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 13.6168 - mse: 13.6168 - mae: 2.2335 - val_loss: 1210.4634 - val_mse: 1210.4634 - val_mae: 13.6827\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11.9270 - mse: 11.9270 - mae: 2.1235 - val_loss: 1281.2136 - val_mse: 1281.2137 - val_mae: 14.4181\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 12.9707 - mse: 12.9707 - mae: 2.1602 - val_loss: 1151.4406 - val_mse: 1151.4406 - val_mae: 13.5380\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11.2085 - mse: 11.2085 - mae: 1.9548 - val_loss: 1194.3918 - val_mse: 1194.3918 - val_mae: 13.6319\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.8019 - mse: 10.8019 - mae: 1.8572 - val_loss: 1241.4623 - val_mse: 1241.4623 - val_mae: 14.0292\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.6717 - mse: 10.6717 - mae: 1.9247 - val_loss: 1123.4752 - val_mse: 1123.4752 - val_mae: 13.3234\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0449 - mse: 11.0449 - mae: 1.9015 - val_loss: 1149.4565 - val_mse: 1149.4565 - val_mae: 13.5056\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.9131 - mse: 10.9131 - mae: 1.8235 - val_loss: 1216.4485 - val_mse: 1216.4485 - val_mae: 13.8590\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6905 - mse: 9.6905 - mae: 1.7241 - val_loss: 1143.4397 - val_mse: 1143.4397 - val_mae: 13.4545\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.2824 - mse: 13.2824 - mae: 2.1771 - val_loss: 1167.0100 - val_mse: 1167.0100 - val_mae: 13.5066\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1404 - mse: 13.1404 - mae: 2.1503 - val_loss: 1315.7019 - val_mse: 1315.7019 - val_mae: 14.5378\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.7778 - mse: 15.7778 - mae: 2.2832 - val_loss: 1210.0831 - val_mse: 1210.0831 - val_mae: 13.8387\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.4993 - mse: 14.4993 - mae: 2.1658 - val_loss: 1128.4756 - val_mse: 1128.4756 - val_mae: 13.2485\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.9084 - mse: 13.9084 - mae: 2.4260 - val_loss: 1327.6792 - val_mse: 1327.6791 - val_mae: 15.1721\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.9041 - mse: 19.9041 - mae: 3.0476 - val_loss: 1176.7052 - val_mse: 1176.7052 - val_mae: 13.3842\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.9021 - mse: 13.9021 - mae: 2.6048 - val_loss: 1131.7643 - val_mse: 1131.7643 - val_mae: 13.5519\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.9996 - mse: 11.9996 - mae: 2.2546 - val_loss: 1301.6858 - val_mse: 1301.6858 - val_mae: 14.2788\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6148 - mse: 11.6148 - mae: 2.0266 - val_loss: 1120.6599 - val_mse: 1120.6599 - val_mae: 13.6233\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8377 - mse: 11.8377 - mae: 1.9856 - val_loss: 1177.8014 - val_mse: 1177.8014 - val_mae: 13.5161\n",
      "Epoch 282/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7232 - mse: 10.7232 - mae: 2.0711 - val_loss: 1241.4380 - val_mse: 1241.4380 - val_mae: 13.8132\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.2911 - mse: 10.2911 - mae: 1.9944 - val_loss: 1221.5880 - val_mse: 1221.5880 - val_mae: 13.6664\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.0599 - mse: 10.0599 - mae: 1.8510 - val_loss: 1108.3898 - val_mse: 1108.3898 - val_mae: 13.3914\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.5641 - mse: 12.5641 - mae: 2.2114 - val_loss: 1212.5546 - val_mse: 1212.5546 - val_mae: 13.5785\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7708 - mse: 9.7708 - mae: 1.7824 - val_loss: 1214.8911 - val_mse: 1214.8911 - val_mae: 13.6714\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8694 - mse: 8.8694 - mae: 1.6566 - val_loss: 1191.7991 - val_mse: 1191.7991 - val_mae: 13.5734\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5667 - mse: 8.5667 - mae: 1.6368 - val_loss: 1136.8831 - val_mse: 1136.8831 - val_mae: 13.3001\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.1647 - mse: 8.1647 - mae: 1.6130 - val_loss: 1224.3762 - val_mse: 1224.3762 - val_mae: 13.8399\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6717 - mse: 7.6717 - mae: 1.5223 - val_loss: 1215.3422 - val_mse: 1215.3422 - val_mae: 13.6314\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8549 - mse: 8.8549 - mae: 1.7190 - val_loss: 1178.3623 - val_mse: 1178.3623 - val_mae: 13.3629\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7294 - mse: 8.7294 - mae: 1.6411 - val_loss: 1184.3699 - val_mse: 1184.3699 - val_mae: 13.4637\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.1463 - mse: 8.1463 - mae: 1.5501 - val_loss: 1188.0054 - val_mse: 1188.0054 - val_mae: 13.5288\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8204 - mse: 7.8204 - mae: 1.6116 - val_loss: 1193.2467 - val_mse: 1193.2467 - val_mae: 13.5085\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3165 - mse: 8.3165 - mae: 1.6437 - val_loss: 1199.5676 - val_mse: 1199.5676 - val_mae: 13.6047\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4503 - mse: 7.4503 - mae: 1.6445 - val_loss: 1154.8606 - val_mse: 1154.8606 - val_mae: 13.2122\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.4973 - mse: 8.4973 - mae: 1.7041 - val_loss: 1158.0406 - val_mse: 1158.0406 - val_mae: 13.3181\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6669 - mse: 11.6669 - mae: 2.0531 - val_loss: 1231.3064 - val_mse: 1231.3064 - val_mae: 13.5536\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.9094 - mse: 9.9094 - mae: 1.8166 - val_loss: 1261.3866 - val_mse: 1261.3866 - val_mae: 14.0326\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.2088 - mse: 14.2088 - mae: 2.3023 - val_loss: 1192.8530 - val_mse: 1192.8530 - val_mae: 13.7349\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.7792 - mse: 9.7792 - mae: 2.0215 - val_loss: 1200.5323 - val_mse: 1200.5323 - val_mae: 13.1511\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.6898 - mse: 7.6898 - mae: 1.7162 - val_loss: 1214.0883 - val_mse: 1214.0883 - val_mae: 13.6223\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3484 - mse: 6.3484 - mae: 1.4703 - val_loss: 1223.8452 - val_mse: 1223.8452 - val_mae: 13.5503\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.4309 - mse: 7.4309 - mae: 1.4946 - val_loss: 1167.4319 - val_mse: 1167.4319 - val_mae: 13.2502\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5816 - mse: 6.5816 - mae: 1.5069 - val_loss: 1186.4941 - val_mse: 1186.4941 - val_mae: 13.3496\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.1295 - mse: 6.1295 - mae: 1.4245 - val_loss: 1226.4347 - val_mse: 1226.4347 - val_mae: 13.5557\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9278 - mse: 5.9278 - mae: 1.3525 - val_loss: 1203.3669 - val_mse: 1203.3669 - val_mae: 13.2537\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6511 - mse: 6.6511 - mae: 1.4360 - val_loss: 1173.6780 - val_mse: 1173.6780 - val_mae: 13.3738\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8624 - mse: 6.8624 - mae: 1.5201 - val_loss: 1304.1588 - val_mse: 1304.1588 - val_mae: 14.0713\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5635 - mse: 6.5635 - mae: 1.5920 - val_loss: 1248.9581 - val_mse: 1248.9581 - val_mae: 13.7249\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3157 - mse: 7.3157 - mae: 1.7096 - val_loss: 1189.9084 - val_mse: 1189.9084 - val_mae: 13.3267\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7887 - mse: 6.7887 - mae: 1.5795 - val_loss: 1175.0786 - val_mse: 1175.0786 - val_mae: 13.3363\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.3852 - mse: 6.3852 - mae: 1.4119 - val_loss: 1327.6017 - val_mse: 1327.6017 - val_mae: 14.0919\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7683 - mse: 8.7683 - mae: 1.6886 - val_loss: 1209.1143 - val_mse: 1209.1143 - val_mae: 13.5894\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4178 - mse: 7.4178 - mae: 1.7043 - val_loss: 1222.6526 - val_mse: 1222.6526 - val_mae: 13.5945\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6715 - mse: 5.6715 - mae: 1.3195 - val_loss: 1136.2162 - val_mse: 1136.2162 - val_mae: 13.1439\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.3589 - mse: 6.3589 - mae: 1.3438 - val_loss: 1248.9827 - val_mse: 1248.9827 - val_mae: 13.7429\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.8380 - mse: 5.8380 - mae: 1.3599 - val_loss: 1168.6979 - val_mse: 1168.6979 - val_mae: 13.3797\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8037 - mse: 5.8037 - mae: 1.3583 - val_loss: 1298.9646 - val_mse: 1298.9647 - val_mae: 13.9039\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.8527 - mse: 6.8527 - mae: 1.5616 - val_loss: 1216.8328 - val_mse: 1216.8328 - val_mae: 13.5904\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8253 - mse: 6.8253 - mae: 1.5174 - val_loss: 1230.4900 - val_mse: 1230.4900 - val_mae: 13.6130\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.9827 - mse: 6.9827 - mae: 1.5186 - val_loss: 1180.9607 - val_mse: 1180.9607 - val_mae: 13.1656\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4352 - mse: 5.4352 - mae: 1.2675 - val_loss: 1191.2926 - val_mse: 1191.2926 - val_mae: 13.4235\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9160 - mse: 4.9160 - mae: 1.2745 - val_loss: 1187.7489 - val_mse: 1187.7489 - val_mae: 13.3813\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8931 - mse: 5.8931 - mae: 1.3768 - val_loss: 1251.6726 - val_mse: 1251.6726 - val_mae: 13.5767\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9396 - mse: 5.9396 - mae: 1.3593 - val_loss: 1170.8737 - val_mse: 1170.8737 - val_mae: 13.2683\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.0592 - mse: 5.0592 - mae: 1.2533 - val_loss: 1209.5925 - val_mse: 1209.5925 - val_mae: 13.3854\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3326 - mse: 4.3326 - mae: 1.0841 - val_loss: 1209.1089 - val_mse: 1209.1089 - val_mae: 13.3704\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7718 - mse: 4.7718 - mae: 1.2300 - val_loss: 1212.2642 - val_mse: 1212.2642 - val_mae: 13.2883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2865 - mse: 4.2865 - mae: 1.0467 - val_loss: 1238.4146 - val_mse: 1238.4146 - val_mae: 13.4329\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5315 - mse: 4.5315 - mae: 1.0850 - val_loss: 1227.0815 - val_mse: 1227.0815 - val_mae: 13.4756\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4638 - mse: 5.4638 - mae: 1.2214 - val_loss: 1337.2479 - val_mse: 1337.2479 - val_mae: 14.2012\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.2679 - mse: 13.2679 - mae: 2.1838 - val_loss: 1153.8445 - val_mse: 1153.8445 - val_mae: 13.5435\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0631 - mse: 11.0631 - mae: 2.2617 - val_loss: 1171.9691 - val_mse: 1171.9691 - val_mae: 13.1740\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.3392 - mse: 9.3392 - mae: 2.2073 - val_loss: 1315.6429 - val_mse: 1315.6429 - val_mae: 14.1304\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.1672 - mse: 11.1672 - mae: 2.2559 - val_loss: 1440.8721 - val_mse: 1440.8721 - val_mae: 15.7200\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.4060 - mse: 24.4060 - mae: 3.4421 - val_loss: 1110.1278 - val_mse: 1110.1278 - val_mae: 13.2554\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.5838 - mse: 24.5838 - mae: 3.3826 - val_loss: 1145.7117 - val_mse: 1145.7117 - val_mae: 13.7182\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.6988 - mse: 12.6988 - mae: 2.3883 - val_loss: 1137.6754 - val_mse: 1137.6754 - val_mae: 13.2961\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8699 - mse: 7.8699 - mae: 1.7959 - val_loss: 1197.8988 - val_mse: 1197.8988 - val_mae: 13.3475\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0673 - mse: 7.0673 - mae: 1.5775 - val_loss: 1230.8989 - val_mse: 1230.8989 - val_mae: 13.8127\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.7546 - mse: 7.7546 - mae: 1.8412 - val_loss: 1145.9950 - val_mse: 1145.9950 - val_mae: 13.2074\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0008 - mse: 6.0008 - mae: 1.5982 - val_loss: 1143.3555 - val_mse: 1143.3555 - val_mae: 13.0942\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4206 - mse: 9.4206 - mae: 2.0462 - val_loss: 1191.4766 - val_mse: 1191.4766 - val_mae: 13.4739\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5830 - mse: 6.5830 - mae: 1.7250 - val_loss: 1343.3842 - val_mse: 1343.3842 - val_mae: 13.9832\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5969 - mse: 5.5969 - mae: 1.5220 - val_loss: 1279.7606 - val_mse: 1279.7606 - val_mae: 13.4717\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3636 - mse: 5.3636 - mae: 1.3730 - val_loss: 1121.2069 - val_mse: 1121.2069 - val_mae: 13.1930\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0933 - mse: 7.0933 - mae: 1.6403 - val_loss: 1118.5182 - val_mse: 1118.5182 - val_mae: 13.3586\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7680 - mse: 6.7680 - mae: 1.4964 - val_loss: 1170.4253 - val_mse: 1170.4253 - val_mae: 13.1439\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4383 - mse: 4.4383 - mae: 1.2413 - val_loss: 1213.7625 - val_mse: 1213.7625 - val_mae: 13.4466\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.1779 - mse: 7.1779 - mae: 1.5689 - val_loss: 1216.1704 - val_mse: 1216.1704 - val_mae: 13.3423\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.1293 - mse: 10.1293 - mae: 2.0163 - val_loss: 1304.4758 - val_mse: 1304.4758 - val_mae: 14.3109\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.5295 - mse: 6.5295 - mae: 1.8656 - val_loss: 1237.8606 - val_mse: 1237.8606 - val_mae: 13.4476\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4194 - mse: 5.4194 - mae: 1.5013 - val_loss: 1178.3584 - val_mse: 1178.3584 - val_mae: 13.3972\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7987 - mse: 6.7987 - mae: 1.4977 - val_loss: 1172.7012 - val_mse: 1172.7012 - val_mae: 13.1395\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3675 - mse: 6.3675 - mae: 1.7752 - val_loss: 1255.6648 - val_mse: 1255.6648 - val_mae: 13.7358\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8905 - mse: 5.8905 - mae: 1.6841 - val_loss: 1234.6678 - val_mse: 1234.6678 - val_mae: 13.3406\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4035 - mse: 4.4035 - mae: 1.3078 - val_loss: 1178.0314 - val_mse: 1178.0314 - val_mae: 13.2048\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0842 - mse: 4.0842 - mae: 1.1641 - val_loss: 1221.5653 - val_mse: 1221.5653 - val_mae: 13.3783\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7247 - mse: 3.7247 - mae: 1.0709 - val_loss: 1273.8676 - val_mse: 1273.8676 - val_mae: 13.6350\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1057 - mse: 4.1057 - mae: 1.1417 - val_loss: 1236.7356 - val_mse: 1236.7356 - val_mae: 13.3683\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4384 - mse: 3.4384 - mae: 1.0060 - val_loss: 1223.5231 - val_mse: 1223.5231 - val_mae: 13.2536\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0674 - mse: 3.0674 - mae: 0.9349 - val_loss: 1201.3063 - val_mse: 1201.3063 - val_mae: 13.1776\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.8842 - mse: 3.8842 - mae: 1.0955 - val_loss: 1210.6982 - val_mse: 1210.6982 - val_mae: 13.2391\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0541 - mse: 3.0541 - mae: 0.9789 - val_loss: 1229.7448 - val_mse: 1229.7448 - val_mae: 13.3485\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7385 - mse: 3.7385 - mae: 1.1162 - val_loss: 1216.8364 - val_mse: 1216.8364 - val_mae: 13.3455\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.1368 - mse: 3.1368 - mae: 1.0415 - val_loss: 1159.7498 - val_mse: 1159.7498 - val_mae: 13.1782\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5845 - mse: 3.5845 - mae: 1.1198 - val_loss: 1203.8770 - val_mse: 1203.8770 - val_mae: 13.3132\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5887 - mse: 4.5887 - mae: 1.1787 - val_loss: 1248.3147 - val_mse: 1248.3147 - val_mae: 13.4037\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9565 - mse: 4.9565 - mae: 1.4129 - val_loss: 1202.2898 - val_mse: 1202.2898 - val_mae: 13.4257\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9797 - mse: 4.9797 - mae: 1.6265 - val_loss: 1246.1864 - val_mse: 1246.1864 - val_mae: 13.5487\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3505 - mse: 5.3505 - mae: 1.6492 - val_loss: 1219.8175 - val_mse: 1219.8175 - val_mae: 13.2470\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7373 - mse: 3.7373 - mae: 1.2964 - val_loss: 1188.5347 - val_mse: 1188.5347 - val_mae: 13.2794\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5250 - mse: 4.5250 - mae: 1.3084 - val_loss: 1225.2483 - val_mse: 1225.2483 - val_mae: 13.4155\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5938 - mse: 4.5938 - mae: 1.5663 - val_loss: 1265.5393 - val_mse: 1265.5394 - val_mae: 13.7626\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8712 - mse: 3.8712 - mae: 1.3919 - val_loss: 1200.1543 - val_mse: 1200.1543 - val_mae: 13.1620\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6178 - mse: 4.6178 - mae: 1.2672 - val_loss: 1160.9810 - val_mse: 1160.9810 - val_mae: 13.2611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 5.1791 - mse: 5.1791 - mae: 1.2661 - val_loss: 1161.6434 - val_mse: 1161.6434 - val_mae: 13.0310\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6533 - mse: 4.6533 - mae: 1.3535 - val_loss: 1153.8372 - val_mse: 1153.8372 - val_mae: 13.2597\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9465 - mse: 3.9465 - mae: 1.1661 - val_loss: 1251.3129 - val_mse: 1251.3129 - val_mae: 13.4349\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7512 - mse: 4.7512 - mae: 1.4204 - val_loss: 1280.5392 - val_mse: 1280.5392 - val_mae: 13.6797\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.2362 - mse: 7.2362 - mae: 1.6536 - val_loss: 1232.6180 - val_mse: 1232.6180 - val_mae: 14.1705\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1412 - mse: 13.1412 - mae: 2.6261 - val_loss: 1221.5636 - val_mse: 1221.5636 - val_mae: 13.2942\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3293 - mse: 10.3293 - mae: 2.3300 - val_loss: 1260.4343 - val_mse: 1260.4343 - val_mae: 13.3796\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.8380 - mse: 7.8380 - mae: 2.0640 - val_loss: 1183.9077 - val_mse: 1183.9077 - val_mae: 13.3208\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8264 - mse: 6.8264 - mae: 1.9730 - val_loss: 1289.9510 - val_mse: 1289.9512 - val_mae: 13.5128\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5867 - mse: 7.5867 - mae: 2.0567 - val_loss: 1169.1829 - val_mse: 1169.1829 - val_mae: 13.2064\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.6066 - mse: 6.6066 - mae: 1.8210 - val_loss: 1246.9658 - val_mse: 1246.9658 - val_mae: 13.8040\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.8938 - mse: 9.8938 - mae: 1.9759 - val_loss: 1301.3448 - val_mse: 1301.3447 - val_mae: 13.9278\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5581 - mse: 7.5581 - mae: 1.9974 - val_loss: 1353.3500 - val_mse: 1353.3500 - val_mae: 14.7706\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.8347 - mse: 10.8347 - mae: 2.4134 - val_loss: 1159.1770 - val_mse: 1159.1770 - val_mae: 13.2797\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.4876 - mse: 14.4876 - mae: 2.4880 - val_loss: 1136.9089 - val_mse: 1136.9089 - val_mae: 13.0013\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.0163 - mse: 14.0163 - mae: 2.5712 - val_loss: 1201.9070 - val_mse: 1201.9070 - val_mae: 13.6297\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.2481 - mse: 11.2481 - mae: 2.2983 - val_loss: 1353.2209 - val_mse: 1353.2209 - val_mae: 13.5026\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.5601 - mse: 10.5601 - mae: 2.2240 - val_loss: 1267.0548 - val_mse: 1267.0547 - val_mae: 13.4852\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8726 - mse: 8.8726 - mae: 2.1976 - val_loss: 1213.2205 - val_mse: 1213.2205 - val_mae: 13.6776\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8884 - mse: 5.8884 - mae: 1.8420 - val_loss: 1302.5000 - val_mse: 1302.5000 - val_mae: 13.7357\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.6351 - mse: 9.6351 - mae: 1.9125 - val_loss: 1158.5363 - val_mse: 1158.5363 - val_mae: 13.7145\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.1706 - mse: 12.1706 - mae: 2.3909 - val_loss: 1087.9679 - val_mse: 1087.9679 - val_mae: 13.1955\n",
      "Epoch 400/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1010 - mse: 13.1010 - mae: 2.5421 - val_loss: 1158.9950 - val_mse: 1158.9950 - val_mae: 13.5200\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.1528 - mse: 8.1528 - mae: 1.9264 - val_loss: 1281.8000 - val_mse: 1281.8000 - val_mae: 13.6951\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.6944 - mse: 7.6944 - mae: 1.7504 - val_loss: 1141.6598 - val_mse: 1141.6598 - val_mae: 13.2108\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6116 - mse: 4.6116 - mae: 1.5398 - val_loss: 1144.9614 - val_mse: 1144.9614 - val_mae: 13.1937\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.7471 - mse: 3.7471 - mae: 1.3354 - val_loss: 1194.4706 - val_mse: 1194.4706 - val_mae: 13.0622\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8148 - mse: 2.8148 - mae: 1.0909 - val_loss: 1190.6290 - val_mse: 1190.6290 - val_mae: 13.1976\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5579 - mse: 2.5579 - mae: 1.0639 - val_loss: 1201.1323 - val_mse: 1201.1323 - val_mae: 13.1545\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9886 - mse: 2.9886 - mae: 1.1932 - val_loss: 1186.9208 - val_mse: 1186.9208 - val_mae: 13.2647\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3475 - mse: 4.3475 - mae: 1.3022 - val_loss: 1194.9434 - val_mse: 1194.9434 - val_mae: 13.0775\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6992 - mse: 3.6992 - mae: 1.3148 - val_loss: 1262.0171 - val_mse: 1262.0171 - val_mae: 13.7267\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7042 - mse: 4.7042 - mae: 1.4694 - val_loss: 1272.6309 - val_mse: 1272.6307 - val_mae: 13.4979\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7989 - mse: 2.7989 - mae: 1.0494 - val_loss: 1216.2106 - val_mse: 1216.2106 - val_mae: 13.3133\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9894 - mse: 3.9894 - mae: 1.2788 - val_loss: 1234.3634 - val_mse: 1234.3634 - val_mae: 13.5301\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4453 - mse: 4.4453 - mae: 1.2956 - val_loss: 1163.0082 - val_mse: 1163.0082 - val_mae: 13.1723\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.8556 - mse: 6.8556 - mae: 1.8265 - val_loss: 1137.9518 - val_mse: 1137.9518 - val_mae: 12.9706\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7130 - mse: 4.7130 - mae: 1.4392 - val_loss: 1156.1904 - val_mse: 1156.1904 - val_mae: 13.3481\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8767 - mse: 2.8767 - mae: 1.1241 - val_loss: 1172.9854 - val_mse: 1172.9854 - val_mae: 12.8845\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7269 - mse: 2.7269 - mae: 1.0775 - val_loss: 1223.2163 - val_mse: 1223.2163 - val_mae: 13.3122\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4097 - mse: 2.4097 - mae: 1.0133 - val_loss: 1179.5251 - val_mse: 1179.5251 - val_mae: 13.1787\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3895 - mse: 2.3895 - mae: 0.9603 - val_loss: 1213.6102 - val_mse: 1213.6102 - val_mae: 13.3343\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8572 - mse: 3.8572 - mae: 1.2799 - val_loss: 1170.8556 - val_mse: 1170.8556 - val_mae: 13.1230\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.9135 - mse: 5.9135 - mae: 1.5891 - val_loss: 1125.6550 - val_mse: 1125.6550 - val_mae: 13.0819\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5520 - mse: 7.5520 - mae: 1.8049 - val_loss: 1307.9271 - val_mse: 1307.9271 - val_mae: 13.4221\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.8569 - mse: 8.8569 - mae: 1.9971 - val_loss: 1371.1942 - val_mse: 1371.1942 - val_mae: 14.4644\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.3869 - mse: 17.3869 - mae: 2.5756 - val_loss: 1262.2947 - val_mse: 1262.2947 - val_mae: 14.6436\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 25.8629 - mse: 25.8629 - mae: 2.9981 - val_loss: 1626.8051 - val_mse: 1626.8051 - val_mae: 16.4424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.0479 - mse: 39.0479 - mae: 4.2118 - val_loss: 1249.4647 - val_mse: 1249.4647 - val_mae: 13.3712\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.0047 - mse: 26.0047 - mae: 3.3815 - val_loss: 1126.6765 - val_mse: 1126.6765 - val_mae: 13.8153\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.2994 - mse: 16.2994 - mae: 2.9610 - val_loss: 1260.1600 - val_mse: 1260.1600 - val_mae: 13.7956\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.7821 - mse: 13.7821 - mae: 2.7587 - val_loss: 1248.0897 - val_mse: 1248.0897 - val_mae: 13.5278\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.4661 - mse: 7.4661 - mae: 2.0961 - val_loss: 1209.6896 - val_mse: 1209.6896 - val_mae: 13.5745\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0238 - mse: 6.0238 - mae: 1.8507 - val_loss: 1323.2485 - val_mse: 1323.2485 - val_mae: 13.7746\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8527 - mse: 5.8527 - mae: 1.7040 - val_loss: 1176.0288 - val_mse: 1176.0288 - val_mae: 13.2587\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7493 - mse: 4.7493 - mae: 1.3714 - val_loss: 1146.3101 - val_mse: 1146.3101 - val_mae: 13.3501\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4340 - mse: 5.4340 - mae: 1.5362 - val_loss: 1367.9746 - val_mse: 1367.9746 - val_mae: 14.0671\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8611 - mse: 5.8611 - mae: 1.5996 - val_loss: 1294.8716 - val_mse: 1294.8716 - val_mae: 14.0955\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5218 - mse: 7.5218 - mae: 1.7664 - val_loss: 1305.1602 - val_mse: 1305.1602 - val_mae: 14.0187\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5945 - mse: 8.5945 - mae: 1.9416 - val_loss: 1295.1671 - val_mse: 1295.1671 - val_mae: 13.2820\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.4190 - mse: 10.4190 - mae: 2.2330 - val_loss: 1202.9824 - val_mse: 1202.9824 - val_mae: 13.1127\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5370 - mse: 8.5370 - mae: 1.9744 - val_loss: 1173.6862 - val_mse: 1173.6862 - val_mae: 13.0204\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.7103 - mse: 6.7103 - mae: 1.7602 - val_loss: 1251.1187 - val_mse: 1251.1187 - val_mae: 13.6088\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.8138 - mse: 5.8138 - mae: 1.5174 - val_loss: 1212.7155 - val_mse: 1212.7155 - val_mae: 13.3694\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4.1288 - mse: 4.1288 - mae: 1.4265 - val_loss: 1293.1761 - val_mse: 1293.1761 - val_mae: 13.8496\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6412 - mse: 3.6412 - mae: 1.2945 - val_loss: 1273.3475 - val_mse: 1273.3475 - val_mae: 13.5736\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6238 - mse: 3.6238 - mae: 1.3890 - val_loss: 1269.7115 - val_mse: 1269.7115 - val_mae: 13.2940\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1783 - mse: 3.1783 - mae: 1.1489 - val_loss: 1198.6542 - val_mse: 1198.6542 - val_mae: 13.0369\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2610 - mse: 3.2610 - mae: 1.1708 - val_loss: 1230.2180 - val_mse: 1230.2180 - val_mae: 13.3202\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5069 - mse: 2.5069 - mae: 1.0229 - val_loss: 1227.3467 - val_mse: 1227.3467 - val_mae: 13.4212\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1346 - mse: 2.1346 - mae: 0.9926 - val_loss: 1208.6664 - val_mse: 1208.6664 - val_mae: 13.1916\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8629 - mse: 1.8629 - mae: 0.8565 - val_loss: 1266.8123 - val_mse: 1266.8123 - val_mae: 13.3797\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6326 - mse: 3.6326 - mae: 1.2238 - val_loss: 1232.1591 - val_mse: 1232.1591 - val_mae: 13.2350\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5690 - mse: 4.5690 - mae: 1.5953 - val_loss: 1253.8668 - val_mse: 1253.8668 - val_mae: 13.3913\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.8269 - mse: 3.8269 - mae: 1.4167 - val_loss: 1185.9554 - val_mse: 1185.9554 - val_mae: 13.2745\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6483 - mse: 3.6483 - mae: 1.3524 - val_loss: 1207.1039 - val_mse: 1207.1039 - val_mae: 13.0772\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9903 - mse: 4.9903 - mae: 1.5462 - val_loss: 1213.4031 - val_mse: 1213.4031 - val_mae: 13.3083\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0936 - mse: 3.0936 - mae: 1.1830 - val_loss: 1245.5338 - val_mse: 1245.5338 - val_mae: 13.1669\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5014 - mse: 3.5014 - mae: 1.3717 - val_loss: 1215.1808 - val_mse: 1215.1808 - val_mae: 13.5706\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6567 - mse: 4.6567 - mae: 1.6373 - val_loss: 1282.2140 - val_mse: 1282.2141 - val_mae: 13.3994\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3415 - mse: 4.3415 - mae: 1.5601 - val_loss: 1318.9790 - val_mse: 1318.9791 - val_mae: 13.7045\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.9608 - mse: 3.9608 - mae: 1.3858 - val_loss: 1216.6632 - val_mse: 1216.6632 - val_mae: 13.3200\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5806 - mse: 3.5806 - mae: 1.2549 - val_loss: 1183.8218 - val_mse: 1183.8218 - val_mae: 13.0145\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4365 - mse: 4.4365 - mae: 1.4820 - val_loss: 1291.9531 - val_mse: 1291.9530 - val_mae: 13.5929\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0331 - mse: 4.0331 - mae: 1.4122 - val_loss: 1209.1281 - val_mse: 1209.1281 - val_mae: 13.0682\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0447 - mse: 6.0447 - mae: 1.8203 - val_loss: 1255.5380 - val_mse: 1255.5380 - val_mae: 13.5822\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6760 - mse: 4.6760 - mae: 1.8056 - val_loss: 1286.8201 - val_mse: 1286.8201 - val_mae: 13.4643\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.7588 - mse: 4.7588 - mae: 1.6742 - val_loss: 1342.1642 - val_mse: 1342.1642 - val_mae: 14.1359\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3402 - mse: 4.3402 - mae: 1.3945 - val_loss: 1243.8153 - val_mse: 1243.8153 - val_mae: 13.4140\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9052 - mse: 1.9052 - mae: 0.9696 - val_loss: 1259.5558 - val_mse: 1259.5558 - val_mae: 13.3522\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.0307 - mse: 2.0307 - mae: 1.0445 - val_loss: 1244.8031 - val_mse: 1244.8031 - val_mae: 13.1803\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0500 - mse: 2.0500 - mae: 0.9740 - val_loss: 1265.9705 - val_mse: 1265.9705 - val_mae: 13.3380\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9809 - mse: 1.9809 - mae: 0.9291 - val_loss: 1224.6326 - val_mse: 1224.6326 - val_mae: 13.1100\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6535 - mse: 2.6535 - mae: 1.0958 - val_loss: 1201.4865 - val_mse: 1201.4865 - val_mae: 12.8686\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6453 - mse: 4.6453 - mae: 1.5074 - val_loss: 1189.2373 - val_mse: 1189.2373 - val_mae: 13.4453\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9720 - mse: 8.9720 - mae: 1.6684 - val_loss: 1257.2306 - val_mse: 1257.2306 - val_mae: 13.1532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 10.4189 - mse: 10.4189 - mae: 2.2114 - val_loss: 1262.3909 - val_mse: 1262.3909 - val_mae: 14.3580\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.5338 - mse: 14.5338 - mae: 2.4249 - val_loss: 1274.0059 - val_mse: 1274.0059 - val_mae: 13.5204\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 11.5449 - mse: 11.5449 - mae: 2.3941 - val_loss: 1279.6628 - val_mse: 1279.6630 - val_mae: 13.8898\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.4457 - mse: 9.4457 - mae: 2.2410 - val_loss: 1438.1350 - val_mse: 1438.1350 - val_mae: 14.7370\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.3672 - mse: 12.3672 - mae: 2.5689 - val_loss: 1218.2603 - val_mse: 1218.2603 - val_mae: 13.5915\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7733 - mse: 5.7733 - mae: 1.7388 - val_loss: 1183.6503 - val_mse: 1183.6503 - val_mae: 13.2166\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.0751 - mse: 6.0751 - mae: 1.6854 - val_loss: 1176.2957 - val_mse: 1176.2957 - val_mae: 13.0436\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4056 - mse: 6.4056 - mae: 1.8361 - val_loss: 1264.9862 - val_mse: 1264.9862 - val_mae: 13.7862\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.6769 - mse: 5.6769 - mae: 1.6862 - val_loss: 1323.2302 - val_mse: 1323.2303 - val_mae: 13.7145\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3927 - mse: 10.3927 - mae: 2.1251 - val_loss: 1306.9656 - val_mse: 1306.9656 - val_mae: 14.0326\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4545 - mse: 13.4545 - mae: 2.2929 - val_loss: 1308.7556 - val_mse: 1308.7556 - val_mae: 13.8999\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0218 - mse: 12.0218 - mae: 2.3740 - val_loss: 1256.0847 - val_mse: 1256.0847 - val_mae: 14.4388\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.2331 - mse: 34.2331 - mae: 3.9680 - val_loss: 1369.0391 - val_mse: 1369.0391 - val_mae: 15.1460\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 113.5325 - mse: 113.5325 - mae: 6.2901 - val_loss: 1323.6876 - val_mse: 1323.6876 - val_mae: 14.8827\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 84.8273 - mse: 84.8273 - mae: 6.0206 - val_loss: 1360.3721 - val_mse: 1360.3721 - val_mae: 15.6888\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 105.5821 - mse: 105.5821 - mae: 6.2520 - val_loss: 1054.4569 - val_mse: 1054.4569 - val_mae: 16.8207\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 167.3595 - mse: 167.3595 - mae: 9.4703 - val_loss: 1295.5106 - val_mse: 1295.5106 - val_mae: 16.4266\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 96.8209 - mse: 96.8209 - mae: 6.0864 - val_loss: 2236.2671 - val_mse: 2236.2671 - val_mae: 18.8447\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 162.6954 - mse: 162.6954 - mae: 7.8541 - val_loss: 984.2103 - val_mse: 984.2103 - val_mae: 14.2888\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 68.7375 - mse: 68.7375 - mae: 6.4802 - val_loss: 880.6840 - val_mse: 880.6840 - val_mae: 13.4675\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 50.0562 - mse: 50.0562 - mae: 5.3080 - val_loss: 920.5891 - val_mse: 920.5891 - val_mae: 13.3062\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.8972 - mse: 29.8972 - mae: 4.2212 - val_loss: 1349.4321 - val_mse: 1349.4321 - val_mae: 14.8746\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 34.1345 - mse: 34.1345 - mae: 3.7742 - val_loss: 1163.6874 - val_mse: 1163.6874 - val_mae: 14.4351\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.1902 - mse: 21.1902 - mae: 3.6403 - val_loss: 1161.5988 - val_mse: 1161.5988 - val_mae: 13.0213\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.2438 - mse: 14.2438 - mae: 2.9505 - val_loss: 1147.2159 - val_mse: 1147.2159 - val_mae: 13.5497\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.3163 - mse: 12.3163 - mae: 2.6326 - val_loss: 1307.2238 - val_mse: 1307.2238 - val_mae: 13.4446\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.9429 - mse: 10.9429 - mae: 2.3314 - val_loss: 1196.4012 - val_mse: 1196.4012 - val_mae: 13.6548\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.1581 - mse: 13.1581 - mae: 2.7834 - val_loss: 1051.5591 - val_mse: 1051.5591 - val_mae: 13.4406\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.8064 - mse: 19.8064 - mae: 3.1162 - val_loss: 1129.3547 - val_mse: 1129.3547 - val_mae: 14.2436\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.5215 - mse: 14.5215 - mae: 2.7117 - val_loss: 1364.7598 - val_mse: 1364.7598 - val_mae: 14.5509\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.4262 - mse: 16.4262 - mae: 2.5528 - val_loss: 1094.0684 - val_mse: 1094.0684 - val_mae: 13.1938\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.3659 - mse: 10.3659 - mae: 2.0948 - val_loss: 1148.4158 - val_mse: 1148.4158 - val_mae: 13.7231\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.6972 - mse: 8.6972 - mae: 2.0117 - val_loss: 1312.6653 - val_mse: 1312.6654 - val_mae: 14.0157\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.3978 - mse: 7.3978 - mae: 1.8038 - val_loss: 1248.6061 - val_mse: 1248.6061 - val_mae: 13.3224\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4886 - mse: 6.4886 - mae: 1.7367 - val_loss: 1215.4869 - val_mse: 1215.4869 - val_mae: 13.3538\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1395 - mse: 4.1395 - mae: 1.4997 - val_loss: 1200.8743 - val_mse: 1200.8743 - val_mae: 13.0107\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5964 - mse: 2.5964 - mae: 1.1563 - val_loss: 1180.4437 - val_mse: 1180.4437 - val_mae: 13.2414\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5456 - mse: 2.5456 - mae: 1.0602 - val_loss: 1220.2932 - val_mse: 1220.2932 - val_mae: 13.1086\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2404 - mse: 2.2404 - mae: 1.0795 - val_loss: 1211.8342 - val_mse: 1211.8342 - val_mae: 13.3792\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0176 - mse: 2.0176 - mae: 0.9746 - val_loss: 1207.6646 - val_mse: 1207.6646 - val_mae: 13.2481\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7427 - mse: 1.7427 - mae: 0.8956 - val_loss: 1225.5989 - val_mse: 1225.5989 - val_mae: 13.2037\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5469 - mse: 1.5469 - mae: 0.8222 - val_loss: 1185.4921 - val_mse: 1185.4921 - val_mae: 13.3317\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.4799 - mse: 2.4799 - mae: 0.9759 - val_loss: 1209.2992 - val_mse: 1209.2992 - val_mae: 13.0874\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1834 - mse: 2.1834 - mae: 1.0476 - val_loss: 1273.8373 - val_mse: 1273.8373 - val_mae: 13.5651\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8184 - mse: 2.8184 - mae: 1.1317 - val_loss: 1268.8872 - val_mse: 1268.8871 - val_mae: 13.6648\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5508 - mse: 2.5508 - mae: 1.0859 - val_loss: 1227.7947 - val_mse: 1227.7947 - val_mae: 13.2464\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1318 - mse: 2.1318 - mae: 0.9531 - val_loss: 1154.1702 - val_mse: 1154.1702 - val_mae: 13.1781\n",
      "Epoch 521/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2463 - mse: 3.2463 - mae: 1.1815 - val_loss: 1264.5630 - val_mse: 1264.5631 - val_mae: 13.5542\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1284 - mse: 3.1284 - mae: 1.2254 - val_loss: 1298.6206 - val_mse: 1298.6207 - val_mae: 13.6633\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5518 - mse: 2.5518 - mae: 1.0168 - val_loss: 1226.8568 - val_mse: 1226.8568 - val_mae: 13.1941\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7541 - mse: 1.7541 - mae: 0.8424 - val_loss: 1160.6782 - val_mse: 1160.6782 - val_mae: 13.1973\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8011 - mse: 1.8011 - mae: 0.8014 - val_loss: 1106.6698 - val_mse: 1106.6698 - val_mae: 12.9801\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9912 - mse: 1.9912 - mae: 0.8990 - val_loss: 1226.2509 - val_mse: 1226.2509 - val_mae: 13.3489\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4315 - mse: 1.4315 - mae: 0.8265 - val_loss: 1216.2261 - val_mse: 1216.2261 - val_mae: 13.2314\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2197 - mse: 1.2197 - mae: 0.7395 - val_loss: 1168.9937 - val_mse: 1168.9937 - val_mae: 13.2235\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9284 - mse: 1.9284 - mae: 0.9439 - val_loss: 1204.4415 - val_mse: 1204.4415 - val_mae: 13.2828\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.7315 - mse: 1.7315 - mae: 0.9003 - val_loss: 1206.0607 - val_mse: 1206.0607 - val_mae: 13.2011\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3990 - mse: 1.3990 - mae: 0.8322 - val_loss: 1273.0038 - val_mse: 1273.0038 - val_mae: 13.2807\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4895 - mse: 1.4895 - mae: 0.7484 - val_loss: 1194.7832 - val_mse: 1194.7832 - val_mae: 13.1671\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.5708 - mse: 1.5708 - mae: 0.8652 - val_loss: 1205.7491 - val_mse: 1205.7491 - val_mae: 13.2376\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5656 - mse: 1.5656 - mae: 0.8779 - val_loss: 1176.5875 - val_mse: 1176.5875 - val_mae: 13.0630\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2053 - mse: 2.2053 - mae: 0.9739 - val_loss: 1261.0984 - val_mse: 1261.0984 - val_mae: 13.2230\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3678 - mse: 2.3678 - mae: 0.9771 - val_loss: 1262.1029 - val_mse: 1262.1028 - val_mae: 13.4887\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7039 - mse: 2.7039 - mae: 1.0659 - val_loss: 1255.3322 - val_mse: 1255.3320 - val_mae: 13.3314\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2984 - mse: 2.2984 - mae: 0.9741 - val_loss: 1230.9884 - val_mse: 1230.9884 - val_mae: 13.2941\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.9908 - mse: 1.9908 - mae: 0.8001 - val_loss: 1179.2334 - val_mse: 1179.2334 - val_mae: 13.2704\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.3516 - mse: 2.3516 - mae: 0.9983 - val_loss: 1162.8121 - val_mse: 1162.8121 - val_mae: 13.1596\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2352 - mse: 3.2352 - mae: 1.2458 - val_loss: 1228.3466 - val_mse: 1228.3466 - val_mae: 13.2550\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1071 - mse: 2.1071 - mae: 0.8666 - val_loss: 1237.4283 - val_mse: 1237.4283 - val_mae: 13.3803\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1398 - mse: 1.1398 - mae: 0.7577 - val_loss: 1215.9127 - val_mse: 1215.9127 - val_mae: 13.2333\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0754 - mse: 1.0754 - mae: 0.6987 - val_loss: 1212.7430 - val_mse: 1212.7430 - val_mae: 13.1897\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8195 - mse: 0.8195 - mae: 0.5612 - val_loss: 1240.9788 - val_mse: 1240.9788 - val_mae: 13.3214\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7754 - mse: 0.7754 - mae: 0.5566 - val_loss: 1217.1255 - val_mse: 1217.1255 - val_mae: 13.2773\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7509 - mse: 0.7509 - mae: 0.5376 - val_loss: 1222.1744 - val_mse: 1222.1744 - val_mae: 13.2438\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7752 - mse: 0.7752 - mae: 0.5259 - val_loss: 1214.9791 - val_mse: 1214.9791 - val_mae: 13.2627\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8334 - mse: 0.8334 - mae: 0.5860 - val_loss: 1181.8220 - val_mse: 1181.8220 - val_mae: 13.1993\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4700 - mse: 1.4700 - mae: 0.7994 - val_loss: 1239.3949 - val_mse: 1239.3949 - val_mae: 13.2893\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5311 - mse: 1.5311 - mae: 0.8052 - val_loss: 1242.2386 - val_mse: 1242.2386 - val_mae: 13.3564\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2189 - mse: 1.2189 - mae: 0.7000 - val_loss: 1255.6694 - val_mse: 1255.6694 - val_mae: 13.2292\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5512 - mse: 1.5512 - mae: 0.8844 - val_loss: 1190.9181 - val_mse: 1190.9181 - val_mae: 13.0619\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8313 - mse: 1.8313 - mae: 1.0069 - val_loss: 1180.2423 - val_mse: 1180.2423 - val_mae: 13.0719\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7145 - mse: 1.7145 - mae: 0.9960 - val_loss: 1153.4927 - val_mse: 1153.4927 - val_mae: 13.2995\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7309 - mse: 2.7309 - mae: 1.0703 - val_loss: 1274.2319 - val_mse: 1274.2319 - val_mae: 13.2658\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9059 - mse: 2.9059 - mae: 1.3505 - val_loss: 1197.5238 - val_mse: 1197.5238 - val_mae: 13.3295\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.9900 - mse: 2.9900 - mae: 1.3203 - val_loss: 1208.7413 - val_mse: 1208.7413 - val_mae: 13.3830\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4633 - mse: 3.4633 - mae: 1.4822 - val_loss: 1265.2522 - val_mse: 1265.2522 - val_mae: 13.5417\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.2762 - mse: 3.2762 - mae: 1.3877 - val_loss: 1233.7114 - val_mse: 1233.7114 - val_mae: 13.4670\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5511 - mse: 5.5511 - mae: 1.5622 - val_loss: 1153.8075 - val_mse: 1153.8075 - val_mae: 12.9281\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.1482 - mse: 5.1482 - mae: 1.4669 - val_loss: 1167.9232 - val_mse: 1167.9232 - val_mae: 13.0649\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0665 - mse: 4.0665 - mae: 1.3390 - val_loss: 1175.9396 - val_mse: 1175.9396 - val_mae: 13.0759\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0279 - mse: 4.0279 - mae: 1.4057 - val_loss: 1226.0320 - val_mse: 1226.0320 - val_mae: 13.4404\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7874 - mse: 2.7874 - mae: 1.0011 - val_loss: 1246.5793 - val_mse: 1246.5793 - val_mae: 13.0531\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8900 - mse: 1.8900 - mae: 0.9551 - val_loss: 1202.4357 - val_mse: 1202.4357 - val_mae: 13.4567\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8241 - mse: 1.8241 - mae: 0.8970 - val_loss: 1219.6777 - val_mse: 1219.6777 - val_mae: 13.0756\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6392 - mse: 1.6392 - mae: 0.9841 - val_loss: 1230.4978 - val_mse: 1230.4978 - val_mae: 13.2051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3233 - mse: 1.3233 - mae: 0.8424 - val_loss: 1228.3171 - val_mse: 1228.3171 - val_mae: 13.3918\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9668 - mse: 0.9668 - mae: 0.7210 - val_loss: 1238.2905 - val_mse: 1238.2905 - val_mae: 13.3018\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8739 - mse: 0.8739 - mae: 0.6671 - val_loss: 1197.7860 - val_mse: 1197.7860 - val_mae: 13.1978\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9410 - mse: 0.9410 - mae: 0.6726 - val_loss: 1177.9453 - val_mse: 1177.9453 - val_mae: 13.1268\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1.4069 - mse: 1.4069 - mae: 0.9041 - val_loss: 1208.7297 - val_mse: 1208.7297 - val_mae: 13.1683\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1577 - mse: 1.1577 - mae: 0.6838 - val_loss: 1245.7332 - val_mse: 1245.7332 - val_mae: 13.5166\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1604 - mse: 2.1604 - mae: 0.8916 - val_loss: 1285.5500 - val_mse: 1285.5500 - val_mae: 13.4868\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5889 - mse: 2.5889 - mae: 1.0215 - val_loss: 1206.6058 - val_mse: 1206.6058 - val_mae: 13.4464\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9770 - mse: 4.9770 - mae: 1.5172 - val_loss: 1194.8535 - val_mse: 1194.8535 - val_mae: 13.4102\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4929 - mse: 5.4929 - mae: 1.3468 - val_loss: 1174.6790 - val_mse: 1174.6790 - val_mae: 12.8486\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.6879 - mse: 4.6879 - mae: 1.4425 - val_loss: 1181.3059 - val_mse: 1181.3059 - val_mae: 13.2865\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5395 - mse: 4.5395 - mae: 1.4501 - val_loss: 1317.9161 - val_mse: 1317.9161 - val_mae: 14.3556\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0551 - mse: 7.0551 - mae: 1.8783 - val_loss: 1342.6074 - val_mse: 1342.6074 - val_mae: 13.5867\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.2566 - mse: 7.2566 - mae: 1.8637 - val_loss: 1464.1161 - val_mse: 1464.1161 - val_mae: 14.9119\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.4745 - mse: 13.4745 - mae: 2.3591 - val_loss: 1214.9446 - val_mse: 1214.9446 - val_mae: 13.7293\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.0085 - mse: 11.0085 - mae: 2.5412 - val_loss: 1231.9592 - val_mse: 1231.9592 - val_mae: 13.3479\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.0803 - mse: 8.0803 - mae: 2.2141 - val_loss: 1186.0593 - val_mse: 1186.0593 - val_mae: 13.5562\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.7893 - mse: 5.7893 - mae: 1.6450 - val_loss: 1145.0635 - val_mse: 1145.0635 - val_mae: 13.3469\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6923 - mse: 3.6923 - mae: 1.3646 - val_loss: 1214.8917 - val_mse: 1214.8917 - val_mae: 13.1180\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6832 - mse: 2.6832 - mae: 1.2074 - val_loss: 1173.9664 - val_mse: 1173.9664 - val_mae: 13.2090\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6666 - mse: 2.6666 - mae: 1.0734 - val_loss: 1183.4139 - val_mse: 1183.4139 - val_mae: 13.3938\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3522 - mse: 2.3522 - mae: 1.0796 - val_loss: 1178.1561 - val_mse: 1178.1561 - val_mae: 13.2140\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7842 - mse: 1.7842 - mae: 0.9933 - val_loss: 1206.4740 - val_mse: 1206.4740 - val_mae: 13.3889\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1368 - mse: 1.1368 - mae: 0.7358 - val_loss: 1215.2645 - val_mse: 1215.2645 - val_mae: 13.3826\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0052 - mse: 1.0052 - mae: 0.6751 - val_loss: 1213.3979 - val_mse: 1213.3979 - val_mae: 13.1453\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8330 - mse: 0.8330 - mae: 0.6502 - val_loss: 1189.7122 - val_mse: 1189.7122 - val_mae: 13.2953\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7150 - mse: 0.7150 - mae: 0.5892 - val_loss: 1200.3406 - val_mse: 1200.3406 - val_mae: 13.2447\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6077 - mse: 0.6077 - mae: 0.5060 - val_loss: 1182.3508 - val_mse: 1182.3508 - val_mae: 13.1618\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6488 - mse: 0.6488 - mae: 0.4982 - val_loss: 1195.8232 - val_mse: 1195.8232 - val_mae: 13.2578\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0121 - mse: 1.0121 - mae: 0.6568 - val_loss: 1211.9890 - val_mse: 1211.9890 - val_mae: 13.1647\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6900 - mse: 0.6900 - mae: 0.5265 - val_loss: 1221.1024 - val_mse: 1221.1024 - val_mae: 13.2994\n",
      "Epoch 600/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5375 - mse: 0.5375 - mae: 0.4788 - val_loss: 1225.3245 - val_mse: 1225.3245 - val_mae: 13.2955\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7436 - mse: 0.7436 - mae: 0.5369 - val_loss: 1194.7766 - val_mse: 1194.7766 - val_mae: 13.3472\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7104 - mse: 0.7104 - mae: 0.5801 - val_loss: 1241.3652 - val_mse: 1241.3652 - val_mae: 13.3480\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0810 - mse: 1.0810 - mae: 0.7225 - val_loss: 1218.6893 - val_mse: 1218.6893 - val_mae: 13.2563\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7035 - mse: 0.7035 - mae: 0.5698 - val_loss: 1210.7009 - val_mse: 1210.7009 - val_mae: 13.2773\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5814 - mse: 0.5814 - mae: 0.5122 - val_loss: 1210.1003 - val_mse: 1210.1003 - val_mae: 13.1786\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6990 - mse: 0.6990 - mae: 0.5599 - val_loss: 1219.0167 - val_mse: 1219.0167 - val_mae: 13.2863\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4374 - mse: 0.4374 - mae: 0.4402 - val_loss: 1229.7466 - val_mse: 1229.7466 - val_mae: 13.3313\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5106 - mse: 0.5106 - mae: 0.4441 - val_loss: 1218.8472 - val_mse: 1218.8472 - val_mae: 13.2340\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4021 - mse: 0.4021 - mae: 0.3871 - val_loss: 1221.8436 - val_mse: 1221.8436 - val_mae: 13.3082\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3780 - mse: 0.3780 - mae: 0.3643 - val_loss: 1221.5022 - val_mse: 1221.5022 - val_mae: 13.3365\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3897 - mse: 0.3897 - mae: 0.3649 - val_loss: 1222.6693 - val_mse: 1222.6693 - val_mae: 13.2921\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3738 - mse: 0.3738 - mae: 0.3436 - val_loss: 1218.3430 - val_mse: 1218.3430 - val_mae: 13.2923\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4088 - mse: 0.4088 - mae: 0.3755 - val_loss: 1233.7805 - val_mse: 1233.7805 - val_mae: 13.3654\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4932 - mse: 0.4932 - mae: 0.4962 - val_loss: 1217.6102 - val_mse: 1217.6102 - val_mae: 13.3048\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7568 - mse: 0.7568 - mae: 0.6079 - val_loss: 1209.1378 - val_mse: 1209.1378 - val_mae: 13.2116\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3961 - mse: 1.3961 - mae: 0.7394 - val_loss: 1193.7036 - val_mse: 1193.7036 - val_mae: 13.1472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2909 - mse: 2.2909 - mae: 0.9154 - val_loss: 1150.5641 - val_mse: 1150.5641 - val_mae: 13.1363\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1251 - mse: 3.1251 - mae: 1.0576 - val_loss: 1228.8462 - val_mse: 1228.8462 - val_mae: 13.2863\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6391 - mse: 2.6391 - mae: 1.0457 - val_loss: 1236.6038 - val_mse: 1236.6038 - val_mae: 13.3178\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8167 - mse: 1.8167 - mae: 0.9240 - val_loss: 1313.4464 - val_mse: 1313.4464 - val_mae: 13.4698\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6166 - mse: 2.6166 - mae: 1.0345 - val_loss: 1212.0364 - val_mse: 1212.0364 - val_mae: 13.3131\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7996 - mse: 1.7996 - mae: 0.8796 - val_loss: 1286.9894 - val_mse: 1286.9894 - val_mae: 13.7353\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6709 - mse: 2.6709 - mae: 1.0952 - val_loss: 1246.1818 - val_mse: 1246.1818 - val_mae: 13.3299\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4437 - mse: 1.4437 - mae: 0.8614 - val_loss: 1239.2252 - val_mse: 1239.2252 - val_mae: 13.5247\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6559 - mse: 1.6559 - mae: 0.8946 - val_loss: 1256.0784 - val_mse: 1256.0782 - val_mae: 13.4693\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.2125 - mse: 2.2125 - mae: 0.9628 - val_loss: 1262.8593 - val_mse: 1262.8593 - val_mae: 13.6071\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.3242 - mse: 2.3242 - mae: 1.0788 - val_loss: 1255.8149 - val_mse: 1255.8149 - val_mae: 13.4240\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.4481 - mse: 6.4481 - mae: 1.4129 - val_loss: 1227.2739 - val_mse: 1227.2739 - val_mae: 13.6748\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.6565 - mse: 15.6565 - mae: 2.5901 - val_loss: 1114.4880 - val_mse: 1114.4880 - val_mae: 13.3429\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.8352 - mse: 13.8352 - mae: 2.3547 - val_loss: 1004.1139 - val_mse: 1004.1139 - val_mae: 12.8358\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.8966 - mse: 38.8966 - mae: 3.6082 - val_loss: 984.9153 - val_mse: 984.9153 - val_mae: 12.9055\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.3839 - mse: 21.3839 - mae: 3.0472 - val_loss: 1200.9880 - val_mse: 1200.9880 - val_mae: 14.5431\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 21.7475 - mse: 21.7475 - mae: 2.9026 - val_loss: 1346.7067 - val_mse: 1346.7067 - val_mae: 13.8474\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 17.4234 - mse: 17.4234 - mae: 2.8680 - val_loss: 1159.3634 - val_mse: 1159.3634 - val_mae: 13.7645\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 22.4033 - mse: 22.4033 - mae: 3.4376 - val_loss: 1372.4741 - val_mse: 1372.4740 - val_mae: 15.0467\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27.2558 - mse: 27.2558 - mae: 3.8976 - val_loss: 1465.2372 - val_mse: 1465.2372 - val_mae: 16.0163\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.3606 - mse: 33.3606 - mae: 4.2872 - val_loss: 1175.9761 - val_mse: 1175.9761 - val_mae: 13.9468\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.8788 - mse: 19.8788 - mae: 2.9955 - val_loss: 1098.9982 - val_mse: 1098.9982 - val_mae: 13.0422\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 16.6705 - mse: 16.6705 - mae: 2.7026 - val_loss: 1065.7006 - val_mse: 1065.7006 - val_mae: 12.9875\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.1903 - mse: 12.1903 - mae: 2.6247 - val_loss: 1165.4272 - val_mse: 1165.4272 - val_mae: 13.5631\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.1024 - mse: 8.1024 - mae: 2.1217 - val_loss: 1135.3796 - val_mse: 1135.3796 - val_mae: 13.4127\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.2980 - mse: 5.2980 - mae: 1.7651 - val_loss: 1244.2610 - val_mse: 1244.2610 - val_mae: 13.8669\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0692 - mse: 4.0692 - mae: 1.5792 - val_loss: 1109.3878 - val_mse: 1109.3878 - val_mae: 13.2205\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.9484 - mse: 4.9484 - mae: 1.4550 - val_loss: 1255.9923 - val_mse: 1255.9923 - val_mae: 13.4150\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4936 - mse: 4.4936 - mae: 1.5582 - val_loss: 1270.3108 - val_mse: 1270.3109 - val_mae: 13.8004\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5866 - mse: 4.5866 - mae: 1.6894 - val_loss: 1284.2729 - val_mse: 1284.2729 - val_mae: 13.7036\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4876 - mse: 3.4876 - mae: 1.4474 - val_loss: 1242.1619 - val_mse: 1242.1619 - val_mae: 13.0947\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3909 - mse: 3.3909 - mae: 1.2289 - val_loss: 1180.3815 - val_mse: 1180.3815 - val_mae: 13.6772\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.3568 - mse: 3.3568 - mae: 1.3483 - val_loss: 1235.0195 - val_mse: 1235.0195 - val_mae: 13.3026\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9439 - mse: 2.9439 - mae: 1.3114 - val_loss: 1219.6155 - val_mse: 1219.6155 - val_mae: 13.3788\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.1125 - mse: 2.1125 - mae: 1.1685 - val_loss: 1210.6199 - val_mse: 1210.6199 - val_mae: 13.6509\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1617 - mse: 1.1617 - mae: 0.7665 - val_loss: 1187.6510 - val_mse: 1187.6510 - val_mae: 13.2289\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 0.6986 - mse: 0.6986 - mae: 0.5619 - val_loss: 1198.0052 - val_mse: 1198.0052 - val_mae: 13.3396\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8914 - mse: 0.8914 - mae: 0.6682 - val_loss: 1219.4382 - val_mse: 1219.4382 - val_mae: 13.4756\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8297 - mse: 0.8297 - mae: 0.5841 - val_loss: 1195.9347 - val_mse: 1195.9347 - val_mae: 13.2310\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7982 - mse: 0.7982 - mae: 0.6160 - val_loss: 1158.9832 - val_mse: 1158.9832 - val_mae: 13.4685\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0869 - mse: 1.0869 - mae: 0.6825 - val_loss: 1178.1525 - val_mse: 1178.1525 - val_mae: 13.2060\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7969 - mse: 0.7969 - mae: 0.5653 - val_loss: 1170.5587 - val_mse: 1170.5587 - val_mae: 13.2470\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6823 - mse: 0.6823 - mae: 0.5478 - val_loss: 1171.9070 - val_mse: 1171.9070 - val_mae: 13.3056\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6943 - mse: 0.6943 - mae: 0.5068 - val_loss: 1231.9611 - val_mse: 1231.9611 - val_mae: 13.3198\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5687 - mse: 0.5687 - mae: 0.4891 - val_loss: 1188.3796 - val_mse: 1188.3796 - val_mae: 13.2891\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7718 - mse: 0.7718 - mae: 0.5499 - val_loss: 1186.9133 - val_mse: 1186.9133 - val_mae: 13.3847\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6429 - mse: 0.6429 - mae: 0.5745 - val_loss: 1187.8195 - val_mse: 1187.8195 - val_mae: 13.2688\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7703 - mse: 0.7703 - mae: 0.6229 - val_loss: 1165.4473 - val_mse: 1165.4473 - val_mae: 13.2941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8323 - mse: 0.8323 - mae: 0.6723 - val_loss: 1174.0537 - val_mse: 1174.0537 - val_mae: 13.3399\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0594 - mse: 1.0594 - mae: 0.6371 - val_loss: 1173.5552 - val_mse: 1173.5552 - val_mae: 13.2550\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.2833 - mse: 1.2833 - mae: 0.8500 - val_loss: 1229.1843 - val_mse: 1229.1843 - val_mae: 13.5311\n",
      "Epoch 668/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1963 - mse: 1.1963 - mae: 0.8049 - val_loss: 1251.9092 - val_mse: 1251.9092 - val_mae: 13.5855\n",
      "Epoch 669/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8938 - mse: 1.8938 - mae: 0.8797 - val_loss: 1277.3671 - val_mse: 1277.3672 - val_mae: 13.8662\n",
      "Epoch 670/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.6774 - mse: 1.6774 - mae: 0.8027 - val_loss: 1212.7975 - val_mse: 1212.7975 - val_mae: 13.2340\n",
      "Epoch 671/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4625 - mse: 1.4625 - mae: 0.7604 - val_loss: 1241.2323 - val_mse: 1241.2323 - val_mae: 13.3603\n",
      "Epoch 672/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9402 - mse: 0.9402 - mae: 0.6424 - val_loss: 1199.9683 - val_mse: 1199.9683 - val_mae: 13.4209\n",
      "Epoch 673/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5842 - mse: 0.5842 - mae: 0.5389 - val_loss: 1192.3455 - val_mse: 1192.3455 - val_mae: 13.3493\n",
      "Epoch 674/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4606 - mse: 0.4606 - mae: 0.4307 - val_loss: 1191.9436 - val_mse: 1191.9436 - val_mae: 13.2795\n",
      "Epoch 675/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5409 - mse: 0.5409 - mae: 0.5082 - val_loss: 1198.0990 - val_mse: 1198.0990 - val_mae: 13.2987\n",
      "Epoch 676/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4142 - mse: 0.4142 - mae: 0.4127 - val_loss: 1185.8141 - val_mse: 1185.8141 - val_mae: 13.3442\n",
      "Epoch 677/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3878 - mse: 0.3878 - mae: 0.3834 - val_loss: 1196.0175 - val_mse: 1196.0175 - val_mae: 13.3209\n",
      "Epoch 678/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3227 - mse: 0.3227 - mae: 0.3320 - val_loss: 1176.7480 - val_mse: 1176.7480 - val_mae: 13.3082\n",
      "Epoch 679/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3345 - mse: 0.3345 - mae: 0.3137 - val_loss: 1189.8906 - val_mse: 1189.8906 - val_mae: 13.3710\n",
      "Epoch 680/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3830 - mse: 0.3830 - mae: 0.3859 - val_loss: 1193.5260 - val_mse: 1193.5260 - val_mae: 13.3314\n",
      "Epoch 681/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3534 - mse: 0.3534 - mae: 0.3473 - val_loss: 1205.6021 - val_mse: 1205.6021 - val_mae: 13.2927\n",
      "Epoch 682/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5046 - mse: 0.5046 - mae: 0.4527 - val_loss: 1228.5251 - val_mse: 1228.5251 - val_mae: 13.5251\n",
      "Epoch 683/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5328 - mse: 0.5328 - mae: 0.5070 - val_loss: 1220.4683 - val_mse: 1220.4683 - val_mae: 13.4439\n",
      "Epoch 684/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4233 - mse: 0.4233 - mae: 0.4495 - val_loss: 1190.3003 - val_mse: 1190.3003 - val_mae: 13.3323\n",
      "Epoch 685/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4694 - mse: 0.4694 - mae: 0.4253 - val_loss: 1167.1089 - val_mse: 1167.1089 - val_mae: 13.3197\n",
      "Epoch 686/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5441 - mse: 0.5441 - mae: 0.4828 - val_loss: 1166.3197 - val_mse: 1166.3197 - val_mae: 13.3148\n",
      "Epoch 687/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0780 - mse: 1.0780 - mae: 0.6099 - val_loss: 1149.9087 - val_mse: 1149.9087 - val_mae: 13.2716\n",
      "Epoch 688/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4674 - mse: 3.4674 - mae: 1.0980 - val_loss: 1194.1813 - val_mse: 1194.1813 - val_mae: 13.1462\n",
      "Epoch 689/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.3860 - mse: 3.3860 - mae: 1.2726 - val_loss: 1192.2283 - val_mse: 1192.2283 - val_mae: 13.4828\n",
      "Epoch 690/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0573 - mse: 4.0573 - mae: 1.3926 - val_loss: 1266.5110 - val_mse: 1266.5110 - val_mae: 13.4570\n",
      "Epoch 691/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.1858 - mse: 4.1858 - mae: 1.3203 - val_loss: 1264.9154 - val_mse: 1264.9154 - val_mae: 13.2345\n",
      "Epoch 692/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.4086 - mse: 3.4086 - mae: 1.1612 - val_loss: 1200.3220 - val_mse: 1200.3220 - val_mae: 13.9403\n",
      "Epoch 693/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.4740 - mse: 4.4740 - mae: 1.6479 - val_loss: 1371.1469 - val_mse: 1371.1469 - val_mae: 14.2740\n",
      "Epoch 694/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.5147 - mse: 7.5147 - mae: 1.9395 - val_loss: 1287.6687 - val_mse: 1287.6687 - val_mae: 13.7725\n",
      "Epoch 695/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4107 - mse: 5.4107 - mae: 1.6533 - val_loss: 1190.1296 - val_mse: 1190.1296 - val_mae: 13.4348\n",
      "Epoch 696/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5190 - mse: 9.5190 - mae: 1.8887 - val_loss: 1306.4032 - val_mse: 1306.4032 - val_mae: 13.8180\n",
      "Epoch 697/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5268 - mse: 5.5268 - mae: 1.6829 - val_loss: 1205.6881 - val_mse: 1205.6881 - val_mae: 13.1022\n",
      "Epoch 698/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.3365 - mse: 8.3365 - mae: 1.8393 - val_loss: 1092.0845 - val_mse: 1092.0845 - val_mae: 13.0855\n",
      "Epoch 699/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.9923 - mse: 7.9923 - mae: 2.0140 - val_loss: 1090.0134 - val_mse: 1090.0134 - val_mae: 12.8049\n",
      "Epoch 700/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.9360 - mse: 7.9360 - mae: 1.8074 - val_loss: 1019.5583 - val_mse: 1019.5583 - val_mae: 12.8791\n",
      "Epoch 701/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.7056 - mse: 10.7056 - mae: 2.0297 - val_loss: 1067.4518 - val_mse: 1067.4518 - val_mae: 13.3277\n",
      "Epoch 702/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.7244 - mse: 24.7244 - mae: 2.9412 - val_loss: 1259.4507 - val_mse: 1259.4508 - val_mae: 13.3489\n",
      "Epoch 703/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.4672 - mse: 19.4672 - mae: 2.9587 - val_loss: 1311.5110 - val_mse: 1311.5110 - val_mae: 15.0037\n",
      "Epoch 704/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 12.3571 - mse: 12.3571 - mae: 2.3403 - val_loss: 1226.8119 - val_mse: 1226.8119 - val_mae: 13.6324\n",
      "Epoch 705/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3145 - mse: 14.3145 - mae: 2.6597 - val_loss: 1118.6198 - val_mse: 1118.6198 - val_mae: 13.0513\n",
      "Epoch 706/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.7994 - mse: 18.7994 - mae: 2.6824 - val_loss: 812.8515 - val_mse: 812.8515 - val_mae: 13.1972\n",
      "Epoch 707/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 29.3581 - mse: 29.3581 - mae: 3.4336 - val_loss: 1046.8218 - val_mse: 1046.8218 - val_mae: 13.4048\n",
      "Epoch 708/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 41.0694 - mse: 41.0694 - mae: 4.3479 - val_loss: 1117.9349 - val_mse: 1117.9349 - val_mae: 13.1647\n",
      "Epoch 709/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.1943 - mse: 32.1943 - mae: 3.3559 - val_loss: 1577.6693 - val_mse: 1577.6693 - val_mae: 14.8845\n",
      "Epoch 710/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 41.5162 - mse: 41.5162 - mae: 4.3717 - val_loss: 1605.4487 - val_mse: 1605.4487 - val_mae: 16.1510\n",
      "Epoch 711/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.0442 - mse: 36.0442 - mae: 3.9288 - val_loss: 1485.9301 - val_mse: 1485.9301 - val_mae: 13.9710\n",
      "Epoch 712/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 20.7646 - mse: 20.7646 - mae: 3.3277 - val_loss: 1424.5056 - val_mse: 1424.5056 - val_mae: 14.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 713/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 26.0796 - mse: 26.0796 - mae: 3.5156 - val_loss: 1453.6567 - val_mse: 1453.6569 - val_mae: 15.6625\n",
      "Epoch 714/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 19.8054 - mse: 19.8054 - mae: 3.0581 - val_loss: 1380.1014 - val_mse: 1380.1014 - val_mae: 14.3358\n",
      "Epoch 715/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 23.5699 - mse: 23.5699 - mae: 3.1337 - val_loss: 1545.0621 - val_mse: 1545.0620 - val_mae: 16.0811\n",
      "Epoch 716/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 36.1800 - mse: 36.1800 - mae: 4.0782 - val_loss: 1482.5255 - val_mse: 1482.5255 - val_mae: 15.4691\n",
      "Epoch 717/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 75.5070 - mse: 75.5070 - mae: 5.2937 - val_loss: 1300.6949 - val_mse: 1300.6951 - val_mae: 14.7146\n",
      "Epoch 718/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 94.1740 - mse: 94.1740 - mae: 6.1647 - val_loss: 1172.2384 - val_mse: 1172.2384 - val_mae: 13.3924\n",
      "Epoch 719/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 89.4293 - mse: 89.4293 - mae: 6.0399 - val_loss: 892.8466 - val_mse: 892.8466 - val_mae: 16.1617\n",
      "Epoch 720/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 135.1181 - mse: 135.1181 - mae: 6.9864 - val_loss: 1008.4438 - val_mse: 1008.4438 - val_mae: 13.2876\n",
      "Epoch 721/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 117.0961 - mse: 117.0961 - mae: 6.6850 - val_loss: 1771.8887 - val_mse: 1771.8887 - val_mae: 17.1244\n",
      "Epoch 722/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 108.9946 - mse: 108.9946 - mae: 6.5442 - val_loss: 1788.3361 - val_mse: 1788.3361 - val_mae: 18.2735\n",
      "Epoch 723/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 123.9436 - mse: 123.9436 - mae: 7.7348 - val_loss: 672.7543 - val_mse: 672.7543 - val_mae: 15.8616\n",
      "Epoch 724/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 105.8204 - mse: 105.8204 - mae: 5.8492 - val_loss: 905.2480 - val_mse: 905.2480 - val_mae: 13.0019\n",
      "Epoch 725/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 38.8933 - mse: 38.8933 - mae: 4.2685 - val_loss: 1364.6304 - val_mse: 1364.6304 - val_mae: 14.0055\n",
      "Epoch 726/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.7888 - mse: 37.7888 - mae: 4.0710 - val_loss: 1479.1196 - val_mse: 1479.1196 - val_mae: 15.5007\n",
      "Epoch 727/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 48.3230 - mse: 48.3230 - mae: 4.5786 - val_loss: 1150.5869 - val_mse: 1150.5869 - val_mae: 14.2151\n",
      "Epoch 728/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 41.0360 - mse: 41.0360 - mae: 3.8509 - val_loss: 967.2798 - val_mse: 967.2798 - val_mae: 14.1159\n",
      "Epoch 729/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 44.5589 - mse: 44.5589 - mae: 4.4608 - val_loss: 1377.4440 - val_mse: 1377.4440 - val_mae: 13.6629\n",
      "Epoch 730/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 27.3635 - mse: 27.3635 - mae: 3.5479 - val_loss: 1538.3995 - val_mse: 1538.3993 - val_mae: 15.7887\n",
      "Epoch 731/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 32.5618 - mse: 32.5618 - mae: 3.8459 - val_loss: 1169.0100 - val_mse: 1169.0100 - val_mae: 13.4655\n",
      "Epoch 732/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 12.0526 - mse: 12.0526 - mae: 2.3655 - val_loss: 1154.5210 - val_mse: 1154.5210 - val_mae: 13.7862\n",
      "Epoch 733/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6028 - mse: 11.6028 - mae: 2.2333 - val_loss: 1114.6394 - val_mse: 1114.6394 - val_mae: 13.1712\n",
      "Epoch 734/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.6602 - mse: 11.6602 - mae: 2.2706 - val_loss: 1092.1914 - val_mse: 1092.1914 - val_mae: 12.6396\n",
      "Epoch 735/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.9692 - mse: 6.9692 - mae: 1.8435 - val_loss: 1156.9559 - val_mse: 1156.9559 - val_mae: 13.2330\n",
      "Epoch 736/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.2536 - mse: 4.2536 - mae: 1.3360 - val_loss: 1148.8772 - val_mse: 1148.8772 - val_mae: 12.9523\n",
      "Epoch 737/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.2105 - mse: 3.2105 - mae: 1.2322 - val_loss: 1130.8846 - val_mse: 1130.8846 - val_mae: 12.9990\n",
      "Epoch 738/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8091 - mse: 2.8091 - mae: 1.1171 - val_loss: 1211.4146 - val_mse: 1211.4146 - val_mae: 13.5437\n",
      "Epoch 739/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.6146 - mse: 3.6146 - mae: 1.3845 - val_loss: 1218.0813 - val_mse: 1218.0813 - val_mae: 13.0422\n",
      "Epoch 740/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3450 - mse: 4.3450 - mae: 1.4958 - val_loss: 1074.5492 - val_mse: 1074.5492 - val_mae: 13.1214\n",
      "Epoch 741/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.9719 - mse: 8.9719 - mae: 1.9961 - val_loss: 1264.7600 - val_mse: 1264.7599 - val_mae: 13.9678\n",
      "Epoch 742/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.5979 - mse: 9.5979 - mae: 1.8965 - val_loss: 1320.3550 - val_mse: 1320.3550 - val_mae: 13.7908\n",
      "Epoch 743/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.5783 - mse: 8.5783 - mae: 1.8969 - val_loss: 994.0453 - val_mse: 994.0453 - val_mae: 12.9835\n",
      "Epoch 744/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.1095 - mse: 8.1095 - mae: 1.6775 - val_loss: 1128.3872 - val_mse: 1128.3872 - val_mae: 13.2938\n",
      "Epoch 745/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.2575 - mse: 8.2575 - mae: 1.9681 - val_loss: 1268.0698 - val_mse: 1268.0698 - val_mae: 13.8507\n",
      "Epoch 746/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 5.9045 - mse: 5.9045 - mae: 1.6439 - val_loss: 1261.0151 - val_mse: 1261.0151 - val_mae: 13.4286\n",
      "Epoch 747/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 7.4624 - mse: 7.4624 - mae: 1.8414 - val_loss: 1077.3932 - val_mse: 1077.3932 - val_mae: 12.6033\n",
      "Epoch 748/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 6.6180 - mse: 6.6180 - mae: 1.8305 - val_loss: 1108.3395 - val_mse: 1108.3395 - val_mae: 13.0076\n",
      "Epoch 749/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.3425 - mse: 6.3425 - mae: 1.8139 - val_loss: 1233.3602 - val_mse: 1233.3602 - val_mae: 13.3242\n",
      "Epoch 750/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.6957 - mse: 4.6957 - mae: 1.4781 - val_loss: 1307.8202 - val_mse: 1307.8202 - val_mae: 13.1745\n",
      "Epoch 751/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 6.2654 - mse: 6.2654 - mae: 1.6252 - val_loss: 1065.1392 - val_mse: 1065.1392 - val_mae: 13.3058\n",
      "Epoch 752/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 14.3287 - mse: 14.3287 - mae: 2.6064 - val_loss: 1198.4188 - val_mse: 1198.4188 - val_mae: 13.3868\n",
      "Epoch 753/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.5542 - mse: 11.5542 - mae: 2.4779 - val_loss: 1253.6597 - val_mse: 1253.6597 - val_mae: 13.8958\n",
      "Epoch 754/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0509 - mse: 7.0509 - mae: 1.9645 - val_loss: 1180.2278 - val_mse: 1180.2278 - val_mae: 13.1597\n",
      "Epoch 755/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.3401 - mse: 4.3401 - mae: 1.4920 - val_loss: 1122.4677 - val_mse: 1122.4677 - val_mae: 12.9613\n",
      "Epoch 756/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.6892 - mse: 3.6892 - mae: 1.3297 - val_loss: 1249.3456 - val_mse: 1249.3455 - val_mae: 13.3884\n",
      "Epoch 757/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.1154 - mse: 3.1154 - mae: 1.2488 - val_loss: 1181.1705 - val_mse: 1181.1705 - val_mae: 12.9386\n",
      "Epoch 758/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7434 - mse: 2.7434 - mae: 1.2671 - val_loss: 1189.1411 - val_mse: 1189.1411 - val_mae: 13.0500\n",
      "Epoch 759/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0954 - mse: 2.0954 - mae: 1.1579 - val_loss: 1171.3408 - val_mse: 1171.3408 - val_mae: 13.2453\n",
      "Epoch 760/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7611 - mse: 2.7611 - mae: 1.2980 - val_loss: 1128.5380 - val_mse: 1128.5380 - val_mae: 12.8281\n",
      "Epoch 761/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0275 - mse: 3.0275 - mae: 1.3130 - val_loss: 1168.4052 - val_mse: 1168.4052 - val_mae: 13.3952\n",
      "Epoch 762/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.0615 - mse: 4.0615 - mae: 1.0971 - val_loss: 1227.9905 - val_mse: 1227.9905 - val_mae: 13.2646\n",
      "Epoch 763/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4.5090 - mse: 4.5090 - mae: 1.3811 - val_loss: 1223.9371 - val_mse: 1223.9371 - val_mae: 13.1482\n",
      "Epoch 764/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.9972 - mse: 2.9972 - mae: 1.2988 - val_loss: 1165.8185 - val_mse: 1165.8185 - val_mae: 13.2316\n",
      "Epoch 765/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.9701 - mse: 1.9701 - mae: 1.1149 - val_loss: 1161.5254 - val_mse: 1161.5254 - val_mae: 13.1547\n",
      "Epoch 766/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2926 - mse: 1.2926 - mae: 0.7831 - val_loss: 1227.0215 - val_mse: 1227.0215 - val_mae: 13.1754\n",
      "Epoch 767/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2869 - mse: 1.2869 - mae: 0.8230 - val_loss: 1195.6378 - val_mse: 1195.6378 - val_mae: 13.1539\n",
      "Epoch 768/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1215 - mse: 1.1215 - mae: 0.7771 - val_loss: 1166.5575 - val_mse: 1166.5575 - val_mae: 13.1756\n",
      "Epoch 769/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0598 - mse: 1.0598 - mae: 0.8091 - val_loss: 1212.2186 - val_mse: 1212.2186 - val_mae: 13.2227\n",
      "Epoch 770/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1437 - mse: 1.1437 - mae: 0.7829 - val_loss: 1185.0579 - val_mse: 1185.0579 - val_mae: 13.1063\n",
      "Epoch 771/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3543 - mse: 1.3543 - mae: 0.7488 - val_loss: 1130.8737 - val_mse: 1130.8737 - val_mae: 12.9847\n",
      "Epoch 772/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2423 - mse: 1.2423 - mae: 0.7449 - val_loss: 1187.9143 - val_mse: 1187.9143 - val_mae: 13.2195\n",
      "Epoch 773/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9509 - mse: 0.9509 - mae: 0.6402 - val_loss: 1236.4668 - val_mse: 1236.4668 - val_mae: 13.3321\n",
      "Epoch 774/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9322 - mse: 0.9322 - mae: 0.6703 - val_loss: 1173.7288 - val_mse: 1173.7288 - val_mae: 13.0687\n",
      "Epoch 775/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0700 - mse: 1.0700 - mae: 0.7368 - val_loss: 1186.1128 - val_mse: 1186.1128 - val_mae: 13.1981\n",
      "Epoch 776/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9249 - mse: 0.9249 - mae: 0.6969 - val_loss: 1182.9169 - val_mse: 1182.9169 - val_mae: 13.1284\n",
      "Epoch 777/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.1471 - mse: 1.1471 - mae: 0.6759 - val_loss: 1260.7050 - val_mse: 1260.7050 - val_mae: 13.4212\n",
      "Epoch 778/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8670 - mse: 1.8670 - mae: 0.9304 - val_loss: 1163.6755 - val_mse: 1163.6755 - val_mae: 13.0012\n",
      "Epoch 779/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0212 - mse: 1.0212 - mae: 0.7624 - val_loss: 1186.7744 - val_mse: 1186.7744 - val_mae: 13.1123\n",
      "Epoch 780/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.0209 - mse: 1.0209 - mae: 0.7733 - val_loss: 1205.4277 - val_mse: 1205.4277 - val_mae: 13.2990\n",
      "Epoch 781/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7370 - mse: 0.7370 - mae: 0.6404 - val_loss: 1196.2908 - val_mse: 1196.2908 - val_mae: 13.1017\n",
      "Epoch 782/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.4609 - mse: 0.4609 - mae: 0.4489 - val_loss: 1193.0184 - val_mse: 1193.0184 - val_mae: 13.2116\n",
      "Epoch 783/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5420 - mse: 0.5420 - mae: 0.5095 - val_loss: 1204.1476 - val_mse: 1204.1476 - val_mae: 13.0156\n",
      "Epoch 784/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5840 - mse: 0.5840 - mae: 0.4784 - val_loss: 1158.1924 - val_mse: 1158.1924 - val_mae: 13.0099\n",
      "Epoch 785/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7157 - mse: 0.7157 - mae: 0.5350 - val_loss: 1179.3967 - val_mse: 1179.3967 - val_mae: 13.0442\n",
      "Epoch 786/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7531 - mse: 0.7531 - mae: 0.5333 - val_loss: 1199.1858 - val_mse: 1199.1858 - val_mae: 13.0430\n",
      "Epoch 787/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6620 - mse: 0.6620 - mae: 0.5379 - val_loss: 1206.6172 - val_mse: 1206.6172 - val_mae: 13.1822\n",
      "Epoch 788/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8019 - mse: 0.8019 - mae: 0.5901 - val_loss: 1194.6812 - val_mse: 1194.6812 - val_mae: 13.1454\n",
      "Epoch 789/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4817 - mse: 0.4817 - mae: 0.4524 - val_loss: 1179.1608 - val_mse: 1179.1608 - val_mae: 12.9956\n",
      "Epoch 790/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5406 - mse: 0.5406 - mae: 0.4499 - val_loss: 1182.6711 - val_mse: 1182.6711 - val_mae: 13.0710\n",
      "Epoch 791/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.7080 - mse: 0.7080 - mae: 0.4982 - val_loss: 1193.7611 - val_mse: 1193.7611 - val_mae: 13.0822\n",
      "Epoch 792/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6014 - mse: 0.6014 - mae: 0.5125 - val_loss: 1196.6458 - val_mse: 1196.6458 - val_mae: 13.1862\n",
      "Epoch 793/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5880 - mse: 0.5880 - mae: 0.5172 - val_loss: 1212.1853 - val_mse: 1212.1853 - val_mae: 13.1308\n",
      "Epoch 794/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5267 - mse: 0.5267 - mae: 0.5048 - val_loss: 1180.7174 - val_mse: 1180.7174 - val_mae: 13.0421\n",
      "Epoch 795/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4404 - mse: 0.4404 - mae: 0.4025 - val_loss: 1195.4897 - val_mse: 1195.4897 - val_mae: 13.0819\n",
      "Epoch 796/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4134 - mse: 0.4134 - mae: 0.4538 - val_loss: 1211.9852 - val_mse: 1211.9852 - val_mae: 13.0799\n",
      "Epoch 797/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3900 - mse: 0.3900 - mae: 0.4086 - val_loss: 1197.7069 - val_mse: 1197.7069 - val_mae: 13.1235\n",
      "Epoch 798/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3990 - mse: 0.3990 - mae: 0.3927 - val_loss: 1190.3948 - val_mse: 1190.3948 - val_mae: 13.0632\n",
      "Epoch 799/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3173 - mse: 0.3173 - mae: 0.3227 - val_loss: 1192.3916 - val_mse: 1192.3916 - val_mae: 13.0323\n",
      "Epoch 800/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3730 - mse: 0.3730 - mae: 0.3450 - val_loss: 1208.9010 - val_mse: 1208.9010 - val_mae: 13.0419\n",
      "Epoch 801/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2958 - mse: 0.2958 - mae: 0.3240 - val_loss: 1207.4712 - val_mse: 1207.4712 - val_mae: 13.1448\n",
      "Epoch 802/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2647 - mse: 0.2647 - mae: 0.3228 - val_loss: 1183.7156 - val_mse: 1183.7156 - val_mae: 13.0492\n",
      "Epoch 803/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3523 - mse: 0.3523 - mae: 0.3680 - val_loss: 1183.4128 - val_mse: 1183.4128 - val_mae: 12.9932\n",
      "Epoch 804/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3168 - mse: 0.3168 - mae: 0.3738 - val_loss: 1193.3835 - val_mse: 1193.3835 - val_mae: 13.1060\n",
      "Epoch 805/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2718 - mse: 0.2718 - mae: 0.3172 - val_loss: 1209.0966 - val_mse: 1209.0966 - val_mae: 13.1015\n",
      "Epoch 806/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3184 - mse: 0.3184 - mae: 0.3668 - val_loss: 1193.9763 - val_mse: 1193.9763 - val_mae: 13.1525\n",
      "Epoch 807/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.3875 - val_loss: 1183.1754 - val_mse: 1183.1754 - val_mae: 13.0639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 808/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3055 - mse: 0.3055 - mae: 0.3698 - val_loss: 1183.7902 - val_mse: 1183.7902 - val_mae: 13.0643\n",
      "Epoch 809/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3296 - mse: 0.3296 - mae: 0.3466 - val_loss: 1198.7791 - val_mse: 1198.7791 - val_mae: 13.1303\n",
      "Epoch 810/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4671 - mse: 0.4671 - mae: 0.4305 - val_loss: 1236.6572 - val_mse: 1236.6572 - val_mae: 13.3258\n",
      "Epoch 811/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5174 - mse: 0.5174 - mae: 0.5233 - val_loss: 1222.2041 - val_mse: 1222.2041 - val_mae: 13.0963\n",
      "Epoch 812/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5680 - mse: 0.5680 - mae: 0.5074 - val_loss: 1161.8411 - val_mse: 1161.8411 - val_mae: 13.0403\n",
      "Epoch 813/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6057 - mse: 0.6057 - mae: 0.4964 - val_loss: 1180.0159 - val_mse: 1180.0159 - val_mae: 13.0587\n",
      "Epoch 814/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6161 - mse: 0.6161 - mae: 0.5214 - val_loss: 1225.2468 - val_mse: 1225.2468 - val_mae: 13.0510\n",
      "Epoch 815/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5967 - mse: 0.5967 - mae: 0.5055 - val_loss: 1229.3728 - val_mse: 1229.3728 - val_mae: 13.2442\n",
      "Epoch 816/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4227 - mse: 0.4227 - mae: 0.4351 - val_loss: 1195.5662 - val_mse: 1195.5662 - val_mae: 13.0869\n",
      "Epoch 817/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3028 - mse: 0.3028 - mae: 0.3462 - val_loss: 1178.1545 - val_mse: 1178.1545 - val_mae: 13.0417\n",
      "Epoch 818/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2959 - mse: 0.2959 - mae: 0.3258 - val_loss: 1190.3301 - val_mse: 1190.3301 - val_mae: 13.0197\n",
      "Epoch 819/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2250 - mse: 0.2250 - mae: 0.3001 - val_loss: 1189.8323 - val_mse: 1189.8323 - val_mae: 13.0546\n",
      "Epoch 820/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2200 - mse: 0.2200 - mae: 0.2754 - val_loss: 1178.6627 - val_mse: 1178.6627 - val_mae: 13.0254\n",
      "Epoch 821/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3971 - mse: 0.3971 - mae: 0.4247 - val_loss: 1181.0455 - val_mse: 1181.0455 - val_mae: 13.0591\n",
      "Epoch 822/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4030 - mse: 0.4030 - mae: 0.4125 - val_loss: 1228.2660 - val_mse: 1228.2660 - val_mae: 13.2978\n",
      "Epoch 823/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4608 - mse: 0.4608 - mae: 0.4622 - val_loss: 1187.3207 - val_mse: 1187.3207 - val_mae: 13.0372\n",
      "Epoch 824/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3499 - mse: 0.3499 - mae: 0.4092 - val_loss: 1196.3745 - val_mse: 1196.3745 - val_mae: 13.0885\n",
      "Epoch 825/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3382 - mse: 0.3382 - mae: 0.4122 - val_loss: 1184.4330 - val_mse: 1184.4330 - val_mae: 13.0673\n",
      "Epoch 826/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3692 - mse: 0.3692 - mae: 0.4132 - val_loss: 1200.2262 - val_mse: 1200.2262 - val_mae: 13.0580\n",
      "Epoch 827/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2528 - mse: 0.2528 - mae: 0.3333 - val_loss: 1195.7964 - val_mse: 1195.7964 - val_mae: 13.0954\n",
      "Epoch 828/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2024 - mse: 0.2024 - mae: 0.2643 - val_loss: 1202.3779 - val_mse: 1202.3779 - val_mae: 13.0447\n",
      "Epoch 829/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.2846 - val_loss: 1203.8002 - val_mse: 1203.8002 - val_mae: 13.0952\n",
      "Epoch 830/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2616 - mse: 0.2616 - mae: 0.3482 - val_loss: 1190.4115 - val_mse: 1190.4115 - val_mae: 13.0419\n",
      "Epoch 831/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2098 - mse: 0.2098 - mae: 0.2887 - val_loss: 1185.0718 - val_mse: 1185.0718 - val_mae: 13.0353\n",
      "Epoch 832/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2262 - mse: 0.2262 - mae: 0.2899 - val_loss: 1188.9520 - val_mse: 1188.9520 - val_mae: 13.0243\n",
      "Epoch 833/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2085 - mse: 0.2085 - mae: 0.2937 - val_loss: 1189.8882 - val_mse: 1189.8882 - val_mae: 13.0666\n",
      "Epoch 834/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1883 - mse: 0.1883 - mae: 0.2596 - val_loss: 1199.2799 - val_mse: 1199.2799 - val_mae: 13.0465\n",
      "Epoch 835/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2114 - mse: 0.2114 - mae: 0.2684 - val_loss: 1202.3527 - val_mse: 1202.3527 - val_mae: 13.1751\n",
      "Epoch 836/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2000 - mse: 0.2000 - mae: 0.2900 - val_loss: 1198.9034 - val_mse: 1198.9034 - val_mae: 13.0941\n",
      "Epoch 837/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2106 - mse: 0.2106 - mae: 0.2765 - val_loss: 1179.6187 - val_mse: 1179.6187 - val_mae: 13.0321\n",
      "Epoch 838/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2507 - mse: 0.2507 - mae: 0.2779 - val_loss: 1186.4326 - val_mse: 1186.4326 - val_mae: 13.0970\n",
      "Epoch 839/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2186 - mse: 0.2186 - mae: 0.3097 - val_loss: 1197.6401 - val_mse: 1197.6401 - val_mae: 13.0491\n",
      "Epoch 840/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2316 - mse: 0.2316 - mae: 0.2860 - val_loss: 1214.2203 - val_mse: 1214.2203 - val_mae: 13.1725\n",
      "Epoch 841/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2532 - mse: 0.2532 - mae: 0.3009 - val_loss: 1206.4341 - val_mse: 1206.4341 - val_mae: 13.1393\n",
      "Epoch 842/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2228 - mse: 0.2228 - mae: 0.3300 - val_loss: 1200.2670 - val_mse: 1200.2670 - val_mae: 13.0883\n",
      "Epoch 843/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2090 - mse: 0.2090 - mae: 0.2627 - val_loss: 1174.5365 - val_mse: 1174.5365 - val_mae: 13.0724\n",
      "Epoch 844/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2371 - mse: 0.2371 - mae: 0.3106 - val_loss: 1179.2216 - val_mse: 1179.2216 - val_mae: 13.0233\n",
      "Epoch 845/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2340 - mse: 0.2340 - mae: 0.3084 - val_loss: 1198.1074 - val_mse: 1198.1074 - val_mae: 13.1127\n",
      "Epoch 846/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.1928 - mse: 0.1928 - mae: 0.2585 - val_loss: 1191.0004 - val_mse: 1191.0004 - val_mae: 13.0787\n",
      "Epoch 847/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2554 - mse: 0.2554 - mae: 0.2976 - val_loss: 1202.0400 - val_mse: 1202.0400 - val_mae: 13.1955\n",
      "Epoch 848/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3837 - mse: 0.3837 - mae: 0.4075 - val_loss: 1214.3866 - val_mse: 1214.3866 - val_mae: 13.0879\n",
      "Epoch 849/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.3457 - val_loss: 1210.2379 - val_mse: 1210.2379 - val_mae: 13.2140\n",
      "Epoch 850/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3230 - mse: 0.3230 - mae: 0.3770 - val_loss: 1201.7720 - val_mse: 1201.7720 - val_mae: 13.1920\n",
      "Epoch 851/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6943 - mse: 0.6943 - mae: 0.5937 - val_loss: 1162.7832 - val_mse: 1162.7832 - val_mae: 12.9756\n",
      "Epoch 852/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6642 - mse: 0.6642 - mae: 0.5738 - val_loss: 1168.0474 - val_mse: 1168.0474 - val_mae: 12.9691\n",
      "Epoch 853/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8151 - mse: 0.8151 - mae: 0.6280 - val_loss: 1200.7053 - val_mse: 1200.7053 - val_mae: 13.1804\n",
      "Epoch 854/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5911 - mse: 0.5911 - mae: 0.5082 - val_loss: 1218.8879 - val_mse: 1218.8879 - val_mae: 13.2147\n",
      "Epoch 855/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4277 - mse: 0.4277 - mae: 0.4478 - val_loss: 1185.1940 - val_mse: 1185.1940 - val_mae: 13.0125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 856/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5187 - mse: 0.5187 - mae: 0.4553 - val_loss: 1219.6876 - val_mse: 1219.6876 - val_mae: 13.2633\n",
      "Epoch 857/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3245 - mse: 1.3245 - mae: 0.6151 - val_loss: 1288.8159 - val_mse: 1288.8159 - val_mae: 13.4352\n",
      "Epoch 858/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.1494 - mse: 2.1494 - mae: 0.8238 - val_loss: 1257.9147 - val_mse: 1257.9146 - val_mae: 13.4199\n",
      "Epoch 859/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7747 - mse: 2.7747 - mae: 1.0786 - val_loss: 1198.1067 - val_mse: 1198.1067 - val_mae: 13.1904\n",
      "Epoch 860/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.1492 - mse: 3.1492 - mae: 1.0685 - val_loss: 1119.1211 - val_mse: 1119.1211 - val_mae: 12.7766\n",
      "Epoch 861/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0218 - mse: 3.0218 - mae: 1.0715 - val_loss: 1140.5831 - val_mse: 1140.5831 - val_mae: 13.1203\n",
      "Epoch 862/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7423 - mse: 1.7423 - mae: 0.9672 - val_loss: 1186.9181 - val_mse: 1186.9181 - val_mae: 13.0198\n",
      "Epoch 863/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3305 - mse: 1.3305 - mae: 0.8243 - val_loss: 1215.1150 - val_mse: 1215.1150 - val_mae: 13.3029\n",
      "Epoch 864/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.4729 - mse: 1.4729 - mae: 0.8999 - val_loss: 1180.5616 - val_mse: 1180.5616 - val_mae: 13.0848\n",
      "Epoch 865/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.9879 - mse: 0.9879 - mae: 0.6976 - val_loss: 1168.2563 - val_mse: 1168.2563 - val_mae: 12.8977\n",
      "Epoch 866/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1878 - mse: 1.1878 - mae: 0.7287 - val_loss: 1144.9260 - val_mse: 1144.9260 - val_mae: 12.8148\n",
      "Epoch 867/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.0678 - mse: 2.0678 - mae: 0.8443 - val_loss: 1161.5710 - val_mse: 1161.5710 - val_mae: 13.0716\n",
      "Epoch 868/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.4060 - mse: 3.4060 - mae: 1.0757 - val_loss: 1087.9331 - val_mse: 1087.9331 - val_mae: 12.9341\n",
      "Epoch 869/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7375 - mse: 8.7375 - mae: 1.6713 - val_loss: 1254.2377 - val_mse: 1254.2375 - val_mae: 13.3702\n",
      "Epoch 870/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.7176 - mse: 2.7176 - mae: 1.2057 - val_loss: 1278.5890 - val_mse: 1278.5890 - val_mae: 13.2944\n",
      "Epoch 871/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8356 - mse: 2.8356 - mae: 1.2491 - val_loss: 1228.4437 - val_mse: 1228.4437 - val_mae: 13.3260\n",
      "Epoch 872/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5730 - mse: 3.5730 - mae: 1.2533 - val_loss: 1115.4250 - val_mse: 1115.4250 - val_mae: 12.8836\n",
      "Epoch 873/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.3982 - mse: 5.3982 - mae: 1.5235 - val_loss: 1031.7104 - val_mse: 1031.7104 - val_mae: 12.6711\n",
      "Epoch 874/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 9.8802 - mse: 9.8802 - mae: 1.9561 - val_loss: 1055.0674 - val_mse: 1055.0674 - val_mae: 13.0644\n",
      "Epoch 875/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 32.8072 - mse: 32.8072 - mae: 3.1231 - val_loss: 1367.9623 - val_mse: 1367.9622 - val_mae: 13.3555\n",
      "Epoch 876/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 43.9439 - mse: 43.9439 - mae: 4.0921 - val_loss: 1321.4430 - val_mse: 1321.4429 - val_mae: 14.4385\n",
      "Epoch 877/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 33.9808 - mse: 33.9808 - mae: 3.7833 - val_loss: 1680.3427 - val_mse: 1680.3427 - val_mae: 16.7715\n",
      "Epoch 878/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 93.7250 - mse: 93.7250 - mae: 5.6884 - val_loss: 1719.7871 - val_mse: 1719.7871 - val_mae: 18.8712\n",
      "Epoch 879/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 105.6032 - mse: 105.6032 - mae: 7.6103 - val_loss: 1585.2677 - val_mse: 1585.2677 - val_mae: 15.0795\n",
      "Epoch 880/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 200.5103 - mse: 200.5103 - mae: 7.9638 - val_loss: 1069.5341 - val_mse: 1069.5341 - val_mae: 15.8491\n",
      "Epoch 881/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 183.3049 - mse: 183.3049 - mae: 8.8978 - val_loss: 1166.5415 - val_mse: 1166.5415 - val_mae: 19.6612\n",
      "Epoch 882/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 180.6710 - mse: 180.6710 - mae: 9.5159 - val_loss: 1213.2686 - val_mse: 1213.2686 - val_mae: 16.1693\n",
      "Epoch 883/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 85.2078 - mse: 85.2078 - mae: 7.0608 - val_loss: 1199.5132 - val_mse: 1199.5132 - val_mae: 13.2007\n",
      "Epoch 884/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 37.5280 - mse: 37.5280 - mae: 4.7083 - val_loss: 1102.2802 - val_mse: 1102.2802 - val_mae: 14.7312\n",
      "Epoch 885/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 30.6063 - mse: 30.6063 - mae: 4.0713 - val_loss: 1278.6627 - val_mse: 1278.6627 - val_mae: 13.3893\n",
      "Epoch 886/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 15.8862 - mse: 15.8862 - mae: 2.9994 - val_loss: 1085.2859 - val_mse: 1085.2859 - val_mae: 13.6470\n",
      "Epoch 887/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17.6132 - mse: 17.6132 - mae: 3.0024 - val_loss: 1226.7388 - val_mse: 1226.7388 - val_mae: 13.4381\n",
      "Epoch 888/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 13.5503 - mse: 13.5503 - mae: 2.8492 - val_loss: 1322.6987 - val_mse: 1322.6987 - val_mae: 14.0111\n",
      "Epoch 889/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.2967 - mse: 11.2967 - mae: 2.6835 - val_loss: 1151.6125 - val_mse: 1151.6125 - val_mae: 13.3569\n",
      "Epoch 890/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 10.9686 - mse: 10.9686 - mae: 2.2749 - val_loss: 984.7457 - val_mse: 984.7457 - val_mae: 13.2595\n",
      "Epoch 891/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 16.4369 - mse: 16.4369 - mae: 2.7649 - val_loss: 1013.1567 - val_mse: 1013.1567 - val_mae: 12.5804\n",
      "Epoch 892/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 8.7767 - mse: 8.7767 - mae: 2.2475 - val_loss: 1140.6948 - val_mse: 1140.6948 - val_mae: 13.4364\n",
      "Epoch 893/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.0269 - mse: 7.0269 - mae: 1.9206 - val_loss: 1042.4154 - val_mse: 1042.4154 - val_mae: 13.0336\n",
      "Epoch 894/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.9788 - mse: 8.9788 - mae: 1.9961 - val_loss: 982.0234 - val_mse: 982.0234 - val_mae: 12.6704\n",
      "Epoch 895/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5978 - mse: 5.5978 - mae: 1.6533 - val_loss: 1061.1127 - val_mse: 1061.1127 - val_mae: 13.1000\n",
      "Epoch 896/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.4245 - mse: 5.4245 - mae: 1.5984 - val_loss: 1188.4701 - val_mse: 1188.4701 - val_mae: 13.3638\n",
      "Epoch 897/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4.2884 - mse: 4.2884 - mae: 1.4985 - val_loss: 1124.5997 - val_mse: 1124.5997 - val_mae: 13.1223\n",
      "Epoch 898/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3.5499 - mse: 3.5499 - mae: 1.3157 - val_loss: 1080.8586 - val_mse: 1080.8586 - val_mae: 12.9736\n",
      "Epoch 899/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.6419 - mse: 2.6419 - mae: 1.1292 - val_loss: 1079.5812 - val_mse: 1079.5812 - val_mae: 12.9019\n",
      "Epoch 900/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0710 - mse: 2.0710 - mae: 1.0601 - val_loss: 1141.6814 - val_mse: 1141.6814 - val_mae: 13.1072\n",
      "Epoch 901/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.0631 - mse: 3.0631 - mae: 1.0737 - val_loss: 1140.4705 - val_mse: 1140.4705 - val_mae: 13.1012\n",
      "Epoch 902/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5448 - mse: 2.5448 - mae: 1.0996 - val_loss: 1042.3230 - val_mse: 1042.3230 - val_mae: 12.6814\n",
      "Epoch 903/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.0994 - mse: 2.0994 - mae: 1.0216 - val_loss: 1150.1460 - val_mse: 1150.1460 - val_mae: 13.0222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 904/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.5688 - mse: 2.5688 - mae: 1.1243 - val_loss: 1137.7170 - val_mse: 1137.7170 - val_mae: 13.0652\n",
      "Epoch 905/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2.8974 - mse: 2.8974 - mae: 1.1564 - val_loss: 1093.5789 - val_mse: 1093.5789 - val_mae: 12.8798\n",
      "Epoch 906/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.6693 - mse: 1.6693 - mae: 0.9235 - val_loss: 1055.6981 - val_mse: 1055.6981 - val_mae: 12.7388\n",
      "Epoch 907/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.7138 - mse: 1.7138 - mae: 0.9298 - val_loss: 1093.1304 - val_mse: 1093.1304 - val_mae: 12.9075\n",
      "Epoch 908/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5652 - mse: 1.5652 - mae: 0.8634 - val_loss: 1108.9779 - val_mse: 1108.9779 - val_mae: 12.9079\n",
      "Epoch 909/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3642 - mse: 1.3642 - mae: 0.7492 - val_loss: 1040.1606 - val_mse: 1040.1606 - val_mae: 12.8278\n",
      "Epoch 910/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8424 - mse: 1.8424 - mae: 0.8446 - val_loss: 1037.2969 - val_mse: 1037.2969 - val_mae: 12.6940\n",
      "Epoch 911/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8580 - mse: 1.8580 - mae: 0.8488 - val_loss: 1094.8793 - val_mse: 1094.8793 - val_mae: 12.7759\n",
      "Epoch 912/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2088 - mse: 1.2088 - mae: 0.7580 - val_loss: 1133.7136 - val_mse: 1133.7136 - val_mae: 13.0287\n",
      "Epoch 913/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.3466 - mse: 1.3466 - mae: 0.7694 - val_loss: 1135.0011 - val_mse: 1135.0011 - val_mae: 12.9147\n",
      "Epoch 914/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.2527 - mse: 1.2527 - mae: 0.7282 - val_loss: 1086.2258 - val_mse: 1086.2258 - val_mae: 12.7663\n",
      "Epoch 915/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8006 - mse: 0.8006 - mae: 0.5897 - val_loss: 1082.6654 - val_mse: 1082.6654 - val_mae: 12.8172\n",
      "Epoch 916/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.1001 - mse: 1.1001 - mae: 0.6885 - val_loss: 1076.1804 - val_mse: 1076.1804 - val_mae: 12.7182\n",
      "Epoch 917/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8169 - mse: 0.8169 - mae: 0.6583 - val_loss: 1087.1749 - val_mse: 1087.1749 - val_mae: 12.8647\n",
      "Epoch 918/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6351 - mse: 0.6351 - mae: 0.5735 - val_loss: 1098.0989 - val_mse: 1098.0989 - val_mae: 12.9129\n",
      "Epoch 919/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5395 - mse: 0.5395 - mae: 0.4898 - val_loss: 1080.6422 - val_mse: 1080.6422 - val_mae: 12.7619\n",
      "Epoch 920/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5433 - mse: 0.5433 - mae: 0.4992 - val_loss: 1068.3625 - val_mse: 1068.3625 - val_mae: 12.7891\n",
      "Epoch 921/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6552 - mse: 0.6552 - mae: 0.5093 - val_loss: 1070.1229 - val_mse: 1070.1229 - val_mae: 12.7673\n",
      "Epoch 922/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.7857 - mse: 0.7857 - mae: 0.6196 - val_loss: 1122.1669 - val_mse: 1122.1669 - val_mae: 13.0534\n",
      "Epoch 923/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1.8924 - mse: 1.8924 - mae: 0.8253 - val_loss: 1144.6198 - val_mse: 1144.6198 - val_mae: 12.9816\n",
      "Epoch 924/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 8.7336 - mse: 8.7336 - mae: 1.5974 - val_loss: 912.0160 - val_mse: 912.0160 - val_mae: 12.8764\n",
      "Epoch 925/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 17.5918 - mse: 17.5918 - mae: 2.8480 - val_loss: 1059.2682 - val_mse: 1059.2682 - val_mae: 13.2434\n",
      "Epoch 926/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 18.3971 - mse: 18.3971 - mae: 3.0275 - val_loss: 1064.3240 - val_mse: 1064.3240 - val_mae: 13.0282\n",
      "Epoch 927/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 22.9800 - mse: 22.9800 - mae: 3.0650 - val_loss: 1091.2941 - val_mse: 1091.2941 - val_mae: 13.6559\n",
      "Epoch 928/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.0136 - mse: 24.0136 - mae: 3.4556 - val_loss: 1127.4404 - val_mse: 1127.4404 - val_mae: 13.8182\n",
      "Epoch 929/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 24.0602 - mse: 24.0602 - mae: 3.3135 - val_loss: 1371.1154 - val_mse: 1371.1154 - val_mae: 14.2069\n",
      "Epoch 930/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 39.9700 - mse: 39.9700 - mae: 3.7895 - val_loss: 976.7740 - val_mse: 976.7740 - val_mae: 13.5308\n",
      "Epoch 931/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 11.8137 - mse: 11.8137 - mae: 2.3661 - val_loss: 1069.3480 - val_mse: 1069.3480 - val_mae: 12.8990\n",
      "Epoch 932/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 10.4697 - mse: 10.4697 - mae: 2.3929 - val_loss: 1109.9314 - val_mse: 1109.9314 - val_mae: 12.9521\n",
      "Epoch 933/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 7.2154 - mse: 7.2154 - mae: 2.1510 - val_loss: 1013.9950 - val_mse: 1013.9950 - val_mae: 12.8620\n",
      "Epoch 934/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 5.5659 - mse: 5.5659 - mae: 1.7942 - val_loss: 1013.4623 - val_mse: 1013.4623 - val_mae: 12.5902\n",
      "Epoch 935/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3.5975 - mse: 3.5975 - mae: 1.3127 - val_loss: 1076.6669 - val_mse: 1076.6669 - val_mae: 13.0903\n",
      "Epoch 936/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2.7099 - mse: 2.7099 - mae: 1.2644 - val_loss: 1061.5553 - val_mse: 1061.5553 - val_mae: 12.8114\n",
      "Epoch 937/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.8587 - mse: 1.8587 - mae: 0.9994 - val_loss: 1049.9897 - val_mse: 1049.9897 - val_mae: 12.7899\n",
      "Epoch 938/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.5835 - mse: 1.5835 - mae: 0.9660 - val_loss: 1013.4184 - val_mse: 1013.4184 - val_mae: 12.6903\n",
      "Epoch 939/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1.3682 - mse: 1.3682 - mae: 0.8593 - val_loss: 1077.6530 - val_mse: 1077.6530 - val_mae: 12.7856\n",
      "Epoch 940/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.8512 - mse: 0.8512 - mae: 0.6765 - val_loss: 1062.5873 - val_mse: 1062.5873 - val_mae: 12.8345\n",
      "Epoch 941/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.8554 - mse: 0.8554 - mae: 0.6517 - val_loss: 1044.8346 - val_mse: 1044.8346 - val_mae: 12.6328\n",
      "Epoch 942/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5201 - mse: 0.5201 - mae: 0.4967 - val_loss: 1044.8737 - val_mse: 1044.8737 - val_mae: 12.6581\n",
      "Epoch 943/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5574 - mse: 0.5574 - mae: 0.4864 - val_loss: 1040.6323 - val_mse: 1040.6323 - val_mae: 12.7588\n",
      "Epoch 944/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5504 - mse: 0.5504 - mae: 0.4957 - val_loss: 1044.2289 - val_mse: 1044.2289 - val_mae: 12.7263\n",
      "Epoch 945/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4392 - mse: 0.4392 - mae: 0.4424 - val_loss: 1027.5870 - val_mse: 1027.5870 - val_mae: 12.6719\n",
      "Epoch 946/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5233 - mse: 0.5233 - mae: 0.4855 - val_loss: 1047.8845 - val_mse: 1047.8845 - val_mae: 12.7662\n",
      "Epoch 947/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3943 - mse: 0.3943 - mae: 0.4106 - val_loss: 1033.1885 - val_mse: 1033.1885 - val_mae: 12.6346\n",
      "Epoch 948/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3428 - mse: 0.3428 - mae: 0.3810 - val_loss: 1039.2954 - val_mse: 1039.2954 - val_mae: 12.7145\n",
      "Epoch 949/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3391 - mse: 0.3391 - mae: 0.3753 - val_loss: 1047.0597 - val_mse: 1047.0597 - val_mae: 12.7447\n",
      "Epoch 950/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3490 - mse: 0.3490 - mae: 0.3471 - val_loss: 1046.3499 - val_mse: 1046.3499 - val_mae: 12.7427\n",
      "Epoch 951/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3801 - mse: 0.3801 - mae: 0.3822 - val_loss: 1054.1752 - val_mse: 1054.1752 - val_mae: 12.7708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 952/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3297 - mse: 0.3297 - mae: 0.3692 - val_loss: 1039.6727 - val_mse: 1039.6727 - val_mae: 12.6632\n",
      "Epoch 953/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4554 - mse: 0.4554 - mae: 0.4146 - val_loss: 1029.1462 - val_mse: 1029.1462 - val_mae: 12.7485\n",
      "Epoch 954/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4362 - mse: 0.4362 - mae: 0.4100 - val_loss: 1049.9923 - val_mse: 1049.9923 - val_mae: 12.8127\n",
      "Epoch 955/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3011 - mse: 0.3011 - mae: 0.3500 - val_loss: 1065.3267 - val_mse: 1065.3267 - val_mae: 12.7525\n",
      "Epoch 956/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3583 - mse: 0.3583 - mae: 0.3584 - val_loss: 1048.7074 - val_mse: 1048.7074 - val_mae: 12.8085\n",
      "Epoch 957/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3457 - mse: 0.3457 - mae: 0.3631 - val_loss: 1047.5258 - val_mse: 1047.5258 - val_mae: 12.7076\n",
      "Epoch 958/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3206 - mse: 0.3206 - mae: 0.3426 - val_loss: 1052.7628 - val_mse: 1052.7628 - val_mae: 12.7918\n",
      "Epoch 959/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2630 - mse: 0.2630 - mae: 0.3014 - val_loss: 1047.5928 - val_mse: 1047.5928 - val_mae: 12.7591\n",
      "Epoch 960/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2626 - mse: 0.2626 - mae: 0.3088 - val_loss: 1044.5419 - val_mse: 1044.5419 - val_mae: 12.6891\n",
      "Epoch 961/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2745 - mse: 0.2745 - mae: 0.3010 - val_loss: 1041.7036 - val_mse: 1041.7036 - val_mae: 12.7173\n",
      "Epoch 962/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3121 - mse: 0.3121 - mae: 0.3544 - val_loss: 1053.1073 - val_mse: 1053.1073 - val_mae: 12.7955\n",
      "Epoch 963/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3119 - mse: 0.3119 - mae: 0.3232 - val_loss: 1055.3933 - val_mse: 1055.3933 - val_mae: 12.7347\n",
      "Epoch 964/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3302 - mse: 0.3302 - mae: 0.3293 - val_loss: 1050.7970 - val_mse: 1050.7970 - val_mae: 12.8332\n",
      "Epoch 965/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3444 - mse: 0.3444 - mae: 0.3472 - val_loss: 1069.5863 - val_mse: 1069.5863 - val_mae: 12.7305\n",
      "Epoch 966/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4191 - mse: 0.4191 - mae: 0.3979 - val_loss: 1068.1718 - val_mse: 1068.1718 - val_mae: 12.8088\n",
      "Epoch 967/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3821 - mse: 0.3821 - mae: 0.4021 - val_loss: 1051.2723 - val_mse: 1051.2723 - val_mae: 12.7273\n",
      "Epoch 968/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3591 - mse: 0.3591 - mae: 0.4414 - val_loss: 1043.9458 - val_mse: 1043.9458 - val_mae: 12.7705\n",
      "Epoch 969/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3824 - mse: 0.3824 - mae: 0.3399 - val_loss: 1055.7451 - val_mse: 1055.7451 - val_mae: 12.7911\n",
      "Epoch 970/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3257 - mse: 0.3257 - mae: 0.3628 - val_loss: 1055.1693 - val_mse: 1055.1693 - val_mae: 12.8037\n",
      "Epoch 971/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3194 - mse: 0.3194 - mae: 0.3488 - val_loss: 1044.1267 - val_mse: 1044.1267 - val_mae: 12.7400\n",
      "Epoch 972/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.2944 - val_loss: 1050.2809 - val_mse: 1050.2809 - val_mae: 12.7074\n",
      "Epoch 973/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2223 - mse: 0.2223 - mae: 0.2660 - val_loss: 1051.7598 - val_mse: 1051.7598 - val_mae: 12.8208\n",
      "Epoch 974/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2446 - mse: 0.2446 - mae: 0.2735 - val_loss: 1045.9545 - val_mse: 1045.9545 - val_mae: 12.7611\n",
      "Epoch 975/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2428 - mse: 0.2428 - mae: 0.2734 - val_loss: 1044.4197 - val_mse: 1044.4197 - val_mae: 12.7079\n",
      "Epoch 976/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2927 - mse: 0.2927 - mae: 0.3418 - val_loss: 1052.3469 - val_mse: 1052.3469 - val_mae: 12.7706\n",
      "Epoch 977/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2808 - mse: 0.2808 - mae: 0.3258 - val_loss: 1065.6436 - val_mse: 1065.6436 - val_mae: 12.7743\n",
      "Epoch 978/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3357 - mse: 0.3357 - mae: 0.3867 - val_loss: 1046.0731 - val_mse: 1046.0731 - val_mae: 12.7819\n",
      "Epoch 979/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3395 - val_loss: 1045.2167 - val_mse: 1045.2167 - val_mae: 12.7349\n",
      "Epoch 980/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2656 - mse: 0.2656 - mae: 0.3262 - val_loss: 1036.3209 - val_mse: 1036.3209 - val_mae: 12.6789\n",
      "Epoch 981/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3876 - mse: 0.3876 - mae: 0.3888 - val_loss: 1048.3058 - val_mse: 1048.3058 - val_mae: 12.7990\n",
      "Epoch 982/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4606 - mse: 0.4606 - mae: 0.3757 - val_loss: 1064.3163 - val_mse: 1064.3163 - val_mae: 12.8272\n",
      "Epoch 983/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.6781 - mse: 0.6781 - mae: 0.4710 - val_loss: 1046.3403 - val_mse: 1046.3403 - val_mae: 12.7789\n",
      "Epoch 984/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5910 - mse: 0.5910 - mae: 0.5016 - val_loss: 1043.2513 - val_mse: 1043.2513 - val_mae: 12.6246\n",
      "Epoch 985/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5285 - mse: 0.5285 - mae: 0.5360 - val_loss: 1020.4659 - val_mse: 1020.4659 - val_mae: 12.8227\n",
      "Epoch 986/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.5023 - mse: 0.5023 - mae: 0.4803 - val_loss: 1064.6204 - val_mse: 1064.6204 - val_mae: 12.7058\n",
      "Epoch 987/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3906 - mse: 0.3906 - mae: 0.4359 - val_loss: 1036.2219 - val_mse: 1036.2219 - val_mae: 12.7348\n",
      "Epoch 988/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.5305 - mse: 0.5305 - mae: 0.4949 - val_loss: 1053.5065 - val_mse: 1053.5065 - val_mae: 12.7520\n",
      "Epoch 989/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4610 - mse: 0.4610 - mae: 0.5202 - val_loss: 1031.5612 - val_mse: 1031.5612 - val_mae: 12.7605\n",
      "Epoch 990/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4214 - mse: 0.4214 - mae: 0.4679 - val_loss: 1050.3677 - val_mse: 1050.3677 - val_mae: 12.6240\n",
      "Epoch 991/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3947 - mse: 0.3947 - mae: 0.4735 - val_loss: 1034.1783 - val_mse: 1034.1783 - val_mae: 12.7390\n",
      "Epoch 992/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4010 - mse: 0.4010 - mae: 0.4625 - val_loss: 1070.4055 - val_mse: 1070.4055 - val_mae: 12.8212\n",
      "Epoch 993/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.4853 - mse: 0.4853 - mae: 0.4363 - val_loss: 1047.2531 - val_mse: 1047.2531 - val_mae: 12.7416\n",
      "Epoch 994/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.3298 - mse: 0.3298 - mae: 0.4059 - val_loss: 1046.5566 - val_mse: 1046.5566 - val_mae: 12.7567\n",
      "Epoch 995/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2190 - mse: 0.2190 - mae: 0.2859 - val_loss: 1039.7148 - val_mse: 1039.7148 - val_mae: 12.7269\n",
      "Epoch 996/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.2255 - mse: 0.2255 - mae: 0.2821 - val_loss: 1049.6819 - val_mse: 1049.6819 - val_mae: 12.7626\n",
      "Epoch 997/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.1729 - mse: 0.1729 - mae: 0.2312 - val_loss: 1068.9716 - val_mse: 1068.9716 - val_mae: 12.7710\n",
      "Epoch 998/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2471 - mse: 0.2471 - mae: 0.2839 - val_loss: 1057.1210 - val_mse: 1057.1210 - val_mae: 12.7714\n",
      "Epoch 999/1000\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.3813 - mse: 0.3813 - mae: 0.3785 - val_loss: 1044.7169 - val_mse: 1044.7169 - val_mae: 12.7568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3134 - val_loss: 1055.1566 - val_mse: 1055.1566 - val_mae: 12.6928\n"
     ]
    }
   ],
   "source": [
    "hist6 = model6.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "KKe9kmdVLcQB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3cbf270280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "예측값 :  [164.01994]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "preds = model6.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941251435356909\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "Em7eDzqgLLlV",
    "outputId": "504c7109-0f70-4a1b-a33d-732e326f0bae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cbf289d80>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('model loss of Train and Validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(hist6.history['loss'], marker = 'o', ms = 2, label='train_loss')\n",
    "plt.plot(hist6.history['val_loss'], marker = 'o', ms = 2, label='validation_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "wmBgbtD9LUV_",
    "outputId": "789a5dfe-2d06-4c02-b115-c5f780a79f80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cbf1c6c50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('model MAE of Train and Validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mae')\n",
    "plt.plot(hist6.history['mae'], marker = 'o', ms = 2, label='train_mae')\n",
    "plt.plot(hist6.history['val_mae'], marker = 'o', ms = 2, label='val_mae')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "6z4fwK7QLZrs",
    "outputId": "44e02d16-8c78-4e24-e9ba-dc4157dddf55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cf47a5cc0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i for i in range(len(preds))]\n",
    "plt.plot(x_axis,preds,label='preds')\n",
    "plt.plot(x_axis,Y_val,label='labels')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1TJRuUagXDg"
   },
   "source": [
    "## 2. TabNet 활용한 잉크젯 생성 데이터 예측\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1cvioZaBXyu"
   },
   "source": [
    "### 2.1 TabNet 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "fasqv7e8giKv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "w5pCIMkGiWme"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('InkjetDB_preprocessing.csv')\n",
    "X = df.loc[:, 'Viscosity': 'PrintingSpeed']\n",
    "Y = df['PatternSize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMit1X0PBakS"
   },
   "source": [
    "### 2.2 Train/Test set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VY1h-PHnUTJ"
   },
   "source": [
    "train/valid 를 8:2 비율로 나눠줍니다. TabNet은 전처리가 필요 없는 것이 장점이기 때문에 Scaling은 하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZD5G2qDih4a",
    "outputId": "5263485d-ca74-405c-f194-11361136c2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 180\n",
      "45 45\n"
     ]
    }
   ],
   "source": [
    "X_len = len(X)\n",
    "Y_len = len(Y)\n",
    "\n",
    "X_train = X[:int(X_len*0.8)]\n",
    "X_val = X[int(X_len*0.8):]\n",
    "Y_train = Y[:int(Y_len*0.8)]\n",
    "Y_val = Y[int(Y_len*0.8):]\n",
    "\n",
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_val), len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "4t5FnWqGmfLc"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_rIN0ttnW50"
   },
   "source": [
    "TabNetRegressor의 내부 구현에 맞추어 레이블의 형태를 (row 수, 1) 로 수정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zj6e7wf5jvzI",
    "outputId": "07c50ec4-3ff4-49ce-c8fc-6b04731d237a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180, 3)\n",
      "(180, 1)\n",
      "(45, 3)\n",
      "(45, 1)\n"
     ]
    }
   ],
   "source": [
    "Y_train = Y_train.to_numpy()\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_val = Y_val.to_numpy()\n",
    "Y_val = Y_val.reshape(-1, 1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eos3WrmGBe0c"
   },
   "source": [
    "### 2.3 TabNet 모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_kjaEnzozJK"
   },
   "source": [
    "TabNet은 별도의 feature engineering이 필요치 않습니다. raw data로 그대로 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh1P-ta9giII",
    "outputId": "5d953fb6-b3b7-4049-f599-13e5d3ca0485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "epoch 0  | loss: 23085.56641| val_0_mse: 32166.83233|  0:00:00s\n",
      "epoch 1  | loss: 22947.19727| val_0_mse: 34882.6751|  0:00:00s\n",
      "epoch 2  | loss: 22773.07227| val_0_mse: 34554.14413|  0:00:00s\n",
      "epoch 3  | loss: 22606.64648| val_0_mse: 34559.53837|  0:00:00s\n",
      "epoch 4  | loss: 22499.34766| val_0_mse: 34109.62506|  0:00:00s\n",
      "epoch 5  | loss: 22332.6543| val_0_mse: 33471.04336|  0:00:00s\n",
      "epoch 6  | loss: 22160.85352| val_0_mse: 32646.84285|  0:00:00s\n",
      "epoch 7  | loss: 22014.74609| val_0_mse: 31622.26007|  0:00:00s\n",
      "epoch 8  | loss: 21883.43945| val_0_mse: 31235.34114|  0:00:00s\n",
      "epoch 9  | loss: 21732.07227| val_0_mse: 31067.62729|  0:00:00s\n",
      "epoch 10 | loss: 21585.35352| val_0_mse: 31015.11199|  0:00:00s\n",
      "epoch 11 | loss: 21412.14258| val_0_mse: 31355.37538|  0:00:00s\n",
      "epoch 12 | loss: 21287.17578| val_0_mse: 31778.8547|  0:00:00s\n",
      "epoch 13 | loss: 21157.02148| val_0_mse: 31915.52028|  0:00:00s\n",
      "epoch 14 | loss: 21034.66016| val_0_mse: 32105.35593|  0:00:00s\n",
      "epoch 15 | loss: 20892.11719| val_0_mse: 32287.68656|  0:00:00s\n",
      "epoch 16 | loss: 20700.17969| val_0_mse: 32530.93896|  0:00:00s\n",
      "epoch 17 | loss: 20541.84375| val_0_mse: 32780.49414|  0:00:00s\n",
      "epoch 18 | loss: 20346.7793| val_0_mse: 32932.33104|  0:00:00s\n",
      "epoch 19 | loss: 20193.39844| val_0_mse: 33083.79381|  0:00:00s\n",
      "epoch 20 | loss: 20068.30859| val_0_mse: 33091.95936|  0:00:00s\n",
      "epoch 21 | loss: 19902.97852| val_0_mse: 33096.29905|  0:00:00s\n",
      "epoch 22 | loss: 19742.22266| val_0_mse: 33012.06971|  0:00:00s\n",
      "epoch 23 | loss: 19567.33008| val_0_mse: 33063.60742|  0:00:00s\n",
      "epoch 24 | loss: 19343.95508| val_0_mse: 33040.18838|  0:00:00s\n",
      "epoch 25 | loss: 19134.67969| val_0_mse: 32973.31155|  0:00:00s\n",
      "epoch 26 | loss: 18958.36133| val_0_mse: 32902.16054|  0:00:00s\n",
      "epoch 27 | loss: 18794.12305| val_0_mse: 32785.23985|  0:00:00s\n",
      "epoch 28 | loss: 18644.52539| val_0_mse: 32713.38526|  0:00:00s\n",
      "epoch 29 | loss: 18425.79102| val_0_mse: 32736.12316|  0:00:00s\n",
      "epoch 30 | loss: 18203.69922| val_0_mse: 32695.07461|  0:00:00s\n",
      "epoch 31 | loss: 17966.69141| val_0_mse: 32618.20188|  0:00:00s\n",
      "epoch 32 | loss: 17732.11328| val_0_mse: 32592.759|  0:00:00s\n",
      "epoch 33 | loss: 17536.13281| val_0_mse: 32413.41784|  0:00:00s\n",
      "epoch 34 | loss: 17335.73242| val_0_mse: 32315.48796|  0:00:00s\n",
      "epoch 35 | loss: 16994.63672| val_0_mse: 32188.10167|  0:00:00s\n",
      "epoch 36 | loss: 16848.10352| val_0_mse: 32107.83851|  0:00:00s\n",
      "epoch 37 | loss: 16535.99219| val_0_mse: 31985.13312|  0:00:00s\n",
      "epoch 38 | loss: 16112.52734| val_0_mse: 31843.26509|  0:00:00s\n",
      "epoch 39 | loss: 15962.34473| val_0_mse: 31703.1364|  0:00:00s\n",
      "epoch 40 | loss: 15680.75098| val_0_mse: 31561.61915|  0:00:00s\n",
      "epoch 41 | loss: 15357.59961| val_0_mse: 31424.68698|  0:00:00s\n",
      "epoch 42 | loss: 15131.12891| val_0_mse: 31449.55347|  0:00:01s\n",
      "epoch 43 | loss: 14856.6084| val_0_mse: 31372.15122|  0:00:01s\n",
      "epoch 44 | loss: 14547.46777| val_0_mse: 31291.13617|  0:00:01s\n",
      "epoch 45 | loss: 14316.64844| val_0_mse: 31201.68071|  0:00:01s\n",
      "epoch 46 | loss: 13982.54688| val_0_mse: 31106.63236|  0:00:01s\n",
      "epoch 47 | loss: 13767.50098| val_0_mse: 30975.32763|  0:00:01s\n",
      "epoch 48 | loss: 13458.48047| val_0_mse: 30878.81405|  0:00:01s\n",
      "epoch 49 | loss: 13270.2793| val_0_mse: 30786.33622|  0:00:01s\n",
      "epoch 50 | loss: 13116.00586| val_0_mse: 30688.53285|  0:00:01s\n",
      "epoch 51 | loss: 12518.2627| val_0_mse: 30595.66335|  0:00:01s\n",
      "epoch 52 | loss: 12503.08887| val_0_mse: 30491.15016|  0:00:01s\n",
      "epoch 53 | loss: 11972.30566| val_0_mse: 30390.33666|  0:00:01s\n",
      "epoch 54 | loss: 11603.63477| val_0_mse: 30292.90573|  0:00:01s\n",
      "epoch 55 | loss: 11321.16699| val_0_mse: 30172.67329|  0:00:01s\n",
      "epoch 56 | loss: 11192.25391| val_0_mse: 30138.71065|  0:00:01s\n",
      "epoch 57 | loss: 10798.8457| val_0_mse: 30150.22088|  0:00:01s\n",
      "epoch 58 | loss: 10375.31543| val_0_mse: 30095.31306|  0:00:01s\n",
      "epoch 59 | loss: 10020.16406| val_0_mse: 29837.68984|  0:00:01s\n",
      "epoch 60 | loss: 9701.22949| val_0_mse: 29665.60806|  0:00:01s\n",
      "epoch 61 | loss: 9379.25586| val_0_mse: 29456.67702|  0:00:01s\n",
      "epoch 62 | loss: 9111.31152| val_0_mse: 29235.4703|  0:00:01s\n",
      "epoch 63 | loss: 8755.19043| val_0_mse: 29094.24779|  0:00:01s\n",
      "epoch 64 | loss: 8401.00684| val_0_mse: 28942.3995|  0:00:01s\n",
      "epoch 65 | loss: 8090.39648| val_0_mse: 28800.89789|  0:00:01s\n",
      "epoch 66 | loss: 7805.57422| val_0_mse: 28676.69066|  0:00:01s\n",
      "epoch 67 | loss: 7499.24902| val_0_mse: 28563.22489|  0:00:01s\n",
      "epoch 68 | loss: 7113.07275| val_0_mse: 28422.74467|  0:00:01s\n",
      "epoch 69 | loss: 6845.48779| val_0_mse: 28261.64836|  0:00:01s\n",
      "epoch 70 | loss: 6552.57129| val_0_mse: 28067.12711|  0:00:01s\n",
      "epoch 71 | loss: 6291.15674| val_0_mse: 27856.54417|  0:00:01s\n",
      "epoch 72 | loss: 5968.94629| val_0_mse: 27647.14201|  0:00:01s\n",
      "epoch 73 | loss: 5711.45801| val_0_mse: 27395.16889|  0:00:01s\n",
      "epoch 74 | loss: 5490.34717| val_0_mse: 27115.37798|  0:00:01s\n",
      "epoch 75 | loss: 5199.0083| val_0_mse: 26799.59704|  0:00:01s\n",
      "epoch 76 | loss: 4876.83301| val_0_mse: 26438.11154|  0:00:01s\n",
      "epoch 77 | loss: 4552.90234| val_0_mse: 26000.18209|  0:00:01s\n",
      "epoch 78 | loss: 4453.69092| val_0_mse: 25545.05068|  0:00:01s\n",
      "epoch 79 | loss: 4050.65845| val_0_mse: 25098.54723|  0:00:01s\n",
      "epoch 80 | loss: 4302.77539| val_0_mse: 24709.3956|  0:00:02s\n",
      "epoch 81 | loss: 3682.89136| val_0_mse: 24397.15997|  0:00:02s\n",
      "epoch 82 | loss: 3625.4353| val_0_mse: 24130.44139|  0:00:02s\n",
      "epoch 83 | loss: 3028.03198| val_0_mse: 23874.30789|  0:00:02s\n",
      "epoch 84 | loss: 3089.44141| val_0_mse: 23453.62409|  0:00:02s\n",
      "epoch 85 | loss: 2499.62939| val_0_mse: 22881.79803|  0:00:02s\n",
      "epoch 86 | loss: 2436.42578| val_0_mse: 21691.16289|  0:00:02s\n",
      "epoch 87 | loss: 2245.20605| val_0_mse: 20954.06511|  0:00:02s\n",
      "epoch 88 | loss: 1964.60913| val_0_mse: 20589.80196|  0:00:02s\n",
      "epoch 89 | loss: 1773.50659| val_0_mse: 20261.07971|  0:00:02s\n",
      "epoch 90 | loss: 1640.8656| val_0_mse: 19647.83941|  0:00:02s\n",
      "epoch 91 | loss: 1537.00122| val_0_mse: 18514.17037|  0:00:02s\n",
      "epoch 92 | loss: 1298.31238| val_0_mse: 17371.29166|  0:00:02s\n",
      "epoch 93 | loss: 1093.77466| val_0_mse: 16390.78589|  0:00:02s\n",
      "epoch 94 | loss: 1118.38135| val_0_mse: 15725.48022|  0:00:02s\n",
      "epoch 95 | loss: 837.08832| val_0_mse: 15074.4318|  0:00:02s\n",
      "epoch 96 | loss: 867.32971| val_0_mse: 14402.32565|  0:00:02s\n",
      "epoch 97 | loss: 694.76776| val_0_mse: 13660.3751|  0:00:02s\n",
      "epoch 98 | loss: 758.44946| val_0_mse: 13149.77406|  0:00:02s\n",
      "epoch 99 | loss: 1325.48718| val_0_mse: 12953.89926|  0:00:02s\n",
      "epoch 100| loss: 440.21692| val_0_mse: 12285.0975|  0:00:02s\n",
      "epoch 101| loss: 429.77063| val_0_mse: 11592.39328|  0:00:02s\n",
      "epoch 102| loss: 339.17477| val_0_mse: 10903.12959|  0:00:02s\n",
      "epoch 103| loss: 465.29871| val_0_mse: 10097.38093|  0:00:02s\n",
      "epoch 104| loss: 454.1825| val_0_mse: 10575.83768|  0:00:02s\n",
      "epoch 105| loss: 719.58429| val_0_mse: 10099.64325|  0:00:02s\n",
      "epoch 106| loss: 1373.56091| val_0_mse: 9117.99485|  0:00:02s\n",
      "epoch 107| loss: 269.22079| val_0_mse: 8350.72037|  0:00:02s\n",
      "epoch 108| loss: 700.40308| val_0_mse: 9238.36687|  0:00:02s\n",
      "epoch 109| loss: 658.71185| val_0_mse: 7998.99658|  0:00:02s\n",
      "epoch 110| loss: 352.4537| val_0_mse: 7451.96275|  0:00:02s\n",
      "epoch 111| loss: 166.688 | val_0_mse: 7320.468|  0:00:02s\n",
      "epoch 112| loss: 276.23935| val_0_mse: 7686.90991|  0:00:02s\n",
      "epoch 113| loss: 172.57224| val_0_mse: 8008.99051|  0:00:02s\n",
      "epoch 114| loss: 576.57336| val_0_mse: 8132.46153|  0:00:02s\n",
      "epoch 115| loss: 425.51895| val_0_mse: 7184.83719|  0:00:02s\n",
      "epoch 116| loss: 206.04633| val_0_mse: 6541.44401|  0:00:02s\n",
      "epoch 117| loss: 133.43727| val_0_mse: 6424.55949|  0:00:03s\n",
      "epoch 118| loss: 271.99902| val_0_mse: 6842.26349|  0:00:03s\n",
      "epoch 119| loss: 219.64719| val_0_mse: 7591.65125|  0:00:03s\n",
      "epoch 120| loss: 247.07239| val_0_mse: 7709.09472|  0:00:03s\n",
      "epoch 121| loss: 385.27448| val_0_mse: 7288.68024|  0:00:03s\n",
      "epoch 122| loss: 268.76382| val_0_mse: 6390.84649|  0:00:03s\n",
      "epoch 123| loss: 237.49109| val_0_mse: 5190.26211|  0:00:03s\n",
      "epoch 124| loss: 103.09377| val_0_mse: 4546.71828|  0:00:03s\n",
      "epoch 125| loss: 321.96255| val_0_mse: 4253.63647|  0:00:03s\n",
      "epoch 126| loss: 238.77455| val_0_mse: 4315.48999|  0:00:03s\n",
      "epoch 127| loss: 283.44293| val_0_mse: 4087.71867|  0:00:03s\n",
      "epoch 128| loss: 266.7511| val_0_mse: 3808.0501|  0:00:03s\n",
      "epoch 129| loss: 654.3725| val_0_mse: 3854.96696|  0:00:03s\n",
      "epoch 130| loss: 422.76489| val_0_mse: 3661.83387|  0:00:03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 131| loss: 584.36206| val_0_mse: 3616.81157|  0:00:03s\n",
      "epoch 132| loss: 112.43831| val_0_mse: 3907.09429|  0:00:03s\n",
      "epoch 133| loss: 568.59283| val_0_mse: 4304.02147|  0:00:03s\n",
      "epoch 134| loss: 193.06  | val_0_mse: 4632.68112|  0:00:03s\n",
      "epoch 135| loss: 132.47795| val_0_mse: 5234.52734|  0:00:03s\n",
      "epoch 136| loss: 1128.78577| val_0_mse: 5636.63255|  0:00:03s\n",
      "epoch 137| loss: 278.23981| val_0_mse: 6525.22887|  0:00:03s\n",
      "epoch 138| loss: 399.62781| val_0_mse: 7591.25991|  0:00:03s\n",
      "epoch 139| loss: 167.77669| val_0_mse: 7950.47857|  0:00:03s\n",
      "epoch 140| loss: 162.03316| val_0_mse: 8136.54273|  0:00:03s\n",
      "epoch 141| loss: 216.96645| val_0_mse: 8352.73223|  0:00:03s\n",
      "epoch 142| loss: 276.52332| val_0_mse: 8669.69593|  0:00:03s\n",
      "epoch 143| loss: 105.37615| val_0_mse: 8573.46553|  0:00:03s\n",
      "epoch 144| loss: 73.87508| val_0_mse: 8185.81104|  0:00:03s\n",
      "epoch 145| loss: 171.09393| val_0_mse: 7871.61872|  0:00:03s\n",
      "epoch 146| loss: 245.46404| val_0_mse: 8307.90138|  0:00:03s\n",
      "epoch 147| loss: 539.59753| val_0_mse: 9021.54607|  0:00:03s\n",
      "epoch 148| loss: 766.33453| val_0_mse: 9617.45337|  0:00:03s\n",
      "epoch 149| loss: 151.26208| val_0_mse: 9686.11855|  0:00:03s\n",
      "epoch 150| loss: 519.1181| val_0_mse: 9320.81385|  0:00:03s\n",
      "epoch 151| loss: 232.59654| val_0_mse: 8621.79475|  0:00:03s\n",
      "epoch 152| loss: 84.33479| val_0_mse: 7691.75491|  0:00:03s\n",
      "epoch 153| loss: 94.42992| val_0_mse: 7114.53496|  0:00:03s\n",
      "epoch 154| loss: 688.26874| val_0_mse: 7157.45872|  0:00:03s\n",
      "epoch 155| loss: 197.45883| val_0_mse: 7155.6565|  0:00:03s\n",
      "epoch 156| loss: 146.80264| val_0_mse: 7069.25308|  0:00:03s\n",
      "epoch 157| loss: 109.15566| val_0_mse: 6741.77784|  0:00:03s\n",
      "epoch 158| loss: 150.73631| val_0_mse: 6691.12894|  0:00:03s\n",
      "epoch 159| loss: 77.59862| val_0_mse: 6941.55273|  0:00:03s\n",
      "epoch 160| loss: 145.59282| val_0_mse: 7199.82472|  0:00:04s\n",
      "epoch 161| loss: 148.0909| val_0_mse: 7384.69312|  0:00:04s\n",
      "epoch 162| loss: 354.51147| val_0_mse: 7482.9826|  0:00:04s\n",
      "epoch 163| loss: 479.46933| val_0_mse: 7580.98045|  0:00:04s\n",
      "epoch 164| loss: 112.62711| val_0_mse: 7758.31964|  0:00:04s\n",
      "epoch 165| loss: 89.5272 | val_0_mse: 7839.91483|  0:00:04s\n",
      "epoch 166| loss: 210.44164| val_0_mse: 7546.55766|  0:00:04s\n",
      "epoch 167| loss: 79.03324| val_0_mse: 7211.52798|  0:00:04s\n",
      "epoch 168| loss: 209.21996| val_0_mse: 6852.98861|  0:00:04s\n",
      "epoch 169| loss: 474.62952| val_0_mse: 6579.64896|  0:00:04s\n",
      "epoch 170| loss: 426.9393| val_0_mse: 6946.44002|  0:00:04s\n",
      "epoch 171| loss: 216.38037| val_0_mse: 7676.9599|  0:00:04s\n",
      "epoch 172| loss: 161.45895| val_0_mse: 8129.32799|  0:00:04s\n",
      "epoch 173| loss: 152.99846| val_0_mse: 8432.19798|  0:00:04s\n",
      "epoch 174| loss: 99.98656| val_0_mse: 8197.98474|  0:00:04s\n",
      "epoch 175| loss: 668.37366| val_0_mse: 8409.59893|  0:00:04s\n",
      "epoch 176| loss: 225.59767| val_0_mse: 8410.51699|  0:00:04s\n",
      "epoch 177| loss: 545.56311| val_0_mse: 8490.2386|  0:00:04s\n",
      "epoch 178| loss: 300.78244| val_0_mse: 8250.08866|  0:00:04s\n",
      "epoch 179| loss: 884.24628| val_0_mse: 8440.57995|  0:00:04s\n",
      "epoch 180| loss: 133.78069| val_0_mse: 8454.24022|  0:00:04s\n",
      "epoch 181| loss: 408.44128| val_0_mse: 8051.49573|  0:00:04s\n",
      "epoch 182| loss: 595.5116| val_0_mse: 8192.71288|  0:00:04s\n",
      "epoch 183| loss: 77.49422| val_0_mse: 8487.34695|  0:00:04s\n",
      "epoch 184| loss: 141.42589| val_0_mse: 8468.76295|  0:00:04s\n",
      "epoch 185| loss: 559.76666| val_0_mse: 8313.58696|  0:00:04s\n",
      "epoch 186| loss: 117.13323| val_0_mse: 7903.67455|  0:00:04s\n",
      "epoch 187| loss: 73.97339| val_0_mse: 7487.19169|  0:00:04s\n",
      "epoch 188| loss: 178.84088| val_0_mse: 7561.64527|  0:00:04s\n",
      "epoch 189| loss: 93.93917| val_0_mse: 7488.65433|  0:00:04s\n",
      "epoch 190| loss: 408.01285| val_0_mse: 7691.65891|  0:00:04s\n",
      "epoch 191| loss: 165.79176| val_0_mse: 7436.99802|  0:00:04s\n",
      "epoch 192| loss: 761.50903| val_0_mse: 7486.98697|  0:00:04s\n",
      "epoch 193| loss: 145.21893| val_0_mse: 7553.50315|  0:00:04s\n",
      "epoch 194| loss: 72.31781| val_0_mse: 7220.97729|  0:00:04s\n",
      "epoch 195| loss: 121.34163| val_0_mse: 6408.89092|  0:00:04s\n",
      "epoch 196| loss: 878.89185| val_0_mse: 5842.55312|  0:00:04s\n",
      "epoch 197| loss: 891.92834| val_0_mse: 5669.99577|  0:00:04s\n",
      "epoch 198| loss: 126.80087| val_0_mse: 5480.61714|  0:00:04s\n",
      "epoch 199| loss: 102.52379| val_0_mse: 5232.37821|  0:00:04s\n",
      "epoch 200| loss: 119.78161| val_0_mse: 4753.01591|  0:00:04s\n",
      "epoch 201| loss: 322.38077| val_0_mse: 4868.82218|  0:00:04s\n",
      "epoch 202| loss: 185.18146| val_0_mse: 4819.20736|  0:00:04s\n",
      "epoch 203| loss: 364.09134| val_0_mse: 4212.93975|  0:00:04s\n",
      "epoch 204| loss: 226.15895| val_0_mse: 3834.21332|  0:00:04s\n",
      "epoch 205| loss: 108.70362| val_0_mse: 3581.08773|  0:00:04s\n",
      "epoch 206| loss: 441.07437| val_0_mse: 3959.99654|  0:00:05s\n",
      "epoch 207| loss: 245.27174| val_0_mse: 4421.16475|  0:00:05s\n",
      "epoch 208| loss: 110.21205| val_0_mse: 4501.64013|  0:00:05s\n",
      "epoch 209| loss: 105.46899| val_0_mse: 4173.31256|  0:00:05s\n",
      "epoch 210| loss: 826.27124| val_0_mse: 4219.02696|  0:00:05s\n",
      "epoch 211| loss: 1169.73877| val_0_mse: 4599.64511|  0:00:05s\n",
      "epoch 212| loss: 249.97597| val_0_mse: 4511.23854|  0:00:05s\n",
      "epoch 213| loss: 154.46574| val_0_mse: 4891.47338|  0:00:05s\n",
      "epoch 214| loss: 110.31686| val_0_mse: 5329.86863|  0:00:05s\n",
      "epoch 215| loss: 217.02487| val_0_mse: 5441.81175|  0:00:05s\n",
      "epoch 216| loss: 145.28299| val_0_mse: 5517.62198|  0:00:05s\n",
      "epoch 217| loss: 115.74068| val_0_mse: 4916.04131|  0:00:05s\n",
      "epoch 218| loss: 655.01093| val_0_mse: 4070.0506|  0:00:05s\n",
      "epoch 219| loss: 86.88072| val_0_mse: 3674.9365|  0:00:05s\n",
      "epoch 220| loss: 208.65489| val_0_mse: 3415.5657|  0:00:05s\n",
      "epoch 221| loss: 389.38193| val_0_mse: 3867.82414|  0:00:05s\n",
      "epoch 222| loss: 105.32066| val_0_mse: 4154.84378|  0:00:05s\n",
      "epoch 223| loss: 128.54124| val_0_mse: 4157.86163|  0:00:05s\n",
      "epoch 224| loss: 96.80546| val_0_mse: 3794.01114|  0:00:05s\n",
      "epoch 225| loss: 77.48945| val_0_mse: 3466.14114|  0:00:05s\n",
      "epoch 226| loss: 207.76642| val_0_mse: 3694.399|  0:00:05s\n",
      "epoch 227| loss: 71.17779| val_0_mse: 4401.48197|  0:00:05s\n",
      "epoch 228| loss: 62.45935| val_0_mse: 4516.39144|  0:00:05s\n",
      "epoch 229| loss: 80.65102| val_0_mse: 4175.69304|  0:00:05s\n",
      "epoch 230| loss: 90.10818| val_0_mse: 4406.37813|  0:00:05s\n",
      "epoch 231| loss: 233.88411| val_0_mse: 4407.04461|  0:00:05s\n",
      "epoch 232| loss: 125.98354| val_0_mse: 4325.48972|  0:00:05s\n",
      "epoch 233| loss: 186.87877| val_0_mse: 3908.0514|  0:00:05s\n",
      "epoch 234| loss: 88.35571| val_0_mse: 3011.80638|  0:00:05s\n",
      "epoch 235| loss: 259.91742| val_0_mse: 2120.89424|  0:00:05s\n",
      "epoch 236| loss: 73.01968| val_0_mse: 1751.29257|  0:00:05s\n",
      "epoch 237| loss: 156.09793| val_0_mse: 1783.06611|  0:00:05s\n",
      "epoch 238| loss: 834.31934| val_0_mse: 2023.66343|  0:00:05s\n",
      "epoch 239| loss: 166.48763| val_0_mse: 2291.55143|  0:00:05s\n",
      "epoch 240| loss: 97.51504| val_0_mse: 2535.59442|  0:00:05s\n",
      "epoch 241| loss: 120.79886| val_0_mse: 2607.6649|  0:00:05s\n",
      "epoch 242| loss: 262.82483| val_0_mse: 3315.01062|  0:00:05s\n",
      "epoch 243| loss: 106.84177| val_0_mse: 4210.48395|  0:00:05s\n",
      "epoch 244| loss: 61.61926| val_0_mse: 5178.63917|  0:00:05s\n",
      "epoch 245| loss: 325.31198| val_0_mse: 6165.57578|  0:00:05s\n",
      "epoch 246| loss: 118.27774| val_0_mse: 6257.25475|  0:00:05s\n",
      "epoch 247| loss: 176.57091| val_0_mse: 5489.9678|  0:00:05s\n",
      "epoch 248| loss: 208.40808| val_0_mse: 4198.42871|  0:00:05s\n",
      "epoch 249| loss: 384.4718| val_0_mse: 3565.83444|  0:00:05s\n",
      "epoch 250| loss: 84.08397| val_0_mse: 3589.46495|  0:00:05s\n",
      "epoch 251| loss: 198.4539| val_0_mse: 3766.40362|  0:00:05s\n",
      "epoch 252| loss: 61.90008| val_0_mse: 3626.89072|  0:00:05s\n",
      "epoch 253| loss: 125.67188| val_0_mse: 3010.54334|  0:00:05s\n",
      "epoch 254| loss: 145.04037| val_0_mse: 2433.83676|  0:00:05s\n",
      "epoch 255| loss: 323.02414| val_0_mse: 2485.35752|  0:00:05s\n",
      "epoch 256| loss: 66.76771| val_0_mse: 2048.27223|  0:00:05s\n",
      "epoch 257| loss: 238.76051| val_0_mse: 1718.51891|  0:00:05s\n",
      "epoch 258| loss: 183.13062| val_0_mse: 1488.93675|  0:00:05s\n",
      "epoch 259| loss: 106.63596| val_0_mse: 1114.91085|  0:00:05s\n",
      "epoch 260| loss: 631.38696| val_0_mse: 1114.8336|  0:00:05s\n",
      "epoch 261| loss: 48.27946| val_0_mse: 1299.92506|  0:00:05s\n",
      "epoch 262| loss: 164.53545| val_0_mse: 1599.52857|  0:00:05s\n",
      "epoch 263| loss: 721.79498| val_0_mse: 2186.92677|  0:00:05s\n",
      "epoch 264| loss: 72.54348| val_0_mse: 2778.59925|  0:00:05s\n",
      "epoch 265| loss: 368.86209| val_0_mse: 2933.5688|  0:00:06s\n",
      "epoch 266| loss: 78.91824| val_0_mse: 2944.23875|  0:00:06s\n",
      "epoch 267| loss: 305.62894| val_0_mse: 3328.94479|  0:00:06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 268| loss: 314.07123| val_0_mse: 4121.80704|  0:00:06s\n",
      "epoch 269| loss: 116.29223| val_0_mse: 4389.23646|  0:00:06s\n",
      "epoch 270| loss: 107.10375| val_0_mse: 3840.80334|  0:00:06s\n",
      "epoch 271| loss: 130.44783| val_0_mse: 2647.37579|  0:00:06s\n",
      "epoch 272| loss: 238.59448| val_0_mse: 1906.15854|  0:00:06s\n",
      "epoch 273| loss: 232.26547| val_0_mse: 1649.54387|  0:00:06s\n",
      "epoch 274| loss: 779.28754| val_0_mse: 1798.75166|  0:00:06s\n",
      "epoch 275| loss: 158.89339| val_0_mse: 1879.76155|  0:00:06s\n",
      "epoch 276| loss: 95.72199| val_0_mse: 1656.45862|  0:00:06s\n",
      "epoch 277| loss: 352.66086| val_0_mse: 1416.47364|  0:00:06s\n",
      "epoch 278| loss: 164.67714| val_0_mse: 1123.5864|  0:00:06s\n",
      "epoch 279| loss: 333.60889| val_0_mse: 830.70341|  0:00:06s\n",
      "epoch 280| loss: 580.48285| val_0_mse: 796.3978|  0:00:06s\n",
      "epoch 281| loss: 300.79602| val_0_mse: 933.08077|  0:00:06s\n",
      "epoch 282| loss: 138.8166| val_0_mse: 831.56159|  0:00:06s\n",
      "epoch 283| loss: 101.96471| val_0_mse: 640.85571|  0:00:06s\n",
      "epoch 284| loss: 148.50343| val_0_mse: 508.72446|  0:00:06s\n",
      "epoch 285| loss: 784.78094| val_0_mse: 456.73866|  0:00:06s\n",
      "epoch 286| loss: 147.88307| val_0_mse: 482.54784|  0:00:06s\n",
      "epoch 287| loss: 200.502 | val_0_mse: 522.2212|  0:00:06s\n",
      "epoch 288| loss: 86.92732| val_0_mse: 555.87963|  0:00:06s\n",
      "epoch 289| loss: 216.78221| val_0_mse: 762.98745|  0:00:06s\n",
      "epoch 290| loss: 251.45631| val_0_mse: 995.72606|  0:00:06s\n",
      "epoch 291| loss: 104.2759| val_0_mse: 1334.2645|  0:00:06s\n",
      "epoch 292| loss: 209.14445| val_0_mse: 1426.76714|  0:00:06s\n",
      "epoch 293| loss: 72.28454| val_0_mse: 1636.07343|  0:00:06s\n",
      "epoch 294| loss: 99.30658| val_0_mse: 1518.51242|  0:00:06s\n",
      "epoch 295| loss: 372.41354| val_0_mse: 1741.81754|  0:00:06s\n",
      "epoch 296| loss: 220.22826| val_0_mse: 1661.23992|  0:00:06s\n",
      "epoch 297| loss: 129.23956| val_0_mse: 1275.2706|  0:00:06s\n",
      "epoch 298| loss: 96.41898| val_0_mse: 1264.27695|  0:00:06s\n",
      "epoch 299| loss: 196.26469| val_0_mse: 1130.69711|  0:00:06s\n",
      "epoch 300| loss: 134.47801| val_0_mse: 854.02494|  0:00:06s\n",
      "epoch 301| loss: 80.83688| val_0_mse: 945.89201|  0:00:06s\n",
      "epoch 302| loss: 147.07768| val_0_mse: 853.69168|  0:00:06s\n",
      "epoch 303| loss: 129.25385| val_0_mse: 835.09235|  0:00:06s\n",
      "epoch 304| loss: 120.1935| val_0_mse: 659.43278|  0:00:06s\n",
      "epoch 305| loss: 170.71349| val_0_mse: 733.61328|  0:00:06s\n",
      "epoch 306| loss: 118.85265| val_0_mse: 978.25019|  0:00:06s\n",
      "epoch 307| loss: 319.09918| val_0_mse: 1190.7622|  0:00:06s\n",
      "epoch 308| loss: 307.63275| val_0_mse: 1116.11454|  0:00:06s\n",
      "epoch 309| loss: 265.71982| val_0_mse: 831.37486|  0:00:06s\n",
      "epoch 310| loss: 166.49153| val_0_mse: 533.84368|  0:00:06s\n",
      "epoch 311| loss: 172.9841| val_0_mse: 493.39775|  0:00:06s\n",
      "epoch 312| loss: 514.27783| val_0_mse: 648.62501|  0:00:06s\n",
      "epoch 313| loss: 189.51039| val_0_mse: 823.61926|  0:00:06s\n",
      "epoch 314| loss: 295.04785| val_0_mse: 1037.52514|  0:00:06s\n",
      "epoch 315| loss: 173.55765| val_0_mse: 1150.72614|  0:00:06s\n",
      "epoch 316| loss: 165.88437| val_0_mse: 979.27794|  0:00:06s\n",
      "epoch 317| loss: 306.1467| val_0_mse: 786.29667|  0:00:06s\n",
      "epoch 318| loss: 137.05989| val_0_mse: 594.93341|  0:00:06s\n",
      "epoch 319| loss: 67.67539| val_0_mse: 492.51652|  0:00:06s\n",
      "epoch 320| loss: 83.5126 | val_0_mse: 664.63448|  0:00:06s\n",
      "epoch 321| loss: 81.43886| val_0_mse: 765.06285|  0:00:06s\n",
      "epoch 322| loss: 99.02801| val_0_mse: 874.48787|  0:00:06s\n",
      "epoch 323| loss: 277.18973| val_0_mse: 843.26077|  0:00:06s\n",
      "epoch 324| loss: 126.40839| val_0_mse: 718.42628|  0:00:06s\n",
      "epoch 325| loss: 178.48085| val_0_mse: 623.54258|  0:00:06s\n",
      "epoch 326| loss: 261.08365| val_0_mse: 540.57309|  0:00:06s\n",
      "epoch 327| loss: 577.94574| val_0_mse: 536.39803|  0:00:06s\n",
      "epoch 328| loss: 135.40958| val_0_mse: 341.96541|  0:00:06s\n",
      "epoch 329| loss: 260.52591| val_0_mse: 434.87931|  0:00:06s\n",
      "epoch 330| loss: 111.09556| val_0_mse: 576.04428|  0:00:06s\n",
      "epoch 331| loss: 134.90988| val_0_mse: 709.18603|  0:00:06s\n",
      "epoch 332| loss: 68.23355| val_0_mse: 756.45157|  0:00:06s\n",
      "epoch 333| loss: 77.81147| val_0_mse: 694.22147|  0:00:06s\n",
      "epoch 334| loss: 481.17654| val_0_mse: 810.31159|  0:00:06s\n",
      "epoch 335| loss: 208.32341| val_0_mse: 966.62579|  0:00:06s\n",
      "epoch 336| loss: 247.12732| val_0_mse: 1294.95026|  0:00:06s\n",
      "epoch 337| loss: 778.25726| val_0_mse: 1755.96199|  0:00:06s\n",
      "epoch 338| loss: 300.79358| val_0_mse: 1978.21134|  0:00:06s\n",
      "epoch 339| loss: 239.90083| val_0_mse: 1770.19493|  0:00:07s\n",
      "epoch 340| loss: 61.36303| val_0_mse: 1793.94538|  0:00:07s\n",
      "epoch 341| loss: 303.68903| val_0_mse: 1843.37595|  0:00:07s\n",
      "epoch 342| loss: 163.65393| val_0_mse: 1787.83187|  0:00:07s\n",
      "epoch 343| loss: 369.20346| val_0_mse: 1424.43357|  0:00:07s\n",
      "epoch 344| loss: 130.0244| val_0_mse: 1353.4887|  0:00:07s\n",
      "epoch 345| loss: 71.14957| val_0_mse: 1153.06578|  0:00:07s\n",
      "epoch 346| loss: 161.81831| val_0_mse: 912.73481|  0:00:07s\n",
      "epoch 347| loss: 155.69803| val_0_mse: 922.63074|  0:00:07s\n",
      "epoch 348| loss: 89.79305| val_0_mse: 1082.72548|  0:00:07s\n",
      "epoch 349| loss: 145.45041| val_0_mse: 1107.8936|  0:00:07s\n",
      "epoch 350| loss: 534.81274| val_0_mse: 918.47802|  0:00:07s\n",
      "epoch 351| loss: 352.3233| val_0_mse: 786.77372|  0:00:07s\n",
      "epoch 352| loss: 149.55608| val_0_mse: 710.17679|  0:00:07s\n",
      "epoch 353| loss: 110.52316| val_0_mse: 701.39198|  0:00:07s\n",
      "epoch 354| loss: 110.2108| val_0_mse: 765.34504|  0:00:07s\n",
      "epoch 355| loss: 152.35699| val_0_mse: 685.9404|  0:00:07s\n",
      "epoch 356| loss: 105.37991| val_0_mse: 501.95368|  0:00:07s\n",
      "epoch 357| loss: 63.50652| val_0_mse: 343.13451|  0:00:07s\n",
      "epoch 358| loss: 244.6387| val_0_mse: 476.55302|  0:00:07s\n",
      "epoch 359| loss: 107.23232| val_0_mse: 455.71574|  0:00:07s\n",
      "epoch 360| loss: 115.65762| val_0_mse: 446.52712|  0:00:07s\n",
      "epoch 361| loss: 119.73138| val_0_mse: 387.61193|  0:00:07s\n",
      "epoch 362| loss: 74.23733| val_0_mse: 397.1372|  0:00:07s\n",
      "epoch 363| loss: 161.45163| val_0_mse: 410.73876|  0:00:07s\n",
      "epoch 364| loss: 323.92407| val_0_mse: 447.73033|  0:00:07s\n",
      "epoch 365| loss: 56.14187| val_0_mse: 452.17956|  0:00:07s\n",
      "epoch 366| loss: 99.20675| val_0_mse: 363.43118|  0:00:07s\n",
      "epoch 367| loss: 321.32532| val_0_mse: 297.82994|  0:00:07s\n",
      "epoch 368| loss: 109.08098| val_0_mse: 307.70618|  0:00:07s\n",
      "epoch 369| loss: 108.20939| val_0_mse: 485.77441|  0:00:07s\n",
      "epoch 370| loss: 110.75846| val_0_mse: 548.33194|  0:00:07s\n",
      "epoch 371| loss: 260.85245| val_0_mse: 592.73742|  0:00:07s\n",
      "epoch 372| loss: 446.94125| val_0_mse: 590.75568|  0:00:07s\n",
      "epoch 373| loss: 112.69043| val_0_mse: 514.08555|  0:00:07s\n",
      "epoch 374| loss: 96.09107| val_0_mse: 629.74521|  0:00:07s\n",
      "epoch 375| loss: 224.77304| val_0_mse: 943.663 |  0:00:07s\n",
      "epoch 376| loss: 80.56005| val_0_mse: 1112.0648|  0:00:07s\n",
      "epoch 377| loss: 276.47568| val_0_mse: 1020.21944|  0:00:07s\n",
      "epoch 378| loss: 338.00699| val_0_mse: 938.78663|  0:00:07s\n",
      "epoch 379| loss: 127.27219| val_0_mse: 1091.71784|  0:00:07s\n",
      "epoch 380| loss: 87.73911| val_0_mse: 921.84314|  0:00:07s\n",
      "epoch 381| loss: 57.5859 | val_0_mse: 726.4243|  0:00:07s\n",
      "epoch 382| loss: 115.11272| val_0_mse: 375.28092|  0:00:07s\n",
      "epoch 383| loss: 108.05127| val_0_mse: 199.17771|  0:00:07s\n",
      "epoch 384| loss: 225.87512| val_0_mse: 268.4315|  0:00:07s\n",
      "epoch 385| loss: 184.00551| val_0_mse: 396.56331|  0:00:07s\n",
      "epoch 386| loss: 105.3687| val_0_mse: 548.99418|  0:00:07s\n",
      "epoch 387| loss: 235.30864| val_0_mse: 917.3255|  0:00:07s\n",
      "epoch 388| loss: 144.24217| val_0_mse: 1750.13409|  0:00:07s\n",
      "epoch 389| loss: 173.83667| val_0_mse: 2436.84228|  0:00:07s\n",
      "epoch 390| loss: 229.28175| val_0_mse: 2715.41372|  0:00:07s\n",
      "epoch 391| loss: 84.05994| val_0_mse: 2940.25829|  0:00:07s\n",
      "epoch 392| loss: 331.75201| val_0_mse: 2755.34756|  0:00:07s\n",
      "epoch 393| loss: 101.03235| val_0_mse: 3019.72003|  0:00:07s\n",
      "epoch 394| loss: 65.86829| val_0_mse: 3550.64914|  0:00:07s\n",
      "epoch 395| loss: 73.29057| val_0_mse: 3687.54549|  0:00:07s\n",
      "epoch 396| loss: 105.1231| val_0_mse: 3844.74142|  0:00:07s\n",
      "epoch 397| loss: 101.55843| val_0_mse: 3836.71713|  0:00:07s\n",
      "epoch 398| loss: 42.70951| val_0_mse: 3340.76172|  0:00:07s\n",
      "epoch 399| loss: 86.82007| val_0_mse: 2923.00279|  0:00:07s\n",
      "epoch 400| loss: 81.15971| val_0_mse: 2570.21472|  0:00:07s\n",
      "epoch 401| loss: 55.60756| val_0_mse: 2092.01355|  0:00:07s\n",
      "epoch 402| loss: 87.69299| val_0_mse: 1500.41254|  0:00:07s\n",
      "epoch 403| loss: 110.3933| val_0_mse: 998.84832|  0:00:07s\n",
      "epoch 404| loss: 257.76138| val_0_mse: 755.74875|  0:00:07s\n",
      "epoch 405| loss: 131.05374| val_0_mse: 435.27664|  0:00:07s\n",
      "epoch 406| loss: 150.29471| val_0_mse: 254.17212|  0:00:07s\n",
      "epoch 407| loss: 79.15211| val_0_mse: 214.62147|  0:00:07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 408| loss: 57.33604| val_0_mse: 213.32816|  0:00:07s\n",
      "epoch 409| loss: 264.85031| val_0_mse: 196.03567|  0:00:07s\n",
      "epoch 410| loss: 100.52213| val_0_mse: 267.08165|  0:00:07s\n",
      "epoch 411| loss: 224.39583| val_0_mse: 396.00492|  0:00:07s\n",
      "epoch 412| loss: 565.9931| val_0_mse: 487.70328|  0:00:07s\n",
      "epoch 413| loss: 144.77979| val_0_mse: 657.53766|  0:00:08s\n",
      "epoch 414| loss: 197.17375| val_0_mse: 818.21863|  0:00:08s\n",
      "epoch 415| loss: 606.48236| val_0_mse: 1297.22094|  0:00:08s\n",
      "epoch 416| loss: 473.19571| val_0_mse: 1775.45356|  0:00:08s\n",
      "epoch 417| loss: 148.03308| val_0_mse: 2283.14158|  0:00:08s\n",
      "epoch 418| loss: 226.9223| val_0_mse: 2680.97715|  0:00:08s\n",
      "epoch 419| loss: 240.90154| val_0_mse: 2750.44836|  0:00:08s\n",
      "epoch 420| loss: 89.59358| val_0_mse: 2605.51995|  0:00:08s\n",
      "epoch 421| loss: 262.88126| val_0_mse: 2563.44151|  0:00:08s\n",
      "epoch 422| loss: 90.07674| val_0_mse: 2949.81909|  0:00:08s\n",
      "epoch 423| loss: 121.52829| val_0_mse: 3148.21481|  0:00:08s\n",
      "epoch 424| loss: 114.71535| val_0_mse: 2848.81974|  0:00:08s\n",
      "epoch 425| loss: 70.70148| val_0_mse: 2037.52187|  0:00:08s\n",
      "epoch 426| loss: 241.8475| val_0_mse: 1360.39217|  0:00:08s\n",
      "epoch 427| loss: 137.71381| val_0_mse: 948.10443|  0:00:08s\n",
      "epoch 428| loss: 164.12866| val_0_mse: 777.81763|  0:00:08s\n",
      "epoch 429| loss: 172.23305| val_0_mse: 779.52101|  0:00:08s\n",
      "epoch 430| loss: 112.12552| val_0_mse: 924.16222|  0:00:08s\n",
      "epoch 431| loss: 182.16032| val_0_mse: 884.83204|  0:00:08s\n",
      "epoch 432| loss: 167.57486| val_0_mse: 702.69642|  0:00:08s\n",
      "epoch 433| loss: 116.57269| val_0_mse: 448.41524|  0:00:08s\n",
      "epoch 434| loss: 191.80177| val_0_mse: 261.54362|  0:00:08s\n",
      "epoch 435| loss: 93.01567| val_0_mse: 261.20184|  0:00:08s\n",
      "epoch 436| loss: 82.15775| val_0_mse: 323.12307|  0:00:08s\n",
      "epoch 437| loss: 102.99689| val_0_mse: 362.0758|  0:00:08s\n",
      "epoch 438| loss: 61.55037| val_0_mse: 357.48492|  0:00:08s\n",
      "epoch 439| loss: 104.25027| val_0_mse: 305.64289|  0:00:08s\n",
      "epoch 440| loss: 43.8314 | val_0_mse: 327.46592|  0:00:08s\n",
      "epoch 441| loss: 77.03487| val_0_mse: 350.9407|  0:00:08s\n",
      "epoch 442| loss: 70.5597 | val_0_mse: 420.90229|  0:00:08s\n",
      "epoch 443| loss: 80.31924| val_0_mse: 565.33584|  0:00:08s\n",
      "epoch 444| loss: 102.85966| val_0_mse: 701.56637|  0:00:08s\n",
      "epoch 445| loss: 71.17417| val_0_mse: 740.25636|  0:00:08s\n",
      "epoch 446| loss: 83.98088| val_0_mse: 703.96962|  0:00:08s\n",
      "epoch 447| loss: 248.21318| val_0_mse: 646.08063|  0:00:08s\n",
      "epoch 448| loss: 66.19996| val_0_mse: 570.59635|  0:00:08s\n",
      "epoch 449| loss: 211.48015| val_0_mse: 505.94455|  0:00:08s\n",
      "epoch 450| loss: 59.63293| val_0_mse: 455.764 |  0:00:08s\n",
      "epoch 451| loss: 188.96889| val_0_mse: 389.64382|  0:00:08s\n",
      "epoch 452| loss: 251.35185| val_0_mse: 370.11301|  0:00:08s\n",
      "epoch 453| loss: 70.08779| val_0_mse: 344.4416|  0:00:08s\n",
      "epoch 454| loss: 125.59069| val_0_mse: 301.063 |  0:00:08s\n",
      "epoch 455| loss: 79.21413| val_0_mse: 328.68613|  0:00:08s\n",
      "epoch 456| loss: 234.84782| val_0_mse: 424.32933|  0:00:08s\n",
      "epoch 457| loss: 73.76687| val_0_mse: 515.99727|  0:00:08s\n",
      "epoch 458| loss: 520.98462| val_0_mse: 592.00329|  0:00:08s\n",
      "epoch 459| loss: 125.98412| val_0_mse: 705.62254|  0:00:08s\n",
      "epoch 460| loss: 143.06091| val_0_mse: 812.63176|  0:00:08s\n",
      "epoch 461| loss: 92.13494| val_0_mse: 913.16277|  0:00:08s\n",
      "epoch 462| loss: 132.9424| val_0_mse: 982.11163|  0:00:08s\n",
      "epoch 463| loss: 101.08154| val_0_mse: 1053.90539|  0:00:08s\n",
      "epoch 464| loss: 75.58108| val_0_mse: 1181.5404|  0:00:08s\n",
      "epoch 465| loss: 269.16071| val_0_mse: 1211.03944|  0:00:08s\n",
      "epoch 466| loss: 191.0909| val_0_mse: 1097.58075|  0:00:08s\n",
      "epoch 467| loss: 131.45566| val_0_mse: 1001.26167|  0:00:08s\n",
      "epoch 468| loss: 152.4375| val_0_mse: 970.47595|  0:00:08s\n",
      "epoch 469| loss: 100.67113| val_0_mse: 912.46711|  0:00:08s\n",
      "epoch 470| loss: 65.00405| val_0_mse: 880.49357|  0:00:08s\n",
      "epoch 471| loss: 175.24126| val_0_mse: 808.50533|  0:00:08s\n",
      "epoch 472| loss: 466.61456| val_0_mse: 772.73437|  0:00:08s\n",
      "epoch 473| loss: 303.53442| val_0_mse: 756.63783|  0:00:08s\n",
      "epoch 474| loss: 274.51859| val_0_mse: 741.65574|  0:00:08s\n",
      "epoch 475| loss: 52.30906| val_0_mse: 715.71105|  0:00:08s\n",
      "epoch 476| loss: 117.47754| val_0_mse: 575.65382|  0:00:08s\n",
      "epoch 477| loss: 237.70358| val_0_mse: 442.19479|  0:00:08s\n",
      "epoch 478| loss: 126.45093| val_0_mse: 315.82695|  0:00:08s\n",
      "epoch 479| loss: 140.34058| val_0_mse: 286.64468|  0:00:08s\n",
      "epoch 480| loss: 86.39484| val_0_mse: 354.11038|  0:00:08s\n",
      "epoch 481| loss: 77.97344| val_0_mse: 450.78057|  0:00:08s\n",
      "epoch 482| loss: 105.22423| val_0_mse: 449.15559|  0:00:08s\n",
      "epoch 483| loss: 323.46487| val_0_mse: 422.23339|  0:00:08s\n",
      "epoch 484| loss: 67.2709 | val_0_mse: 374.18891|  0:00:08s\n",
      "epoch 485| loss: 68.36982| val_0_mse: 402.65951|  0:00:08s\n",
      "epoch 486| loss: 111.80157| val_0_mse: 445.70701|  0:00:08s\n",
      "epoch 487| loss: 83.6562 | val_0_mse: 474.69162|  0:00:08s\n",
      "epoch 488| loss: 55.02716| val_0_mse: 563.85702|  0:00:09s\n",
      "epoch 489| loss: 121.33971| val_0_mse: 689.20385|  0:00:09s\n",
      "epoch 490| loss: 177.45656| val_0_mse: 677.34528|  0:00:09s\n",
      "epoch 491| loss: 53.20713| val_0_mse: 592.11782|  0:00:09s\n",
      "epoch 492| loss: 131.90964| val_0_mse: 567.56704|  0:00:09s\n",
      "epoch 493| loss: 82.0296 | val_0_mse: 548.4949|  0:00:09s\n",
      "epoch 494| loss: 149.5103| val_0_mse: 435.14341|  0:00:09s\n",
      "epoch 495| loss: 185.60719| val_0_mse: 371.15705|  0:00:09s\n",
      "epoch 496| loss: 173.95502| val_0_mse: 347.72332|  0:00:09s\n",
      "epoch 497| loss: 77.24535| val_0_mse: 328.90724|  0:00:09s\n",
      "epoch 498| loss: 43.01637| val_0_mse: 317.54761|  0:00:09s\n",
      "epoch 499| loss: 195.87924| val_0_mse: 291.83673|  0:00:09s\n",
      "epoch 500| loss: 52.08096| val_0_mse: 340.33128|  0:00:09s\n",
      "epoch 501| loss: 207.71515| val_0_mse: 425.47538|  0:00:09s\n",
      "epoch 502| loss: 58.67824| val_0_mse: 348.42145|  0:00:09s\n",
      "epoch 503| loss: 81.74703| val_0_mse: 390.04304|  0:00:09s\n",
      "epoch 504| loss: 81.43893| val_0_mse: 400.46315|  0:00:09s\n",
      "epoch 505| loss: 372.65408| val_0_mse: 393.2023|  0:00:09s\n",
      "epoch 506| loss: 188.74673| val_0_mse: 458.94406|  0:00:09s\n",
      "epoch 507| loss: 79.5091 | val_0_mse: 475.98369|  0:00:09s\n",
      "epoch 508| loss: 74.08337| val_0_mse: 555.11383|  0:00:09s\n",
      "epoch 509| loss: 161.66074| val_0_mse: 552.90039|  0:00:09s\n",
      "epoch 510| loss: 169.72768| val_0_mse: 510.53222|  0:00:09s\n",
      "epoch 511| loss: 72.77209| val_0_mse: 591.15634|  0:00:09s\n",
      "epoch 512| loss: 141.7933| val_0_mse: 585.68643|  0:00:09s\n",
      "epoch 513| loss: 100.21732| val_0_mse: 465.5509|  0:00:09s\n",
      "epoch 514| loss: 72.92632| val_0_mse: 347.20312|  0:00:09s\n",
      "epoch 515| loss: 181.23264| val_0_mse: 251.51543|  0:00:09s\n",
      "epoch 516| loss: 255.48029| val_0_mse: 272.56803|  0:00:09s\n",
      "epoch 517| loss: 139.20142| val_0_mse: 318.98884|  0:00:09s\n",
      "epoch 518| loss: 112.94343| val_0_mse: 326.14692|  0:00:09s\n",
      "epoch 519| loss: 88.72154| val_0_mse: 301.63433|  0:00:09s\n",
      "epoch 520| loss: 304.27756| val_0_mse: 244.3559|  0:00:09s\n",
      "epoch 521| loss: 64.64846| val_0_mse: 175.98711|  0:00:09s\n",
      "epoch 522| loss: 153.94066| val_0_mse: 163.00093|  0:00:09s\n",
      "epoch 523| loss: 171.48444| val_0_mse: 176.80748|  0:00:09s\n",
      "epoch 524| loss: 135.03127| val_0_mse: 214.25609|  0:00:09s\n",
      "epoch 525| loss: 278.59088| val_0_mse: 259.58665|  0:00:09s\n",
      "epoch 526| loss: 71.99378| val_0_mse: 342.68458|  0:00:09s\n",
      "epoch 527| loss: 75.07198| val_0_mse: 375.27857|  0:00:09s\n",
      "epoch 528| loss: 81.7692 | val_0_mse: 493.80586|  0:00:09s\n",
      "epoch 529| loss: 184.25832| val_0_mse: 603.67892|  0:00:09s\n",
      "epoch 530| loss: 113.69493| val_0_mse: 550.0211|  0:00:09s\n",
      "epoch 531| loss: 53.49997| val_0_mse: 445.28153|  0:00:09s\n",
      "epoch 532| loss: 54.26678| val_0_mse: 352.9098|  0:00:09s\n",
      "epoch 533| loss: 45.34982| val_0_mse: 390.12444|  0:00:09s\n",
      "epoch 534| loss: 163.47121| val_0_mse: 449.01389|  0:00:09s\n",
      "epoch 535| loss: 49.20359| val_0_mse: 503.29672|  0:00:09s\n",
      "epoch 536| loss: 61.10319| val_0_mse: 541.68115|  0:00:09s\n",
      "epoch 537| loss: 37.09579| val_0_mse: 517.57042|  0:00:09s\n",
      "epoch 538| loss: 132.51332| val_0_mse: 519.72419|  0:00:09s\n",
      "epoch 539| loss: 59.73876| val_0_mse: 552.7667|  0:00:09s\n",
      "epoch 540| loss: 83.58311| val_0_mse: 559.25288|  0:00:09s\n",
      "epoch 541| loss: 77.01865| val_0_mse: 504.02806|  0:00:09s\n",
      "epoch 542| loss: 231.00011| val_0_mse: 376.81721|  0:00:09s\n",
      "epoch 543| loss: 82.07416| val_0_mse: 467.85004|  0:00:09s\n",
      "epoch 544| loss: 107.7234| val_0_mse: 507.06664|  0:00:09s\n",
      "epoch 545| loss: 68.69102| val_0_mse: 472.95341|  0:00:09s\n",
      "epoch 546| loss: 143.11055| val_0_mse: 575.24446|  0:00:09s\n",
      "epoch 547| loss: 154.61641| val_0_mse: 601.90056|  0:00:09s\n",
      "epoch 548| loss: 101.63736| val_0_mse: 515.37742|  0:00:09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 549| loss: 76.42796| val_0_mse: 338.84564|  0:00:09s\n",
      "epoch 550| loss: 79.82983| val_0_mse: 281.91032|  0:00:09s\n",
      "epoch 551| loss: 125.22348| val_0_mse: 338.64996|  0:00:09s\n",
      "epoch 552| loss: 270.5463| val_0_mse: 450.03933|  0:00:09s\n",
      "epoch 553| loss: 104.2912| val_0_mse: 525.57276|  0:00:09s\n",
      "epoch 554| loss: 89.66048| val_0_mse: 479.03534|  0:00:09s\n",
      "epoch 555| loss: 45.15277| val_0_mse: 383.89347|  0:00:09s\n",
      "epoch 556| loss: 37.18834| val_0_mse: 282.00194|  0:00:09s\n",
      "epoch 557| loss: 174.35075| val_0_mse: 258.48569|  0:00:09s\n",
      "epoch 558| loss: 75.46384| val_0_mse: 230.98425|  0:00:09s\n",
      "epoch 559| loss: 216.98195| val_0_mse: 228.01436|  0:00:09s\n",
      "epoch 560| loss: 151.16599| val_0_mse: 239.81333|  0:00:09s\n",
      "epoch 561| loss: 100.29947| val_0_mse: 290.16519|  0:00:09s\n",
      "epoch 562| loss: 94.54443| val_0_mse: 301.16272|  0:00:10s\n",
      "epoch 563| loss: 78.12361| val_0_mse: 314.40875|  0:00:10s\n",
      "epoch 564| loss: 160.88564| val_0_mse: 292.29196|  0:00:10s\n",
      "epoch 565| loss: 39.48642| val_0_mse: 308.57807|  0:00:10s\n",
      "epoch 566| loss: 100.29081| val_0_mse: 344.76515|  0:00:10s\n",
      "epoch 567| loss: 80.76441| val_0_mse: 347.02213|  0:00:10s\n",
      "epoch 568| loss: 134.90796| val_0_mse: 330.75119|  0:00:10s\n",
      "epoch 569| loss: 91.90459| val_0_mse: 265.54448|  0:00:10s\n",
      "epoch 570| loss: 65.36977| val_0_mse: 262.7136|  0:00:10s\n",
      "epoch 571| loss: 56.56758| val_0_mse: 366.821 |  0:00:10s\n",
      "epoch 572| loss: 58.80198| val_0_mse: 291.3281|  0:00:10s\n",
      "epoch 573| loss: 90.8391 | val_0_mse: 296.34505|  0:00:10s\n",
      "epoch 574| loss: 58.62633| val_0_mse: 297.68967|  0:00:10s\n",
      "epoch 575| loss: 103.0953| val_0_mse: 251.39135|  0:00:10s\n",
      "epoch 576| loss: 122.75652| val_0_mse: 220.75914|  0:00:10s\n",
      "epoch 577| loss: 81.56051| val_0_mse: 234.40644|  0:00:10s\n",
      "epoch 578| loss: 128.0011| val_0_mse: 286.37047|  0:00:10s\n",
      "epoch 579| loss: 82.51158| val_0_mse: 323.8258|  0:00:10s\n",
      "epoch 580| loss: 63.04787| val_0_mse: 347.73549|  0:00:10s\n",
      "epoch 581| loss: 118.94425| val_0_mse: 332.19947|  0:00:10s\n",
      "epoch 582| loss: 103.32824| val_0_mse: 273.06774|  0:00:10s\n",
      "epoch 583| loss: 56.49616| val_0_mse: 239.64133|  0:00:10s\n",
      "epoch 584| loss: 178.79437| val_0_mse: 249.70396|  0:00:10s\n",
      "epoch 585| loss: 124.25577| val_0_mse: 332.79748|  0:00:10s\n",
      "epoch 586| loss: 235.85143| val_0_mse: 516.42556|  0:00:10s\n",
      "epoch 587| loss: 103.7167| val_0_mse: 664.60912|  0:00:10s\n",
      "epoch 588| loss: 101.94576| val_0_mse: 587.41745|  0:00:10s\n",
      "epoch 589| loss: 59.38013| val_0_mse: 604.43879|  0:00:10s\n",
      "epoch 590| loss: 67.909  | val_0_mse: 667.84346|  0:00:10s\n",
      "epoch 591| loss: 68.75513| val_0_mse: 835.07169|  0:00:10s\n",
      "epoch 592| loss: 62.38998| val_0_mse: 1005.7217|  0:00:10s\n",
      "epoch 593| loss: 56.67897| val_0_mse: 846.76232|  0:00:10s\n",
      "epoch 594| loss: 40.3751 | val_0_mse: 659.5927|  0:00:10s\n",
      "epoch 595| loss: 131.78304| val_0_mse: 578.11043|  0:00:10s\n",
      "epoch 596| loss: 87.82507| val_0_mse: 564.22832|  0:00:10s\n",
      "epoch 597| loss: 32.85109| val_0_mse: 618.32084|  0:00:10s\n",
      "epoch 598| loss: 58.93685| val_0_mse: 577.9529|  0:00:10s\n",
      "epoch 599| loss: 114.36752| val_0_mse: 333.26908|  0:00:10s\n",
      "epoch 600| loss: 97.85719| val_0_mse: 299.52254|  0:00:10s\n",
      "epoch 601| loss: 68.74621| val_0_mse: 277.17536|  0:00:10s\n",
      "epoch 602| loss: 76.28533| val_0_mse: 224.44859|  0:00:10s\n",
      "epoch 603| loss: 107.63213| val_0_mse: 292.7169|  0:00:10s\n",
      "epoch 604| loss: 106.10185| val_0_mse: 352.65976|  0:00:10s\n",
      "epoch 605| loss: 134.50049| val_0_mse: 364.31958|  0:00:10s\n",
      "epoch 606| loss: 244.16605| val_0_mse: 288.64745|  0:00:10s\n",
      "epoch 607| loss: 168.39339| val_0_mse: 306.85935|  0:00:10s\n",
      "epoch 608| loss: 87.79542| val_0_mse: 255.21355|  0:00:10s\n",
      "epoch 609| loss: 169.82254| val_0_mse: 243.51636|  0:00:10s\n",
      "epoch 610| loss: 90.40839| val_0_mse: 207.05675|  0:00:10s\n",
      "epoch 611| loss: 51.95922| val_0_mse: 294.20477|  0:00:10s\n",
      "epoch 612| loss: 372.83624| val_0_mse: 330.41368|  0:00:10s\n",
      "epoch 613| loss: 152.53458| val_0_mse: 324.58948|  0:00:10s\n",
      "epoch 614| loss: 106.91648| val_0_mse: 268.94399|  0:00:10s\n",
      "epoch 615| loss: 33.26416| val_0_mse: 192.13434|  0:00:10s\n",
      "epoch 616| loss: 74.05722| val_0_mse: 217.24054|  0:00:10s\n",
      "epoch 617| loss: 57.80411| val_0_mse: 245.14967|  0:00:10s\n",
      "epoch 618| loss: 287.99835| val_0_mse: 326.42355|  0:00:10s\n",
      "epoch 619| loss: 115.75221| val_0_mse: 356.55074|  0:00:10s\n",
      "epoch 620| loss: 86.93353| val_0_mse: 325.50121|  0:00:10s\n",
      "epoch 621| loss: 41.847  | val_0_mse: 273.63855|  0:00:10s\n",
      "epoch 622| loss: 46.84578| val_0_mse: 257.50137|  0:00:10s\n",
      "epoch 623| loss: 57.45256| val_0_mse: 243.54977|  0:00:10s\n",
      "epoch 624| loss: 109.47649| val_0_mse: 257.95534|  0:00:10s\n",
      "epoch 625| loss: 196.22083| val_0_mse: 267.78202|  0:00:10s\n",
      "epoch 626| loss: 95.61405| val_0_mse: 240.41346|  0:00:10s\n",
      "epoch 627| loss: 282.26648| val_0_mse: 268.32056|  0:00:10s\n",
      "epoch 628| loss: 106.32529| val_0_mse: 340.06365|  0:00:10s\n",
      "epoch 629| loss: 112.62371| val_0_mse: 324.25971|  0:00:10s\n",
      "epoch 630| loss: 64.29078| val_0_mse: 315.19312|  0:00:10s\n",
      "epoch 631| loss: 103.20591| val_0_mse: 272.53126|  0:00:10s\n",
      "epoch 632| loss: 85.94025| val_0_mse: 304.82569|  0:00:10s\n",
      "epoch 633| loss: 278.92136| val_0_mse: 401.06845|  0:00:10s\n",
      "epoch 634| loss: 82.33327| val_0_mse: 436.66523|  0:00:10s\n",
      "epoch 635| loss: 76.24139| val_0_mse: 414.6713|  0:00:10s\n",
      "epoch 636| loss: 46.91643| val_0_mse: 306.21829|  0:00:10s\n",
      "epoch 637| loss: 61.92244| val_0_mse: 254.08881|  0:00:11s\n",
      "epoch 638| loss: 134.91719| val_0_mse: 260.19976|  0:00:11s\n",
      "epoch 639| loss: 121.73296| val_0_mse: 358.93472|  0:00:11s\n",
      "epoch 640| loss: 36.4509 | val_0_mse: 588.25092|  0:00:11s\n",
      "epoch 641| loss: 87.72179| val_0_mse: 721.21366|  0:00:11s\n",
      "epoch 642| loss: 183.86255| val_0_mse: 687.42822|  0:00:11s\n",
      "epoch 643| loss: 99.68114| val_0_mse: 561.70562|  0:00:11s\n",
      "epoch 644| loss: 207.56027| val_0_mse: 514.12003|  0:00:11s\n",
      "epoch 645| loss: 78.37893| val_0_mse: 492.58227|  0:00:11s\n",
      "epoch 646| loss: 118.1851| val_0_mse: 482.24602|  0:00:11s\n",
      "epoch 647| loss: 128.97359| val_0_mse: 455.94848|  0:00:11s\n",
      "epoch 648| loss: 108.86893| val_0_mse: 437.27622|  0:00:11s\n",
      "epoch 649| loss: 80.29017| val_0_mse: 545.38212|  0:00:11s\n",
      "epoch 650| loss: 79.32889| val_0_mse: 701.12653|  0:00:11s\n",
      "epoch 651| loss: 90.74583| val_0_mse: 727.55559|  0:00:11s\n",
      "epoch 652| loss: 124.64278| val_0_mse: 589.09674|  0:00:11s\n",
      "epoch 653| loss: 113.52268| val_0_mse: 428.23555|  0:00:11s\n",
      "epoch 654| loss: 179.39357| val_0_mse: 337.34727|  0:00:11s\n",
      "epoch 655| loss: 88.65476| val_0_mse: 320.8804|  0:00:11s\n",
      "epoch 656| loss: 109.10023| val_0_mse: 278.70717|  0:00:11s\n",
      "epoch 657| loss: 77.4376 | val_0_mse: 269.25462|  0:00:11s\n",
      "epoch 658| loss: 56.16899| val_0_mse: 285.43243|  0:00:11s\n",
      "epoch 659| loss: 46.91313| val_0_mse: 299.43059|  0:00:11s\n",
      "epoch 660| loss: 56.14554| val_0_mse: 335.27674|  0:00:11s\n",
      "epoch 661| loss: 63.32715| val_0_mse: 362.2666|  0:00:11s\n",
      "epoch 662| loss: 51.80995| val_0_mse: 361.24126|  0:00:11s\n",
      "epoch 663| loss: 110.26701| val_0_mse: 331.18455|  0:00:11s\n",
      "epoch 664| loss: 56.50276| val_0_mse: 254.62279|  0:00:11s\n",
      "epoch 665| loss: 141.09453| val_0_mse: 176.25604|  0:00:11s\n",
      "epoch 666| loss: 94.54621| val_0_mse: 167.70647|  0:00:11s\n",
      "epoch 667| loss: 150.07103| val_0_mse: 193.55005|  0:00:11s\n",
      "epoch 668| loss: 110.23814| val_0_mse: 212.99024|  0:00:11s\n",
      "epoch 669| loss: 64.53495| val_0_mse: 234.79248|  0:00:11s\n",
      "epoch 670| loss: 61.69108| val_0_mse: 318.40662|  0:00:11s\n",
      "epoch 671| loss: 60.74595| val_0_mse: 402.6758|  0:00:11s\n",
      "epoch 672| loss: 93.39892| val_0_mse: 388.14939|  0:00:11s\n",
      "epoch 673| loss: 41.97723| val_0_mse: 465.07901|  0:00:11s\n",
      "epoch 674| loss: 73.1432 | val_0_mse: 334.03313|  0:00:11s\n",
      "epoch 675| loss: 95.11415| val_0_mse: 353.21464|  0:00:11s\n",
      "epoch 676| loss: 131.96997| val_0_mse: 363.6764|  0:00:11s\n",
      "epoch 677| loss: 108.86035| val_0_mse: 455.2836|  0:00:11s\n",
      "epoch 678| loss: 85.34882| val_0_mse: 413.27548|  0:00:11s\n",
      "epoch 679| loss: 47.87893| val_0_mse: 532.26745|  0:00:11s\n",
      "epoch 680| loss: 57.21736| val_0_mse: 621.12152|  0:00:11s\n",
      "epoch 681| loss: 131.61005| val_0_mse: 296.09267|  0:00:11s\n",
      "epoch 682| loss: 103.28429| val_0_mse: 211.22515|  0:00:11s\n",
      "epoch 683| loss: 95.16969| val_0_mse: 177.67888|  0:00:11s\n",
      "epoch 684| loss: 67.55027| val_0_mse: 224.7737|  0:00:11s\n",
      "epoch 685| loss: 155.34633| val_0_mse: 243.78095|  0:00:11s\n",
      "epoch 686| loss: 131.02274| val_0_mse: 202.41598|  0:00:11s\n",
      "epoch 687| loss: 86.64642| val_0_mse: 245.51956|  0:00:11s\n",
      "epoch 688| loss: 140.29633| val_0_mse: 269.52365|  0:00:11s\n",
      "epoch 689| loss: 55.79163| val_0_mse: 267.27396|  0:00:11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 690| loss: 68.71866| val_0_mse: 240.56322|  0:00:11s\n",
      "epoch 691| loss: 133.74792| val_0_mse: 220.64544|  0:00:11s\n",
      "epoch 692| loss: 56.8097 | val_0_mse: 252.02895|  0:00:11s\n",
      "epoch 693| loss: 76.97115| val_0_mse: 252.81396|  0:00:11s\n",
      "epoch 694| loss: 58.48356| val_0_mse: 249.85915|  0:00:11s\n",
      "epoch 695| loss: 61.05163| val_0_mse: 173.95511|  0:00:11s\n",
      "epoch 696| loss: 56.5734 | val_0_mse: 134.52989|  0:00:11s\n",
      "epoch 697| loss: 58.1039 | val_0_mse: 176.89984|  0:00:11s\n",
      "epoch 698| loss: 110.70933| val_0_mse: 211.63484|  0:00:11s\n",
      "epoch 699| loss: 293.71841| val_0_mse: 191.42558|  0:00:11s\n",
      "epoch 700| loss: 146.40306| val_0_mse: 175.25886|  0:00:11s\n",
      "epoch 701| loss: 49.1085 | val_0_mse: 198.18289|  0:00:11s\n",
      "epoch 702| loss: 169.30608| val_0_mse: 207.07129|  0:00:11s\n",
      "epoch 703| loss: 74.2458 | val_0_mse: 240.38892|  0:00:11s\n",
      "epoch 704| loss: 94.89658| val_0_mse: 282.09259|  0:00:11s\n",
      "epoch 705| loss: 93.9006 | val_0_mse: 330.04119|  0:00:11s\n",
      "epoch 706| loss: 70.72801| val_0_mse: 299.53  |  0:00:11s\n",
      "epoch 707| loss: 63.02452| val_0_mse: 443.46345|  0:00:11s\n",
      "epoch 708| loss: 60.42138| val_0_mse: 508.71036|  0:00:11s\n",
      "epoch 709| loss: 58.07003| val_0_mse: 499.49386|  0:00:11s\n",
      "epoch 710| loss: 69.47996| val_0_mse: 503.48472|  0:00:11s\n",
      "epoch 711| loss: 210.20244| val_0_mse: 339.93569|  0:00:11s\n",
      "epoch 712| loss: 128.07919| val_0_mse: 221.03361|  0:00:12s\n",
      "epoch 713| loss: 117.93716| val_0_mse: 205.04772|  0:00:12s\n",
      "epoch 714| loss: 67.44591| val_0_mse: 300.631 |  0:00:12s\n",
      "epoch 715| loss: 110.15555| val_0_mse: 374.23482|  0:00:12s\n",
      "epoch 716| loss: 65.65113| val_0_mse: 360.78167|  0:00:12s\n",
      "epoch 717| loss: 273.07343| val_0_mse: 317.2922|  0:00:12s\n",
      "epoch 718| loss: 258.3815| val_0_mse: 334.0247|  0:00:12s\n",
      "epoch 719| loss: 75.00501| val_0_mse: 396.75055|  0:00:12s\n",
      "epoch 720| loss: 66.9975 | val_0_mse: 392.95819|  0:00:12s\n",
      "epoch 721| loss: 50.4399 | val_0_mse: 350.81651|  0:00:12s\n",
      "epoch 722| loss: 59.92603| val_0_mse: 323.73507|  0:00:12s\n",
      "epoch 723| loss: 304.20023| val_0_mse: 351.44536|  0:00:12s\n",
      "epoch 724| loss: 78.92481| val_0_mse: 480.36471|  0:00:12s\n",
      "epoch 725| loss: 198.01022| val_0_mse: 624.72828|  0:00:12s\n",
      "epoch 726| loss: 437.07986| val_0_mse: 1018.79346|  0:00:12s\n",
      "epoch 727| loss: 107.953 | val_0_mse: 1210.87877|  0:00:12s\n",
      "epoch 728| loss: 95.65576| val_0_mse: 1192.90304|  0:00:12s\n",
      "epoch 729| loss: 194.98799| val_0_mse: 1059.42431|  0:00:12s\n",
      "epoch 730| loss: 107.31115| val_0_mse: 879.50719|  0:00:12s\n",
      "epoch 731| loss: 100.60674| val_0_mse: 629.38959|  0:00:12s\n",
      "epoch 732| loss: 113.2965| val_0_mse: 456.9094|  0:00:12s\n",
      "epoch 733| loss: 137.11456| val_0_mse: 406.5141|  0:00:12s\n",
      "epoch 734| loss: 157.01253| val_0_mse: 398.86028|  0:00:12s\n",
      "epoch 735| loss: 112.92168| val_0_mse: 323.92571|  0:00:12s\n",
      "epoch 736| loss: 116.31592| val_0_mse: 250.9626|  0:00:12s\n",
      "epoch 737| loss: 87.99055| val_0_mse: 201.05569|  0:00:12s\n",
      "epoch 738| loss: 90.03399| val_0_mse: 257.50804|  0:00:12s\n",
      "epoch 739| loss: 75.28463| val_0_mse: 274.38066|  0:00:12s\n",
      "epoch 740| loss: 77.62266| val_0_mse: 323.36044|  0:00:12s\n",
      "epoch 741| loss: 66.87131| val_0_mse: 391.99056|  0:00:12s\n",
      "epoch 742| loss: 123.85268| val_0_mse: 425.22697|  0:00:12s\n",
      "epoch 743| loss: 163.41734| val_0_mse: 448.03288|  0:00:12s\n",
      "epoch 744| loss: 102.31593| val_0_mse: 460.8605|  0:00:12s\n",
      "epoch 745| loss: 102.26794| val_0_mse: 426.27827|  0:00:12s\n",
      "epoch 746| loss: 149.46539| val_0_mse: 465.91282|  0:00:12s\n",
      "epoch 747| loss: 212.56999| val_0_mse: 393.7748|  0:00:12s\n",
      "epoch 748| loss: 137.04381| val_0_mse: 351.28113|  0:00:12s\n",
      "epoch 749| loss: 44.80442| val_0_mse: 320.82957|  0:00:12s\n",
      "epoch 750| loss: 126.14854| val_0_mse: 365.58565|  0:00:12s\n",
      "epoch 751| loss: 271.67792| val_0_mse: 415.42905|  0:00:12s\n",
      "epoch 752| loss: 211.07927| val_0_mse: 444.65736|  0:00:12s\n",
      "epoch 753| loss: 209.67479| val_0_mse: 429.57419|  0:00:12s\n",
      "epoch 754| loss: 84.6328 | val_0_mse: 495.2279|  0:00:12s\n",
      "epoch 755| loss: 131.02235| val_0_mse: 465.12637|  0:00:12s\n",
      "epoch 756| loss: 67.62025| val_0_mse: 422.28185|  0:00:12s\n",
      "epoch 757| loss: 101.9815| val_0_mse: 375.76331|  0:00:12s\n",
      "epoch 758| loss: 86.36386| val_0_mse: 323.26247|  0:00:12s\n",
      "epoch 759| loss: 120.93932| val_0_mse: 308.20624|  0:00:12s\n",
      "epoch 760| loss: 88.45612| val_0_mse: 328.91989|  0:00:12s\n",
      "epoch 761| loss: 54.55316| val_0_mse: 343.64375|  0:00:12s\n",
      "epoch 762| loss: 228.48405| val_0_mse: 438.97556|  0:00:12s\n",
      "epoch 763| loss: 102.43134| val_0_mse: 522.9814|  0:00:12s\n",
      "epoch 764| loss: 87.8843 | val_0_mse: 530.89061|  0:00:12s\n",
      "epoch 765| loss: 121.24024| val_0_mse: 532.94434|  0:00:12s\n",
      "epoch 766| loss: 64.75556| val_0_mse: 501.72913|  0:00:12s\n",
      "epoch 767| loss: 43.94293| val_0_mse: 499.35177|  0:00:12s\n",
      "epoch 768| loss: 154.8159| val_0_mse: 390.45828|  0:00:12s\n",
      "epoch 769| loss: 109.02538| val_0_mse: 338.03396|  0:00:12s\n",
      "epoch 770| loss: 64.55711| val_0_mse: 314.38523|  0:00:12s\n",
      "epoch 771| loss: 45.28916| val_0_mse: 280.8611|  0:00:12s\n",
      "epoch 772| loss: 51.54855| val_0_mse: 330.78464|  0:00:12s\n",
      "epoch 773| loss: 46.49851| val_0_mse: 384.27209|  0:00:12s\n",
      "epoch 774| loss: 152.52498| val_0_mse: 369.27351|  0:00:12s\n",
      "epoch 775| loss: 117.99383| val_0_mse: 306.91906|  0:00:12s\n",
      "epoch 776| loss: 115.24726| val_0_mse: 233.28528|  0:00:12s\n",
      "epoch 777| loss: 101.13528| val_0_mse: 201.7819|  0:00:12s\n",
      "epoch 778| loss: 49.49944| val_0_mse: 228.57824|  0:00:12s\n",
      "epoch 779| loss: 36.02917| val_0_mse: 233.97175|  0:00:12s\n",
      "epoch 780| loss: 297.3829| val_0_mse: 264.92725|  0:00:12s\n",
      "epoch 781| loss: 75.13046| val_0_mse: 294.13836|  0:00:12s\n",
      "epoch 782| loss: 82.08832| val_0_mse: 392.70846|  0:00:12s\n",
      "epoch 783| loss: 59.1823 | val_0_mse: 451.93239|  0:00:12s\n",
      "epoch 784| loss: 98.46883| val_0_mse: 447.64814|  0:00:12s\n",
      "epoch 785| loss: 81.28947| val_0_mse: 375.19627|  0:00:13s\n",
      "epoch 786| loss: 81.6401 | val_0_mse: 342.35921|  0:00:13s\n",
      "epoch 787| loss: 190.42674| val_0_mse: 338.9391|  0:00:13s\n",
      "epoch 788| loss: 152.52592| val_0_mse: 415.10203|  0:00:13s\n",
      "epoch 789| loss: 97.48046| val_0_mse: 545.29447|  0:00:13s\n",
      "epoch 790| loss: 66.92923| val_0_mse: 703.60778|  0:00:13s\n",
      "epoch 791| loss: 149.83287| val_0_mse: 718.18837|  0:00:13s\n",
      "epoch 792| loss: 174.16695| val_0_mse: 532.90436|  0:00:13s\n",
      "epoch 793| loss: 99.2687 | val_0_mse: 344.54096|  0:00:13s\n",
      "epoch 794| loss: 61.30075| val_0_mse: 226.0519|  0:00:13s\n",
      "epoch 795| loss: 122.03844| val_0_mse: 189.10581|  0:00:13s\n",
      "epoch 796| loss: 148.87125| val_0_mse: 182.14312|  0:00:13s\n",
      "epoch 797| loss: 76.89361| val_0_mse: 189.21901|  0:00:13s\n",
      "epoch 798| loss: 62.98454| val_0_mse: 238.74446|  0:00:13s\n",
      "epoch 799| loss: 96.83229| val_0_mse: 238.88879|  0:00:13s\n",
      "epoch 800| loss: 86.25019| val_0_mse: 258.15543|  0:00:13s\n",
      "epoch 801| loss: 140.22493| val_0_mse: 220.10381|  0:00:13s\n",
      "epoch 802| loss: 118.84617| val_0_mse: 245.04913|  0:00:13s\n",
      "epoch 803| loss: 81.63623| val_0_mse: 205.49942|  0:00:13s\n",
      "epoch 804| loss: 69.98728| val_0_mse: 169.98445|  0:00:13s\n",
      "epoch 805| loss: 84.2993 | val_0_mse: 181.07992|  0:00:13s\n",
      "epoch 806| loss: 59.55527| val_0_mse: 167.96449|  0:00:13s\n",
      "epoch 807| loss: 108.48694| val_0_mse: 167.59565|  0:00:13s\n",
      "epoch 808| loss: 227.10095| val_0_mse: 170.9627|  0:00:13s\n",
      "epoch 809| loss: 106.56054| val_0_mse: 163.1356|  0:00:13s\n",
      "epoch 810| loss: 179.47903| val_0_mse: 171.13353|  0:00:13s\n",
      "epoch 811| loss: 334.76373| val_0_mse: 256.45577|  0:00:13s\n",
      "epoch 812| loss: 75.56679| val_0_mse: 355.20473|  0:00:13s\n",
      "epoch 813| loss: 139.30891| val_0_mse: 506.00046|  0:00:13s\n",
      "epoch 814| loss: 227.35217| val_0_mse: 519.2995|  0:00:13s\n",
      "epoch 815| loss: 204.47182| val_0_mse: 361.75942|  0:00:13s\n",
      "epoch 816| loss: 234.42261| val_0_mse: 328.88052|  0:00:13s\n",
      "epoch 817| loss: 55.63271| val_0_mse: 233.09813|  0:00:13s\n",
      "epoch 818| loss: 165.26616| val_0_mse: 196.56877|  0:00:13s\n",
      "epoch 819| loss: 90.77354| val_0_mse: 192.77433|  0:00:13s\n",
      "epoch 820| loss: 88.36442| val_0_mse: 220.44296|  0:00:13s\n",
      "epoch 821| loss: 81.24323| val_0_mse: 252.56258|  0:00:13s\n",
      "epoch 822| loss: 137.93451| val_0_mse: 305.44211|  0:00:13s\n",
      "epoch 823| loss: 71.67971| val_0_mse: 324.70833|  0:00:13s\n",
      "epoch 824| loss: 79.33286| val_0_mse: 278.2211|  0:00:13s\n",
      "epoch 825| loss: 64.98447| val_0_mse: 191.68854|  0:00:13s\n",
      "epoch 826| loss: 92.83549| val_0_mse: 173.93748|  0:00:13s\n",
      "epoch 827| loss: 119.32584| val_0_mse: 197.43283|  0:00:13s\n",
      "epoch 828| loss: 176.91306| val_0_mse: 237.08835|  0:00:13s\n",
      "epoch 829| loss: 169.58797| val_0_mse: 269.82634|  0:00:13s\n",
      "epoch 830| loss: 142.57994| val_0_mse: 331.68983|  0:00:13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 831| loss: 91.76626| val_0_mse: 317.30338|  0:00:13s\n",
      "epoch 832| loss: 83.40885| val_0_mse: 241.33089|  0:00:13s\n",
      "epoch 833| loss: 165.68864| val_0_mse: 267.21921|  0:00:13s\n",
      "epoch 834| loss: 122.33272| val_0_mse: 305.88491|  0:00:13s\n",
      "epoch 835| loss: 124.99906| val_0_mse: 341.14799|  0:00:13s\n",
      "epoch 836| loss: 109.05976| val_0_mse: 376.55119|  0:00:13s\n",
      "epoch 837| loss: 81.03655| val_0_mse: 368.24256|  0:00:13s\n",
      "epoch 838| loss: 83.88097| val_0_mse: 315.78911|  0:00:13s\n",
      "epoch 839| loss: 67.23376| val_0_mse: 237.11416|  0:00:13s\n",
      "epoch 840| loss: 52.67363| val_0_mse: 233.36562|  0:00:13s\n",
      "epoch 841| loss: 73.39273| val_0_mse: 229.04689|  0:00:13s\n",
      "epoch 842| loss: 125.55729| val_0_mse: 266.4862|  0:00:13s\n",
      "epoch 843| loss: 184.05455| val_0_mse: 312.31596|  0:00:13s\n",
      "epoch 844| loss: 66.8587 | val_0_mse: 414.67386|  0:00:13s\n",
      "epoch 845| loss: 91.91307| val_0_mse: 435.79837|  0:00:13s\n",
      "epoch 846| loss: 43.92969| val_0_mse: 364.83584|  0:00:13s\n",
      "epoch 847| loss: 72.69176| val_0_mse: 325.53814|  0:00:13s\n",
      "epoch 848| loss: 203.15425| val_0_mse: 284.73546|  0:00:13s\n",
      "epoch 849| loss: 82.75102| val_0_mse: 305.29108|  0:00:13s\n",
      "epoch 850| loss: 48.38911| val_0_mse: 359.84691|  0:00:13s\n",
      "epoch 851| loss: 120.9716| val_0_mse: 338.83141|  0:00:13s\n",
      "epoch 852| loss: 42.79422| val_0_mse: 291.8086|  0:00:13s\n",
      "epoch 853| loss: 121.45496| val_0_mse: 347.44173|  0:00:13s\n",
      "epoch 854| loss: 41.12136| val_0_mse: 589.59651|  0:00:13s\n",
      "epoch 855| loss: 66.13066| val_0_mse: 631.07129|  0:00:13s\n",
      "epoch 856| loss: 127.13254| val_0_mse: 502.50179|  0:00:13s\n",
      "epoch 857| loss: 118.17494| val_0_mse: 358.73218|  0:00:13s\n",
      "epoch 858| loss: 136.46062| val_0_mse: 392.48037|  0:00:13s\n",
      "epoch 859| loss: 154.76772| val_0_mse: 502.86257|  0:00:13s\n",
      "epoch 860| loss: 81.9068 | val_0_mse: 712.01028|  0:00:13s\n",
      "epoch 861| loss: 52.79759| val_0_mse: 934.48373|  0:00:14s\n",
      "epoch 862| loss: 119.39605| val_0_mse: 1017.82626|  0:00:14s\n",
      "epoch 863| loss: 226.23352| val_0_mse: 912.20305|  0:00:14s\n",
      "epoch 864| loss: 268.48697| val_0_mse: 797.31806|  0:00:14s\n",
      "epoch 865| loss: 118.26609| val_0_mse: 555.36789|  0:00:14s\n",
      "epoch 866| loss: 100.01694| val_0_mse: 404.64807|  0:00:14s\n",
      "epoch 867| loss: 97.11533| val_0_mse: 316.62037|  0:00:14s\n",
      "epoch 868| loss: 84.86808| val_0_mse: 318.02901|  0:00:14s\n",
      "epoch 869| loss: 95.50018| val_0_mse: 318.27076|  0:00:14s\n",
      "epoch 870| loss: 124.03687| val_0_mse: 342.57599|  0:00:14s\n",
      "epoch 871| loss: 151.09322| val_0_mse: 435.17649|  0:00:14s\n",
      "epoch 872| loss: 123.43027| val_0_mse: 471.73514|  0:00:14s\n",
      "epoch 873| loss: 155.89212| val_0_mse: 405.08443|  0:00:14s\n",
      "epoch 874| loss: 210.68304| val_0_mse: 255.30413|  0:00:14s\n",
      "epoch 875| loss: 72.69913| val_0_mse: 183.8866|  0:00:14s\n",
      "epoch 876| loss: 96.17854| val_0_mse: 169.12626|  0:00:14s\n",
      "epoch 877| loss: 91.28745| val_0_mse: 193.82474|  0:00:14s\n",
      "epoch 878| loss: 120.55415| val_0_mse: 216.74706|  0:00:14s\n",
      "epoch 879| loss: 86.46523| val_0_mse: 217.67443|  0:00:14s\n",
      "epoch 880| loss: 102.54346| val_0_mse: 218.39918|  0:00:14s\n",
      "epoch 881| loss: 78.14408| val_0_mse: 209.13772|  0:00:14s\n",
      "epoch 882| loss: 175.31615| val_0_mse: 216.91214|  0:00:14s\n",
      "epoch 883| loss: 149.35703| val_0_mse: 242.44727|  0:00:14s\n",
      "epoch 884| loss: 72.77378| val_0_mse: 235.62487|  0:00:14s\n",
      "epoch 885| loss: 63.3845 | val_0_mse: 205.06396|  0:00:14s\n",
      "epoch 886| loss: 121.08545| val_0_mse: 179.98174|  0:00:14s\n",
      "epoch 887| loss: 149.51282| val_0_mse: 186.29519|  0:00:14s\n",
      "epoch 888| loss: 59.04634| val_0_mse: 216.41331|  0:00:14s\n",
      "epoch 889| loss: 108.06468| val_0_mse: 290.93152|  0:00:14s\n",
      "epoch 890| loss: 88.77855| val_0_mse: 315.18499|  0:00:14s\n",
      "epoch 891| loss: 46.52354| val_0_mse: 306.26413|  0:00:14s\n",
      "epoch 892| loss: 52.64149| val_0_mse: 321.38181|  0:00:14s\n",
      "epoch 893| loss: 66.26109| val_0_mse: 338.43732|  0:00:14s\n",
      "epoch 894| loss: 59.09424| val_0_mse: 407.3764|  0:00:14s\n",
      "epoch 895| loss: 53.04128| val_0_mse: 416.30015|  0:00:14s\n",
      "epoch 896| loss: 240.42229| val_0_mse: 289.4942|  0:00:14s\n",
      "epoch 897| loss: 35.27106| val_0_mse: 240.33189|  0:00:14s\n",
      "epoch 898| loss: 51.12408| val_0_mse: 196.55704|  0:00:14s\n",
      "epoch 899| loss: 54.59438| val_0_mse: 186.19005|  0:00:14s\n",
      "epoch 900| loss: 59.19751| val_0_mse: 208.95128|  0:00:14s\n",
      "epoch 901| loss: 54.69196| val_0_mse: 257.54492|  0:00:14s\n",
      "epoch 902| loss: 105.4956| val_0_mse: 274.16249|  0:00:14s\n",
      "epoch 903| loss: 56.00005| val_0_mse: 270.48504|  0:00:14s\n",
      "epoch 904| loss: 61.45229| val_0_mse: 327.6343|  0:00:14s\n",
      "epoch 905| loss: 80.91706| val_0_mse: 342.03088|  0:00:14s\n",
      "epoch 906| loss: 73.69736| val_0_mse: 298.20818|  0:00:14s\n",
      "epoch 907| loss: 156.87825| val_0_mse: 246.94682|  0:00:14s\n",
      "epoch 908| loss: 217.88438| val_0_mse: 259.94469|  0:00:14s\n",
      "epoch 909| loss: 86.85079| val_0_mse: 262.43225|  0:00:14s\n",
      "epoch 910| loss: 57.47736| val_0_mse: 269.16056|  0:00:14s\n",
      "epoch 911| loss: 40.19148| val_0_mse: 240.77059|  0:00:14s\n",
      "epoch 912| loss: 76.88381| val_0_mse: 220.04979|  0:00:14s\n",
      "epoch 913| loss: 45.74645| val_0_mse: 238.81676|  0:00:14s\n",
      "epoch 914| loss: 35.28104| val_0_mse: 263.61627|  0:00:14s\n",
      "epoch 915| loss: 39.44469| val_0_mse: 259.77265|  0:00:14s\n",
      "epoch 916| loss: 50.42754| val_0_mse: 234.18459|  0:00:14s\n",
      "epoch 917| loss: 46.1579 | val_0_mse: 234.52712|  0:00:14s\n",
      "epoch 918| loss: 62.04869| val_0_mse: 251.42817|  0:00:14s\n",
      "epoch 919| loss: 48.96782| val_0_mse: 242.54713|  0:00:14s\n",
      "epoch 920| loss: 75.69713| val_0_mse: 234.98647|  0:00:14s\n",
      "epoch 921| loss: 31.99775| val_0_mse: 255.6289|  0:00:14s\n",
      "epoch 922| loss: 56.51544| val_0_mse: 261.94392|  0:00:14s\n",
      "epoch 923| loss: 48.56414| val_0_mse: 352.2415|  0:00:14s\n",
      "epoch 924| loss: 50.27796| val_0_mse: 368.59055|  0:00:14s\n",
      "epoch 925| loss: 76.85698| val_0_mse: 302.61837|  0:00:14s\n",
      "epoch 926| loss: 62.38571| val_0_mse: 317.57019|  0:00:14s\n",
      "epoch 927| loss: 32.26159| val_0_mse: 334.22647|  0:00:14s\n",
      "epoch 928| loss: 63.66304| val_0_mse: 382.96977|  0:00:14s\n",
      "epoch 929| loss: 77.43683| val_0_mse: 381.20509|  0:00:14s\n",
      "epoch 930| loss: 155.4782| val_0_mse: 363.98595|  0:00:14s\n",
      "epoch 931| loss: 158.90663| val_0_mse: 359.8516|  0:00:14s\n",
      "epoch 932| loss: 36.52367| val_0_mse: 326.11254|  0:00:14s\n",
      "epoch 933| loss: 94.00607| val_0_mse: 294.17318|  0:00:14s\n",
      "epoch 934| loss: 123.08777| val_0_mse: 291.67019|  0:00:14s\n",
      "epoch 935| loss: 94.37602| val_0_mse: 315.2362|  0:00:15s\n",
      "epoch 936| loss: 107.00179| val_0_mse: 388.01746|  0:00:15s\n",
      "epoch 937| loss: 71.64783| val_0_mse: 380.04265|  0:00:15s\n",
      "epoch 938| loss: 88.37699| val_0_mse: 365.81168|  0:00:15s\n",
      "epoch 939| loss: 57.96813| val_0_mse: 370.49403|  0:00:15s\n",
      "epoch 940| loss: 51.89512| val_0_mse: 366.36996|  0:00:15s\n",
      "epoch 941| loss: 81.34128| val_0_mse: 374.38454|  0:00:15s\n",
      "epoch 942| loss: 71.28009| val_0_mse: 380.59051|  0:00:15s\n",
      "epoch 943| loss: 59.10184| val_0_mse: 323.58021|  0:00:15s\n",
      "epoch 944| loss: 104.89135| val_0_mse: 269.1195|  0:00:15s\n",
      "epoch 945| loss: 86.62739| val_0_mse: 244.18364|  0:00:15s\n",
      "epoch 946| loss: 54.75771| val_0_mse: 278.17549|  0:00:15s\n",
      "epoch 947| loss: 103.79265| val_0_mse: 298.49908|  0:00:15s\n",
      "epoch 948| loss: 90.30317| val_0_mse: 263.45201|  0:00:15s\n",
      "epoch 949| loss: 254.58618| val_0_mse: 378.78639|  0:00:15s\n",
      "epoch 950| loss: 114.76855| val_0_mse: 430.4519|  0:00:15s\n",
      "epoch 951| loss: 58.24144| val_0_mse: 534.92725|  0:00:15s\n",
      "epoch 952| loss: 63.54487| val_0_mse: 529.79175|  0:00:15s\n",
      "epoch 953| loss: 67.68755| val_0_mse: 408.69472|  0:00:15s\n",
      "epoch 954| loss: 73.39838| val_0_mse: 491.07391|  0:00:15s\n",
      "epoch 955| loss: 174.28587| val_0_mse: 456.54712|  0:00:15s\n",
      "epoch 956| loss: 209.17609| val_0_mse: 521.11267|  0:00:15s\n",
      "epoch 957| loss: 123.83793| val_0_mse: 726.01687|  0:00:15s\n",
      "epoch 958| loss: 39.73089| val_0_mse: 725.58391|  0:00:15s\n",
      "epoch 959| loss: 81.43998| val_0_mse: 863.43368|  0:00:15s\n",
      "epoch 960| loss: 97.08871| val_0_mse: 974.83189|  0:00:15s\n",
      "epoch 961| loss: 140.15387| val_0_mse: 1051.17948|  0:00:15s\n",
      "epoch 962| loss: 186.53784| val_0_mse: 988.22299|  0:00:15s\n",
      "epoch 963| loss: 82.70642| val_0_mse: 850.04924|  0:00:15s\n",
      "epoch 964| loss: 111.54928| val_0_mse: 559.06687|  0:00:15s\n",
      "epoch 965| loss: 87.80694| val_0_mse: 356.24842|  0:00:15s\n",
      "epoch 966| loss: 41.06176| val_0_mse: 329.80604|  0:00:15s\n",
      "epoch 967| loss: 124.0584| val_0_mse: 341.41921|  0:00:15s\n",
      "epoch 968| loss: 60.15024| val_0_mse: 365.51661|  0:00:15s\n",
      "epoch 969| loss: 48.7789 | val_0_mse: 351.13749|  0:00:15s\n",
      "epoch 970| loss: 75.41649| val_0_mse: 257.62454|  0:00:15s\n",
      "epoch 971| loss: 203.91563| val_0_mse: 190.39552|  0:00:15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 972| loss: 61.55759| val_0_mse: 210.95392|  0:00:15s\n",
      "epoch 973| loss: 133.28471| val_0_mse: 204.78565|  0:00:15s\n",
      "epoch 974| loss: 142.58722| val_0_mse: 458.09335|  0:00:15s\n",
      "epoch 975| loss: 167.76503| val_0_mse: 1069.862|  0:00:15s\n",
      "epoch 976| loss: 97.82956| val_0_mse: 1369.68831|  0:00:15s\n",
      "epoch 977| loss: 126.17056| val_0_mse: 1329.89429|  0:00:15s\n",
      "epoch 978| loss: 76.21614| val_0_mse: 1034.83587|  0:00:15s\n",
      "epoch 979| loss: 71.29897| val_0_mse: 633.46068|  0:00:15s\n",
      "epoch 980| loss: 64.23603| val_0_mse: 345.47401|  0:00:15s\n",
      "epoch 981| loss: 121.659 | val_0_mse: 268.06558|  0:00:15s\n",
      "epoch 982| loss: 103.94711| val_0_mse: 207.03843|  0:00:15s\n",
      "epoch 983| loss: 85.59187| val_0_mse: 258.25829|  0:00:15s\n",
      "epoch 984| loss: 102.08182| val_0_mse: 237.73978|  0:00:15s\n",
      "epoch 985| loss: 67.83831| val_0_mse: 194.42975|  0:00:15s\n",
      "epoch 986| loss: 201.62378| val_0_mse: 164.01925|  0:00:15s\n",
      "epoch 987| loss: 72.30637| val_0_mse: 169.08576|  0:00:15s\n",
      "epoch 988| loss: 95.4477 | val_0_mse: 171.36093|  0:00:15s\n",
      "epoch 989| loss: 38.02837| val_0_mse: 193.52411|  0:00:15s\n",
      "epoch 990| loss: 68.4511 | val_0_mse: 230.0196|  0:00:15s\n",
      "epoch 991| loss: 71.99135| val_0_mse: 239.36479|  0:00:15s\n",
      "epoch 992| loss: 116.82769| val_0_mse: 204.43526|  0:00:15s\n",
      "epoch 993| loss: 88.31082| val_0_mse: 174.9524|  0:00:15s\n",
      "epoch 994| loss: 78.20216| val_0_mse: 195.53926|  0:00:15s\n",
      "epoch 995| loss: 91.12894| val_0_mse: 225.16878|  0:00:15s\n",
      "epoch 996| loss: 57.76677| val_0_mse: 262.6563|  0:00:15s\n",
      "\n",
      "Early stopping occurred at epoch 996 with best_epoch = 696 and best_val_0_mse = 134.52989\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "n_steps = 2\n",
    "\n",
    "regressor = TabNetRegressor(verbose=1, seed=42, n_steps=n_steps)\n",
    "regressor.fit(X_train=X_train, y_train=Y_train,\n",
    "             eval_set=[(X_val, Y_val)],\n",
    "             patience=300, max_epochs=2000,\n",
    "             eval_metric=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u9YR1SoBhh4"
   },
   "source": [
    "### 2.4 TabNet 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkDcUnIpnH9f"
   },
   "source": [
    "실제 값에 보다 가까워진 예측값을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OdhY-iI9giFR",
    "outputId": "80436073-5a9a-4619-9207-d302c1a22c9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 :  [169.44333]\n",
      "정답 :  [171]\n"
     ]
    }
   ],
   "source": [
    "preds = regressor.predict(X_val)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BjvuXc3nMfP"
   },
   "source": [
    "r2 score도 MLP 베이스 모델에 비해 높은 성능을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWFsWlz6giCs",
    "outputId": "3a18f3a7-b21f-4899-8ce8-d761a7689b4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925097014981012\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6lMay1LHj8H"
   },
   "source": [
    "학습된 mask를 시각화하여 각 test case를 regression하며 어떤 feature가 중요하게 여겨졌는지 확인합니다. n_steps를 조정하면 생성되는 mask 수를 조절할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "16e6HjBHF8-s"
   },
   "outputs": [],
   "source": [
    "explain_matrix, masks = regressor.explain(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wKEqq6-qFz9i",
    "outputId": "9d0d3baa-7a9a-4ecb-9ea5-8a1154f88329"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, n_steps, figsize=(20,20))\n",
    "\n",
    "for i in range(n_steps):\n",
    "    axs[i].imshow(masks[i][:50])\n",
    "    axs[i].set_title(f\"mask {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J93fskyLnrXS"
   },
   "source": [
    "## 3. 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvCbAT5cnwx5"
   },
   "source": [
    "### 3.1 MLP 모델 구조를 수정하거나, 오버피팅 방지 기법을 모두 적용하며 성능을 최대한 끌어올려 봅니다.\n",
    "  평가 지표는 'R2 score'입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbQat-fFoBFM"
   },
   "source": [
    "- epochs\n",
    "- batch size\n",
    "- dropout\n",
    "- weight restriction\n",
    "- weight initialization\n",
    "- early stopping\n",
    "- 그 외 모델 변형 및 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0-ocSYkPR4H",
    "outputId": "67a423c5-ce5e-426b-d833-afc86856e3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "Viscosity          6.080000\n",
      "Velocity           6.537778\n",
      "PrintingSpeed    129.777778\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "Viscosity           3.493571\n",
      "Velocity            1.356825\n",
      "PrintingSpeed    4728.745040\n",
      "dtype: float64\n",
      "\n",
      "=========== 표준화 ==============\n",
      "\n",
      "(Train) feature 들의 평균 값\n",
      "Viscosity       -1.973730e-16\n",
      "Velocity        -1.973730e-17\n",
      "PrintingSpeed   -1.875043e-16\n",
      "dtype: float64\n",
      "\n",
      "(Train) feature 들의 분산 값\n",
      "Viscosity        1.005587\n",
      "Velocity         1.005587\n",
      "PrintingSpeed    1.005587\n",
      "dtype: float64\n",
      "X_train_shape :  (180, 3)\n",
      "X_val_shape :  (45, 3)\n",
      "Y_train_shape :  (180,)\n",
      "Y_val_shape : (45,)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('InkjetDB_preprocessing.csv')\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.head()\n",
    "X = df.loc[:, 'Viscosity': 'PrintingSpeed']\n",
    "Y = df['PatternSize']\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 원래 데이터들의 평균과 분산 확인 \n",
    "print('feature 들의 평균 값')\n",
    "print(X.mean())\n",
    "print('\\nfeature 들의 분산 값')\n",
    "print(X.var())\n",
    "\n",
    "X_train = X.loc[:int(len(X)*0.8)-1]\n",
    "X_val = X.loc[int(len(X)*0.8):]\n",
    "Y_train = Y.loc[:int(len(Y)*0.8)-1]\n",
    "Y_val = Y.loc[int(len(Y)*0.8):]\n",
    "print(\"\\n=========== 표준화 ==============\\n\")\n",
    "\n",
    "# StandardScaler객체 생성\n",
    "scaler = StandardScaler()\n",
    "# StandardScaler 로 데이터 셋 변환. fit( ) 과 transform( ) 호출.  \n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled=scaler.transform(X_val)\n",
    "\n",
    "#transform( )시 scale 변환된 데이터 셋이 numpy ndarry로 반환되어 이를 DataFrame으로 변환\n",
    "X_df_scaled = pd.DataFrame(data=X_train_scaled, columns=X.columns)\n",
    "print('(Train) feature 들의 평균 값')\n",
    "print(X_df_scaled.mean())\n",
    "print('\\n(Train) feature 들의 분산 값')\n",
    "print(X_df_scaled.var())\n",
    "\n",
    "X_train=tf.convert_to_tensor(X_train_scaled)\n",
    "X_val=tf.convert_to_tensor(X_val_scaled)\n",
    "Y_train=tf.convert_to_tensor(Y_train)\n",
    "Y_val=tf.convert_to_tensor(Y_val)\n",
    "\n",
    "\n",
    "print('X_train_shape : ', X_train.shape)\n",
    "print('X_val_shape : ', X_val.shape)\n",
    "print('Y_train_shape : ', Y_train.shape)\n",
    "print('Y_val_shape :', Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "0yQ1FuP5Nr7N"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEG6XidVor6u",
    "outputId": "104e89bc-c16c-408b-f0c2-a89f8440feee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(lr = learning_rate)\n",
    "loss = MeanSquaredError()\n",
    "epochs = 1000\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ZSqvve9zntKK"
   },
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor='val_loss',  # 모니터 기준 설정 (val loss) \n",
    "                patience=100,               # _____회 Epoch동안 개선되지 않는다면 종료\n",
    "                )\n",
    "\n",
    "ckpt_name = 'checkpoint-epoch-{}-batch-{}-trial-001.h5'.format(epochs, batch_size)\n",
    "checkpoint = ModelCheckpoint(ckpt_name,             # file명을 지정합니다\n",
    "                            monitor='val_loss',     # val_loss 값이 개선되었을때 호출됩니다\n",
    "                            verbose=1,              # 로그를 출력합니다\n",
    "                            save_best_only=True,    # 가장 best 값만 저장합니다\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_layer = tf.keras.layers.Dropout(0.2, input_shape=(2,)) # 0.2는 dropout이 걸릴 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_custom = tf.keras.Sequential()\n",
    "\n",
    "model_custom.add(tf.keras.Input(shape = 3))\n",
    "model_custom.add(Dense(300, kernel_constraint=max_norm(2.0), activation='relu', kernel_initializer='he_normal'))\n",
    "model_custom.add(dropout_layer) # dropout을 추가할 layer 입니다.\n",
    "model_custom.add(Dense(200, kernel_constraint=max_norm(2.0), activation='relu', kernel_initializer='he_normal'))\n",
    "model_custom.add(dropout_layer) # dropout을 추가할 layer 입니다.\n",
    "model_custom.add(Dense(150, kernel_constraint=max_norm(2.0), activation='relu', kernel_initializer='he_normal'))\n",
    "model_custom.add(dropout_layer) # dropout을 추가할 layer 입니다.\n",
    "model_custom.add(Dense(100, kernel_constraint=max_norm(2.0), activation='relu', kernel_initializer='he_normal'))\n",
    "model_custom.add(dropout_layer)\n",
    "model_custom.add(Dense(50, kernel_constraint=max_norm(2.0), activation='relu', kernel_initializer='he_normal'))\n",
    "model_custom.add(dropout_layer)\n",
    "model_custom.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzSyfCA3oS_i",
    "outputId": "2cfc4e2b-083b-40ae-b79d-e921c31adcb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 300)               1200      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 150)               30150     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 100)               15100     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111,751\n",
      "Trainable params: 111,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_custom.compile(loss=loss, optimizer=optimizer, metrics=['mse', 'mae'])\n",
    "model_custom.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/6 [====>.........................] - ETA: 4s - loss: 20817.4043 - mse: 20817.4043 - mae: 110.5926\n",
      "Epoch 1: val_loss improved from inf to 31366.75586, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 1s 58ms/step - loss: 22382.2188 - mse: 22382.2188 - mae: 99.0758 - val_loss: 31366.7559 - val_mse: 31366.7559 - val_mae: 117.8805\n",
      "Epoch 2/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 21721.0156 - mse: 21721.0156 - mae: 98.0977\n",
      "Epoch 2: val_loss improved from 31366.75586 to 25655.87695, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 19385.1602 - mse: 19385.1602 - mae: 87.8143 - val_loss: 25655.8770 - val_mse: 25655.8809 - val_mae: 99.9403\n",
      "Epoch 3/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 14136.9316 - mse: 14136.9316 - mae: 64.1915\n",
      "Epoch 3: val_loss improved from 25655.87695 to 16230.37891, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 14557.1748 - mse: 14557.1748 - mae: 71.8314 - val_loss: 16230.3789 - val_mse: 16230.3789 - val_mae: 68.7866\n",
      "Epoch 4/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 9341.2480 - mse: 9341.2480 - mae: 64.0060\n",
      "Epoch 4: val_loss improved from 16230.37891 to 7972.48730, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 8313.8271 - mse: 8313.8271 - mae: 62.0695 - val_loss: 7972.4873 - val_mse: 7972.4873 - val_mae: 56.6508\n",
      "Epoch 5/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4339.3179 - mse: 4339.3179 - mae: 51.0164\n",
      "Epoch 5: val_loss improved from 7972.48730 to 5412.59326, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 4924.5439 - mse: 4924.5439 - mae: 52.0875 - val_loss: 5412.5933 - val_mse: 5412.5933 - val_mae: 41.4617\n",
      "Epoch 6/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2774.0786 - mse: 2774.0786 - mae: 38.7131\n",
      "Epoch 6: val_loss improved from 5412.59326 to 5007.93408, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 4015.4036 - mse: 4015.4036 - mae: 38.8969 - val_loss: 5007.9341 - val_mse: 5007.9341 - val_mae: 35.4615\n",
      "Epoch 7/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 5186.4990 - mse: 5186.4990 - mae: 39.7981\n",
      "Epoch 7: val_loss improved from 5007.93408 to 3723.53638, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 3704.9949 - mse: 3704.9949 - mae: 32.9454 - val_loss: 3723.5364 - val_mse: 3723.5364 - val_mae: 31.6549\n",
      "Epoch 8/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1147.6416 - mse: 1147.6416 - mae: 27.0238\n",
      "Epoch 8: val_loss improved from 3723.53638 to 2876.67944, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2336.5586 - mse: 2336.5586 - mae: 32.1078 - val_loss: 2876.6794 - val_mse: 2876.6794 - val_mae: 32.3502\n",
      "Epoch 9/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2948.5068 - mse: 2948.5068 - mae: 35.2054\n",
      "Epoch 9: val_loss improved from 2876.67944 to 2630.77222, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 2626.4932 - mse: 2626.4932 - mae: 33.8740 - val_loss: 2630.7722 - val_mse: 2630.7722 - val_mae: 30.8520\n",
      "Epoch 10/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4164.7227 - mse: 4164.7227 - mae: 36.5536\n",
      "Epoch 10: val_loss did not improve from 2630.77222\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2238.9937 - mse: 2238.9937 - mae: 31.8707 - val_loss: 3375.5117 - val_mse: 3375.5117 - val_mae: 30.0224\n",
      "Epoch 11/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 841.8111 - mse: 841.8111 - mae: 24.0168\n",
      "Epoch 11: val_loss did not improve from 2630.77222\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2070.6931 - mse: 2070.6931 - mae: 29.8901 - val_loss: 2813.9727 - val_mse: 2813.9727 - val_mae: 29.0559\n",
      "Epoch 12/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1280.5558 - mse: 1280.5558 - mae: 26.1023\n",
      "Epoch 12: val_loss improved from 2630.77222 to 2444.60132, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 2201.3579 - mse: 2201.3579 - mae: 31.6231 - val_loss: 2444.6013 - val_mse: 2444.6013 - val_mae: 28.5987\n",
      "Epoch 13/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2404.9844 - mse: 2404.9844 - mae: 36.1630\n",
      "Epoch 13: val_loss did not improve from 2444.60132\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2261.7332 - mse: 2261.7332 - mae: 30.0697 - val_loss: 3457.3447 - val_mse: 3457.3447 - val_mae: 30.5560\n",
      "Epoch 14/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 526.8974 - mse: 526.8974 - mae: 19.1420\n",
      "Epoch 14: val_loss did not improve from 2444.60132\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2086.4766 - mse: 2086.4766 - mae: 27.3614 - val_loss: 2519.7412 - val_mse: 2519.7412 - val_mae: 25.1380\n",
      "Epoch 15/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 796.6747 - mse: 796.6747 - mae: 21.4101\n",
      "Epoch 15: val_loss improved from 2444.60132 to 2142.63159, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1826.2040 - mse: 1826.2040 - mae: 28.7690 - val_loss: 2142.6316 - val_mse: 2142.6316 - val_mae: 25.5377\n",
      "Epoch 16/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3155.4980 - mse: 3155.4980 - mae: 35.5809\n",
      "Epoch 16: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2263.0291 - mse: 2263.0291 - mae: 28.1847 - val_loss: 3980.1406 - val_mse: 3980.1406 - val_mae: 34.6457\n",
      "Epoch 17/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2227.6672 - mse: 2227.6672 - mae: 30.2795\n",
      "Epoch 17: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2432.8853 - mse: 2432.8853 - mae: 29.8740 - val_loss: 3501.9104 - val_mse: 3501.9104 - val_mae: 31.6888\n",
      "Epoch 18/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1784.1068 - mse: 1784.1068 - mae: 31.0634\n",
      "Epoch 18: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2426.0339 - mse: 2426.0339 - mae: 27.9309 - val_loss: 2637.0273 - val_mse: 2637.0273 - val_mae: 25.5326\n",
      "Epoch 19/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1409.5967 - mse: 1409.5967 - mae: 24.2304\n",
      "Epoch 19: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2312.5063 - mse: 2312.5063 - mae: 27.4583 - val_loss: 2331.8750 - val_mse: 2331.8750 - val_mae: 23.9312\n",
      "Epoch 20/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2531.1597 - mse: 2531.1597 - mae: 32.5354\n",
      "Epoch 20: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1734.7723 - mse: 1734.7723 - mae: 26.4417 - val_loss: 2320.0159 - val_mse: 2320.0159 - val_mae: 23.9731\n",
      "Epoch 21/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1432.7189 - mse: 1432.7189 - mae: 25.7916\n",
      "Epoch 21: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1111.9397 - mse: 1111.9397 - mae: 22.4024 - val_loss: 3279.0295 - val_mse: 3279.0295 - val_mae: 29.2591\n",
      "Epoch 22/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 914.7872 - mse: 914.7872 - mae: 20.4992\n",
      "Epoch 22: val_loss did not improve from 2142.63159\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2057.1750 - mse: 2057.1750 - mae: 26.0997 - val_loss: 2514.2168 - val_mse: 2514.2168 - val_mae: 23.3428\n",
      "Epoch 23/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2932.1580 - mse: 2932.1580 - mae: 32.1923\n",
      "Epoch 23: val_loss improved from 2142.63159 to 1629.40613, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 2605.9758 - mse: 2605.9758 - mae: 29.4154 - val_loss: 1629.4061 - val_mse: 1629.4061 - val_mae: 23.9617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1724.8049 - mse: 1724.8049 - mae: 28.2804\n",
      "Epoch 24: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1584.3309 - mse: 1584.3309 - mae: 25.5556 - val_loss: 3192.9785 - val_mse: 3192.9785 - val_mae: 30.1058\n",
      "Epoch 25/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2005.6135 - mse: 2005.6135 - mae: 29.6991\n",
      "Epoch 25: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1802.2042 - mse: 1802.2042 - mae: 26.0690 - val_loss: 2807.7773 - val_mse: 2807.7773 - val_mae: 27.9569\n",
      "Epoch 26/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2970.8835 - mse: 2970.8835 - mae: 32.8341\n",
      "Epoch 26: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1798.1768 - mse: 1798.1768 - mae: 25.9990 - val_loss: 1849.1012 - val_mse: 1849.1012 - val_mae: 22.1446\n",
      "Epoch 27/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1472.6158 - mse: 1472.6158 - mae: 27.8151\n",
      "Epoch 27: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1909.1722 - mse: 1909.1722 - mae: 27.3679 - val_loss: 2595.1323 - val_mse: 2595.1323 - val_mae: 26.0329\n",
      "Epoch 28/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3755.9104 - mse: 3755.9104 - mae: 30.9354\n",
      "Epoch 28: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1527.2755 - mse: 1527.2755 - mae: 24.0801 - val_loss: 2368.1116 - val_mse: 2368.1116 - val_mae: 24.6183\n",
      "Epoch 29/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 982.6558 - mse: 982.6558 - mae: 22.2409\n",
      "Epoch 29: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1123.7546 - mse: 1123.7546 - mae: 22.2851 - val_loss: 2038.5403 - val_mse: 2038.5403 - val_mae: 22.6999\n",
      "Epoch 30/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1280.0110 - mse: 1280.0110 - mae: 23.0091\n",
      "Epoch 30: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1276.4469 - mse: 1276.4469 - mae: 23.0302 - val_loss: 2426.6799 - val_mse: 2426.6799 - val_mae: 25.0394\n",
      "Epoch 31/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 978.7690 - mse: 978.7690 - mae: 22.1649\n",
      "Epoch 31: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1807.8776 - mse: 1807.8776 - mae: 25.8508 - val_loss: 3286.4221 - val_mse: 3286.4221 - val_mae: 30.2722\n",
      "Epoch 32/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3850.8738 - mse: 3850.8738 - mae: 38.8567\n",
      "Epoch 32: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1773.7454 - mse: 1773.7454 - mae: 25.7986 - val_loss: 1831.2012 - val_mse: 1831.2012 - val_mae: 21.9594\n",
      "Epoch 33/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1318.8981 - mse: 1318.8981 - mae: 23.0360\n",
      "Epoch 33: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1610.0410 - mse: 1610.0410 - mae: 25.3362 - val_loss: 1829.9354 - val_mse: 1829.9354 - val_mae: 22.4739\n",
      "Epoch 34/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 622.0243 - mse: 622.0243 - mae: 18.4157\n",
      "Epoch 34: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1029.9493 - mse: 1029.9493 - mae: 20.3053 - val_loss: 3111.7388 - val_mse: 3111.7388 - val_mae: 29.0500\n",
      "Epoch 35/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1393.7787 - mse: 1393.7787 - mae: 24.8913\n",
      "Epoch 35: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1775.0477 - mse: 1775.0477 - mae: 25.6344 - val_loss: 2895.7075 - val_mse: 2895.7075 - val_mae: 25.7736\n",
      "Epoch 36/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1511.4385 - mse: 1511.4385 - mae: 27.7548\n",
      "Epoch 36: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1717.4420 - mse: 1717.4420 - mae: 25.2821 - val_loss: 1654.7496 - val_mse: 1654.7495 - val_mae: 22.8854\n",
      "Epoch 37/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2262.8279 - mse: 2262.8279 - mae: 33.0871\n",
      "Epoch 37: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1530.3354 - mse: 1530.3354 - mae: 26.7335 - val_loss: 2008.1014 - val_mse: 2008.1014 - val_mae: 23.7145\n",
      "Epoch 38/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1479.8380 - mse: 1479.8380 - mae: 23.7399\n",
      "Epoch 38: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1638.9849 - mse: 1638.9849 - mae: 25.9047 - val_loss: 3218.1101 - val_mse: 3218.1101 - val_mae: 32.0880\n",
      "Epoch 39/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1413.5148 - mse: 1413.5148 - mae: 25.5388\n",
      "Epoch 39: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1498.7524 - mse: 1498.7524 - mae: 23.8599 - val_loss: 2288.2676 - val_mse: 2288.2676 - val_mae: 24.0564\n",
      "Epoch 40/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 809.2275 - mse: 809.2275 - mae: 19.8897\n",
      "Epoch 40: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2090.0391 - mse: 2090.0391 - mae: 26.6333 - val_loss: 2210.6201 - val_mse: 2210.6201 - val_mae: 22.7598\n",
      "Epoch 41/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2094.6250 - mse: 2094.6250 - mae: 27.0392\n",
      "Epoch 41: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2820.0840 - mse: 2820.0840 - mae: 28.2470 - val_loss: 2614.0508 - val_mse: 2614.0508 - val_mae: 24.7815\n",
      "Epoch 42/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 645.3337 - mse: 645.3337 - mae: 18.9934\n",
      "Epoch 42: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1325.3374 - mse: 1325.3374 - mae: 22.7561 - val_loss: 1792.3717 - val_mse: 1792.3717 - val_mae: 21.6409\n",
      "Epoch 43/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 970.3973 - mse: 970.3973 - mae: 22.6054\n",
      "Epoch 43: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1616.4631 - mse: 1616.4631 - mae: 23.9801 - val_loss: 2715.4709 - val_mse: 2715.4709 - val_mae: 25.7382\n",
      "Epoch 44/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2395.6572 - mse: 2395.6572 - mae: 26.4887\n",
      "Epoch 44: val_loss did not improve from 1629.40613\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1532.2015 - mse: 1532.2015 - mae: 22.7138 - val_loss: 2676.3218 - val_mse: 2676.3218 - val_mae: 24.8986\n",
      "Epoch 45/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3024.0146 - mse: 3024.0146 - mae: 28.3334\n",
      "Epoch 45: val_loss improved from 1629.40613 to 1605.03162, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 2008.0281 - mse: 2008.0281 - mae: 27.3392 - val_loss: 1605.0316 - val_mse: 1605.0316 - val_mae: 22.6692\n",
      "Epoch 46/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1348.3096 - mse: 1348.3096 - mae: 25.8545\n",
      "Epoch 46: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1662.8809 - mse: 1662.8809 - mae: 24.6848 - val_loss: 2402.3550 - val_mse: 2402.3550 - val_mae: 25.3140\n",
      "Epoch 47/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3894.9644 - mse: 3894.9644 - mae: 29.4043\n",
      "Epoch 47: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1477.9120 - mse: 1477.9120 - mae: 22.5107 - val_loss: 2659.8350 - val_mse: 2659.8350 - val_mae: 27.5752\n",
      "Epoch 48/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1256.5754 - mse: 1256.5754 - mae: 24.5592\n",
      "Epoch 48: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1434.4250 - mse: 1434.4250 - mae: 23.3699 - val_loss: 1917.8455 - val_mse: 1917.8455 - val_mae: 22.2505\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 738.3123 - mse: 738.3123 - mae: 17.7468\n",
      "Epoch 49: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1932.8751 - mse: 1932.8751 - mae: 26.0490 - val_loss: 2080.1169 - val_mse: 2080.1169 - val_mae: 22.8414\n",
      "Epoch 50/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 940.5175 - mse: 940.5175 - mae: 21.9734\n",
      "Epoch 50: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1429.4000 - mse: 1429.4000 - mae: 25.1427 - val_loss: 2232.3279 - val_mse: 2232.3279 - val_mae: 24.1879\n",
      "Epoch 51/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 458.8578 - mse: 458.8578 - mae: 15.2436\n",
      "Epoch 51: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1928.5675 - mse: 1928.5675 - mae: 26.4221 - val_loss: 1785.2063 - val_mse: 1785.2063 - val_mae: 21.8848\n",
      "Epoch 52/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1179.1312 - mse: 1179.1312 - mae: 21.7553\n",
      "Epoch 52: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1489.7245 - mse: 1489.7245 - mae: 24.3748 - val_loss: 2339.5090 - val_mse: 2339.5090 - val_mae: 24.6066\n",
      "Epoch 53/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1278.0132 - mse: 1278.0132 - mae: 23.6541\n",
      "Epoch 53: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1216.3009 - mse: 1216.3009 - mae: 21.9071 - val_loss: 2487.6040 - val_mse: 2487.6040 - val_mae: 24.9616\n",
      "Epoch 54/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 304.6329 - mse: 304.6329 - mae: 13.8733\n",
      "Epoch 54: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1799.5917 - mse: 1799.5917 - mae: 23.9749 - val_loss: 2831.7942 - val_mse: 2831.7942 - val_mae: 25.7727\n",
      "Epoch 55/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1891.1357 - mse: 1891.1357 - mae: 23.9275\n",
      "Epoch 55: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1717.0873 - mse: 1717.0873 - mae: 24.0459 - val_loss: 1902.3223 - val_mse: 1902.3223 - val_mae: 21.8461\n",
      "Epoch 56/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1075.3973 - mse: 1075.3973 - mae: 22.1219\n",
      "Epoch 56: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1362.5145 - mse: 1362.5145 - mae: 23.6051 - val_loss: 2547.7407 - val_mse: 2547.7407 - val_mae: 25.7812\n",
      "Epoch 57/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 730.2175 - mse: 730.2175 - mae: 18.4889\n",
      "Epoch 57: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1417.3101 - mse: 1417.3101 - mae: 22.3595 - val_loss: 1651.9884 - val_mse: 1651.9884 - val_mae: 21.0101\n",
      "Epoch 58/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1050.6042 - mse: 1050.6042 - mae: 24.4558\n",
      "Epoch 58: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1918.6270 - mse: 1918.6270 - mae: 25.5649 - val_loss: 2522.1719 - val_mse: 2522.1719 - val_mae: 25.6986\n",
      "Epoch 59/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 391.0687 - mse: 391.0687 - mae: 16.8378\n",
      "Epoch 59: val_loss did not improve from 1605.03162\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1610.9230 - mse: 1610.9230 - mae: 23.3731 - val_loss: 2186.8267 - val_mse: 2186.8267 - val_mae: 23.0822\n",
      "Epoch 60/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3231.2197 - mse: 3231.2197 - mae: 25.8117\n",
      "Epoch 60: val_loss improved from 1605.03162 to 1462.18420, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1517.3931 - mse: 1517.3931 - mae: 22.5730 - val_loss: 1462.1842 - val_mse: 1462.1842 - val_mae: 21.2224\n",
      "Epoch 61/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2486.4531 - mse: 2486.4531 - mae: 34.0772\n",
      "Epoch 61: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1372.3845 - mse: 1372.3845 - mae: 23.8194 - val_loss: 3411.1396 - val_mse: 3411.1396 - val_mae: 32.1988\n",
      "Epoch 62/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 714.9114 - mse: 714.9114 - mae: 19.6779\n",
      "Epoch 62: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1857.4321 - mse: 1857.4321 - mae: 25.3211 - val_loss: 3090.1338 - val_mse: 3090.1338 - val_mae: 27.9060\n",
      "Epoch 63/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 440.6790 - mse: 440.6790 - mae: 14.4658\n",
      "Epoch 63: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1166.3700 - mse: 1166.3700 - mae: 22.9460 - val_loss: 1588.8469 - val_mse: 1588.8469 - val_mae: 20.8485\n",
      "Epoch 64/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2629.0571 - mse: 2629.0571 - mae: 30.1983\n",
      "Epoch 64: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1703.9165 - mse: 1703.9165 - mae: 25.2994 - val_loss: 2241.5352 - val_mse: 2241.5352 - val_mae: 23.6134\n",
      "Epoch 65/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4632.1689 - mse: 4632.1689 - mae: 26.2718\n",
      "Epoch 65: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1788.2988 - mse: 1788.2988 - mae: 23.0382 - val_loss: 2544.2654 - val_mse: 2544.2654 - val_mae: 26.6436\n",
      "Epoch 66/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1325.1434 - mse: 1325.1434 - mae: 23.8894\n",
      "Epoch 66: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1281.5363 - mse: 1281.5363 - mae: 22.0930 - val_loss: 1853.6942 - val_mse: 1853.6942 - val_mae: 22.9404\n",
      "Epoch 67/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1443.5858 - mse: 1443.5858 - mae: 24.8154\n",
      "Epoch 67: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2030.7339 - mse: 2030.7339 - mae: 26.2094 - val_loss: 1645.9358 - val_mse: 1645.9358 - val_mae: 21.7662\n",
      "Epoch 68/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2555.4209 - mse: 2555.4209 - mae: 28.8922\n",
      "Epoch 68: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1538.3009 - mse: 1538.3009 - mae: 22.6693 - val_loss: 2137.4805 - val_mse: 2137.4805 - val_mae: 25.2775\n",
      "Epoch 69/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1197.5848 - mse: 1197.5848 - mae: 22.4618\n",
      "Epoch 69: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1673.9479 - mse: 1673.9479 - mae: 24.6443 - val_loss: 2655.7170 - val_mse: 2655.7170 - val_mae: 28.3633\n",
      "Epoch 70/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 755.4650 - mse: 755.4650 - mae: 17.2524\n",
      "Epoch 70: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1624.4277 - mse: 1624.4277 - mae: 23.3300 - val_loss: 1888.0403 - val_mse: 1888.0403 - val_mae: 22.3866\n",
      "Epoch 71/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 421.5923 - mse: 421.5923 - mae: 16.0706\n",
      "Epoch 71: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1379.6290 - mse: 1379.6290 - mae: 22.9095 - val_loss: 1717.5135 - val_mse: 1717.5135 - val_mae: 21.4353\n",
      "Epoch 72/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 520.2777 - mse: 520.2777 - mae: 18.4482\n",
      "Epoch 72: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1602.4861 - mse: 1602.4861 - mae: 23.4847 - val_loss: 2450.1116 - val_mse: 2450.1116 - val_mae: 26.6391\n",
      "Epoch 73/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1120.4248 - mse: 1120.4248 - mae: 24.9849\n",
      "Epoch 73: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1102.2267 - mse: 1102.2267 - mae: 21.7032 - val_loss: 2558.1943 - val_mse: 2558.1943 - val_mae: 27.2177\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 322.7011 - mse: 322.7011 - mae: 13.9053\n",
      "Epoch 74: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1575.2850 - mse: 1575.2850 - mae: 22.9967 - val_loss: 1738.0021 - val_mse: 1738.0021 - val_mae: 21.0952\n",
      "Epoch 75/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1952.5208 - mse: 1952.5208 - mae: 30.2608\n",
      "Epoch 75: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1451.2339 - mse: 1451.2339 - mae: 22.9661 - val_loss: 1539.6038 - val_mse: 1539.6038 - val_mae: 20.9477\n",
      "Epoch 76/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2036.5170 - mse: 2036.5170 - mae: 25.1290\n",
      "Epoch 76: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1572.1342 - mse: 1572.1342 - mae: 22.1860 - val_loss: 1833.5332 - val_mse: 1833.5332 - val_mae: 22.8478\n",
      "Epoch 77/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1056.8556 - mse: 1056.8556 - mae: 19.6422\n",
      "Epoch 77: val_loss did not improve from 1462.18420\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1471.9548 - mse: 1471.9548 - mae: 23.0278 - val_loss: 2101.8225 - val_mse: 2101.8225 - val_mae: 23.6980\n",
      "Epoch 78/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2117.4177 - mse: 2117.4177 - mae: 26.5670\n",
      "Epoch 78: val_loss improved from 1462.18420 to 1461.64136, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1598.2186 - mse: 1598.2186 - mae: 24.8190 - val_loss: 1461.6414 - val_mse: 1461.6414 - val_mae: 21.0002\n",
      "Epoch 79/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1678.6565 - mse: 1678.6565 - mae: 30.2266\n",
      "Epoch 79: val_loss did not improve from 1461.64136\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1154.3555 - mse: 1154.3555 - mae: 22.6147 - val_loss: 2843.8740 - val_mse: 2843.8740 - val_mae: 29.5364\n",
      "Epoch 80/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 366.0865 - mse: 366.0865 - mae: 13.4534\n",
      "Epoch 80: val_loss did not improve from 1461.64136\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1326.9159 - mse: 1326.9159 - mae: 23.3135 - val_loss: 2330.1646 - val_mse: 2330.1646 - val_mae: 25.1682\n",
      "Epoch 81/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1418.7408 - mse: 1418.7408 - mae: 27.7137\n",
      "Epoch 81: val_loss did not improve from 1461.64136\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1821.8376 - mse: 1821.8376 - mae: 25.1348 - val_loss: 1927.8092 - val_mse: 1927.8092 - val_mae: 21.2462\n",
      "Epoch 82/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1819.7889 - mse: 1819.7889 - mae: 28.1339\n",
      "Epoch 82: val_loss improved from 1461.64136 to 1443.30762, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1278.5040 - mse: 1278.5040 - mae: 23.0532 - val_loss: 1443.3076 - val_mse: 1443.3076 - val_mae: 20.4821\n",
      "Epoch 83/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1466.5883 - mse: 1466.5883 - mae: 23.5254\n",
      "Epoch 83: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1517.5173 - mse: 1517.5173 - mae: 22.8929 - val_loss: 1789.4467 - val_mse: 1789.4467 - val_mae: 24.1738\n",
      "Epoch 84/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 627.8674 - mse: 627.8674 - mae: 18.3640\n",
      "Epoch 84: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1524.1884 - mse: 1524.1884 - mae: 23.9384 - val_loss: 1814.8951 - val_mse: 1814.8950 - val_mae: 25.5506\n",
      "Epoch 85/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 792.5222 - mse: 792.5222 - mae: 21.2197\n",
      "Epoch 85: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1623.3278 - mse: 1623.3278 - mae: 24.2388 - val_loss: 2667.7185 - val_mse: 2667.7185 - val_mae: 29.2670\n",
      "Epoch 86/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 873.0228 - mse: 873.0228 - mae: 21.2145\n",
      "Epoch 86: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 993.8162 - mse: 993.8162 - mae: 20.8185 - val_loss: 2241.9646 - val_mse: 2241.9646 - val_mae: 24.3326\n",
      "Epoch 87/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1456.1843 - mse: 1456.1843 - mae: 26.0699\n",
      "Epoch 87: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1415.9506 - mse: 1415.9506 - mae: 22.6592 - val_loss: 1556.6029 - val_mse: 1556.6029 - val_mae: 19.6828\n",
      "Epoch 88/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1129.7192 - mse: 1129.7192 - mae: 21.3157\n",
      "Epoch 88: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1258.6802 - mse: 1258.6802 - mae: 21.8691 - val_loss: 1911.3309 - val_mse: 1911.3309 - val_mae: 23.0352\n",
      "Epoch 89/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2893.8162 - mse: 2893.8162 - mae: 29.3443\n",
      "Epoch 89: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1460.5912 - mse: 1460.5912 - mae: 23.0865 - val_loss: 3426.1750 - val_mse: 3426.1750 - val_mae: 32.2614\n",
      "Epoch 90/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3784.0334 - mse: 3784.0334 - mae: 38.1483\n",
      "Epoch 90: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1442.9523 - mse: 1442.9523 - mae: 24.1263 - val_loss: 1902.5167 - val_mse: 1902.5167 - val_mae: 20.8803\n",
      "Epoch 91/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 895.7648 - mse: 895.7648 - mae: 22.4988\n",
      "Epoch 91: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1293.1543 - mse: 1293.1543 - mae: 22.4242 - val_loss: 1705.0500 - val_mse: 1705.0500 - val_mae: 20.4092\n",
      "Epoch 92/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 744.4457 - mse: 744.4457 - mae: 21.6617\n",
      "Epoch 92: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1032.8655 - mse: 1032.8655 - mae: 21.0120 - val_loss: 1966.9811 - val_mse: 1966.9811 - val_mae: 24.5006\n",
      "Epoch 93/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 759.4497 - mse: 759.4497 - mae: 19.4042\n",
      "Epoch 93: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1472.8158 - mse: 1472.8158 - mae: 21.7481 - val_loss: 2173.4878 - val_mse: 2173.4878 - val_mae: 26.3415\n",
      "Epoch 94/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1752.9957 - mse: 1752.9957 - mae: 23.9157\n",
      "Epoch 94: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1033.2025 - mse: 1033.2025 - mae: 20.5346 - val_loss: 2337.7222 - val_mse: 2337.7222 - val_mae: 25.8672\n",
      "Epoch 95/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1540.9620 - mse: 1540.9620 - mae: 22.7466\n",
      "Epoch 95: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1449.4893 - mse: 1449.4893 - mae: 23.1323 - val_loss: 2739.7732 - val_mse: 2739.7732 - val_mae: 26.5726\n",
      "Epoch 96/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 492.1513 - mse: 492.1513 - mae: 14.8248\n",
      "Epoch 96: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1076.0842 - mse: 1076.0842 - mae: 20.7346 - val_loss: 1678.7648 - val_mse: 1678.7648 - val_mae: 20.0338\n",
      "Epoch 97/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 571.9244 - mse: 571.9244 - mae: 16.8852\n",
      "Epoch 97: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1988.9518 - mse: 1988.9518 - mae: 25.5960 - val_loss: 1807.4503 - val_mse: 1807.4503 - val_mae: 21.9837\n",
      "Epoch 98/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 490.1704 - mse: 490.1704 - mae: 16.2717\n",
      "Epoch 98: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1407.3671 - mse: 1407.3671 - mae: 22.1435 - val_loss: 2722.6316 - val_mse: 2722.6316 - val_mae: 27.4687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 585.3562 - mse: 585.3562 - mae: 17.9675\n",
      "Epoch 99: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1316.2942 - mse: 1316.2942 - mae: 21.6360 - val_loss: 2027.5059 - val_mse: 2027.5059 - val_mae: 22.0457\n",
      "Epoch 100/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 534.4846 - mse: 534.4846 - mae: 15.4572\n",
      "Epoch 100: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1119.7826 - mse: 1119.7826 - mae: 20.0833 - val_loss: 2431.1052 - val_mse: 2431.1052 - val_mae: 23.1809\n",
      "Epoch 101/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 500.4440 - mse: 500.4440 - mae: 15.9906\n",
      "Epoch 101: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 858.2317 - mse: 858.2317 - mae: 19.4550 - val_loss: 1818.0983 - val_mse: 1818.0983 - val_mae: 20.9337\n",
      "Epoch 102/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 924.5219 - mse: 924.5219 - mae: 20.5282\n",
      "Epoch 102: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1258.0677 - mse: 1258.0677 - mae: 22.5906 - val_loss: 2430.0774 - val_mse: 2430.0774 - val_mae: 26.6895\n",
      "Epoch 103/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 325.6624 - mse: 325.6624 - mae: 13.5252\n",
      "Epoch 103: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1187.8920 - mse: 1187.8920 - mae: 20.7164 - val_loss: 2214.9216 - val_mse: 2214.9216 - val_mae: 25.2658\n",
      "Epoch 104/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1262.5426 - mse: 1262.5426 - mae: 23.5131\n",
      "Epoch 104: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1605.1874 - mse: 1605.1874 - mae: 23.8428 - val_loss: 2113.4612 - val_mse: 2113.4612 - val_mae: 22.6155\n",
      "Epoch 105/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 898.2906 - mse: 898.2906 - mae: 20.6624\n",
      "Epoch 105: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1111.1284 - mse: 1111.1284 - mae: 21.7065 - val_loss: 2129.3169 - val_mse: 2129.3169 - val_mae: 22.7073\n",
      "Epoch 106/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2463.3943 - mse: 2463.3943 - mae: 30.3406\n",
      "Epoch 106: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1442.1477 - mse: 1442.1477 - mae: 23.8984 - val_loss: 2163.2896 - val_mse: 2163.2896 - val_mae: 25.1480\n",
      "Epoch 107/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 600.8741 - mse: 600.8741 - mae: 17.9071\n",
      "Epoch 107: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1700.2908 - mse: 1700.2908 - mae: 23.8546 - val_loss: 2032.9087 - val_mse: 2032.9087 - val_mae: 23.4796\n",
      "Epoch 108/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1863.3140 - mse: 1863.3140 - mae: 24.2258\n",
      "Epoch 108: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1314.1744 - mse: 1314.1744 - mae: 21.7068 - val_loss: 1777.5546 - val_mse: 1777.5546 - val_mae: 21.1450\n",
      "Epoch 109/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3144.9434 - mse: 3144.9434 - mae: 25.9769\n",
      "Epoch 109: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1853.0182 - mse: 1853.0182 - mae: 22.6641 - val_loss: 1582.5256 - val_mse: 1582.5256 - val_mae: 20.6121\n",
      "Epoch 110/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 319.9572 - mse: 319.9572 - mae: 12.1537\n",
      "Epoch 110: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 856.3582 - mse: 856.3582 - mae: 17.7625 - val_loss: 2234.0315 - val_mse: 2234.0315 - val_mae: 25.5061\n",
      "Epoch 111/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 861.7700 - mse: 861.7700 - mae: 17.9885\n",
      "Epoch 111: val_loss did not improve from 1443.30762\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1610.4473 - mse: 1610.4473 - mae: 22.3038 - val_loss: 2050.9910 - val_mse: 2050.9910 - val_mae: 23.5438\n",
      "Epoch 112/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 784.3322 - mse: 784.3322 - mae: 17.5797\n",
      "Epoch 112: val_loss improved from 1443.30762 to 1430.10474, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 1215.5190 - mse: 1215.5190 - mae: 21.2525 - val_loss: 1430.1047 - val_mse: 1430.1047 - val_mae: 20.8518\n",
      "Epoch 113/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 958.9479 - mse: 958.9479 - mae: 22.0891\n",
      "Epoch 113: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1207.8081 - mse: 1207.8081 - mae: 22.0076 - val_loss: 2762.2212 - val_mse: 2762.2212 - val_mae: 29.9981\n",
      "Epoch 114/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 995.5843 - mse: 995.5843 - mae: 20.5236\n",
      "Epoch 114: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1132.9374 - mse: 1132.9374 - mae: 21.8448 - val_loss: 2116.6807 - val_mse: 2116.6807 - val_mae: 23.2379\n",
      "Epoch 115/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2306.9753 - mse: 2306.9753 - mae: 25.7032\n",
      "Epoch 115: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1488.0151 - mse: 1488.0151 - mae: 22.3766 - val_loss: 1738.8036 - val_mse: 1738.8036 - val_mae: 19.9305\n",
      "Epoch 116/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1490.5082 - mse: 1490.5082 - mae: 27.2571\n",
      "Epoch 116: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1264.0852 - mse: 1264.0852 - mae: 22.0881 - val_loss: 2466.3147 - val_mse: 2466.3147 - val_mae: 28.4538\n",
      "Epoch 117/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 760.6960 - mse: 760.6960 - mae: 19.5247\n",
      "Epoch 117: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1389.9265 - mse: 1389.9265 - mae: 22.1465 - val_loss: 1925.9575 - val_mse: 1925.9575 - val_mae: 23.2869\n",
      "Epoch 118/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1091.6855 - mse: 1091.6855 - mae: 20.0026\n",
      "Epoch 118: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1303.9362 - mse: 1303.9362 - mae: 22.0076 - val_loss: 2131.4805 - val_mse: 2131.4805 - val_mae: 24.0624\n",
      "Epoch 119/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2686.0022 - mse: 2686.0022 - mae: 28.9377\n",
      "Epoch 119: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1418.1381 - mse: 1418.1381 - mae: 22.4774 - val_loss: 3430.7153 - val_mse: 3430.7153 - val_mae: 28.2603\n",
      "Epoch 120/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1797.0157 - mse: 1797.0157 - mae: 22.5096\n",
      "Epoch 120: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1676.4232 - mse: 1676.4232 - mae: 21.5163 - val_loss: 1639.0299 - val_mse: 1639.0299 - val_mae: 18.8825\n",
      "Epoch 121/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 877.9010 - mse: 877.9010 - mae: 19.2316\n",
      "Epoch 121: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1238.7826 - mse: 1238.7826 - mae: 21.7029 - val_loss: 2151.0767 - val_mse: 2151.0767 - val_mae: 25.2272\n",
      "Epoch 122/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1096.6311 - mse: 1096.6311 - mae: 20.6091\n",
      "Epoch 122: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1442.1719 - mse: 1442.1719 - mae: 22.6981 - val_loss: 2186.6992 - val_mse: 2186.6992 - val_mae: 27.5317\n",
      "Epoch 123/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 746.5405 - mse: 746.5405 - mae: 18.6181\n",
      "Epoch 123: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1148.3319 - mse: 1148.3319 - mae: 20.3716 - val_loss: 1605.7570 - val_mse: 1605.7570 - val_mae: 21.2563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2383.9158 - mse: 2383.9158 - mae: 27.3238\n",
      "Epoch 124: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1334.6750 - mse: 1334.6750 - mae: 21.1758 - val_loss: 2640.4724 - val_mse: 2640.4724 - val_mae: 25.4002\n",
      "Epoch 125/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1654.9941 - mse: 1654.9941 - mae: 23.6435\n",
      "Epoch 125: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1159.5482 - mse: 1159.5482 - mae: 20.5718 - val_loss: 2027.4774 - val_mse: 2027.4774 - val_mae: 20.8359\n",
      "Epoch 126/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3165.7009 - mse: 3165.7009 - mae: 26.9331\n",
      "Epoch 126: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1635.7991 - mse: 1635.7991 - mae: 23.7576 - val_loss: 1842.1505 - val_mse: 1842.1505 - val_mae: 20.8142\n",
      "Epoch 127/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 350.1896 - mse: 350.1896 - mae: 12.7791\n",
      "Epoch 127: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 970.2247 - mse: 970.2247 - mae: 19.7851 - val_loss: 2947.8982 - val_mse: 2947.8982 - val_mae: 28.6169\n",
      "Epoch 128/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1123.3673 - mse: 1123.3673 - mae: 22.3918\n",
      "Epoch 128: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1203.2490 - mse: 1203.2490 - mae: 19.1110 - val_loss: 2225.6121 - val_mse: 2225.6121 - val_mae: 21.4599\n",
      "Epoch 129/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1826.4427 - mse: 1826.4427 - mae: 23.5126\n",
      "Epoch 129: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1593.8638 - mse: 1593.8638 - mae: 21.4965 - val_loss: 1895.8187 - val_mse: 1895.8187 - val_mae: 18.9731\n",
      "Epoch 130/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2898.6628 - mse: 2898.6628 - mae: 29.6835\n",
      "Epoch 130: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1500.9335 - mse: 1500.9335 - mae: 23.1547 - val_loss: 1752.3232 - val_mse: 1752.3232 - val_mae: 21.8507\n",
      "Epoch 131/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 631.6566 - mse: 631.6566 - mae: 17.0276\n",
      "Epoch 131: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1481.5054 - mse: 1481.5054 - mae: 21.7919 - val_loss: 1700.8981 - val_mse: 1700.8981 - val_mae: 22.4928\n",
      "Epoch 132/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1250.0374 - mse: 1250.0374 - mae: 23.4390\n",
      "Epoch 132: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1044.2153 - mse: 1044.2153 - mae: 20.4111 - val_loss: 1929.6356 - val_mse: 1929.6356 - val_mae: 23.5757\n",
      "Epoch 133/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 941.5586 - mse: 941.5586 - mae: 20.3420\n",
      "Epoch 133: val_loss did not improve from 1430.10474\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1321.8926 - mse: 1321.8926 - mae: 20.1334 - val_loss: 1947.7889 - val_mse: 1947.7889 - val_mae: 23.4857\n",
      "Epoch 134/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 588.4694 - mse: 588.4694 - mae: 15.2122\n",
      "Epoch 134: val_loss improved from 1430.10474 to 1213.85571, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1147.9941 - mse: 1147.9941 - mae: 20.9209 - val_loss: 1213.8557 - val_mse: 1213.8557 - val_mae: 18.2601\n",
      "Epoch 135/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 738.7361 - mse: 738.7361 - mae: 19.9169\n",
      "Epoch 135: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1207.6578 - mse: 1207.6578 - mae: 21.2527 - val_loss: 2307.5532 - val_mse: 2307.5532 - val_mae: 25.4884\n",
      "Epoch 136/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 735.9250 - mse: 735.9250 - mae: 20.0813\n",
      "Epoch 136: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 808.5739 - mse: 808.5739 - mae: 18.1170 - val_loss: 1878.7991 - val_mse: 1878.7991 - val_mae: 19.7572\n",
      "Epoch 137/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2606.2527 - mse: 2606.2527 - mae: 26.1078\n",
      "Epoch 137: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1628.1389 - mse: 1628.1389 - mae: 21.7085 - val_loss: 1475.7217 - val_mse: 1475.7217 - val_mae: 18.2445\n",
      "Epoch 138/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 719.1819 - mse: 719.1819 - mae: 21.4531\n",
      "Epoch 138: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1510.7026 - mse: 1510.7026 - mae: 21.5255 - val_loss: 1724.7598 - val_mse: 1724.7598 - val_mae: 22.5557\n",
      "Epoch 139/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 930.9923 - mse: 930.9923 - mae: 19.8731\n",
      "Epoch 139: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 871.4206 - mse: 871.4206 - mae: 18.5919 - val_loss: 1720.3837 - val_mse: 1720.3837 - val_mae: 24.0967\n",
      "Epoch 140/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1944.1311 - mse: 1944.1311 - mae: 19.7509\n",
      "Epoch 140: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1526.4041 - mse: 1526.4041 - mae: 20.6417 - val_loss: 1830.5349 - val_mse: 1830.5349 - val_mae: 24.6647\n",
      "Epoch 141/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 577.2263 - mse: 577.2263 - mae: 14.3268\n",
      "Epoch 141: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1736.2924 - mse: 1736.2924 - mae: 22.1833 - val_loss: 2799.0400 - val_mse: 2799.0400 - val_mae: 28.8770\n",
      "Epoch 142/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 958.6474 - mse: 958.6474 - mae: 18.9110\n",
      "Epoch 142: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1330.8441 - mse: 1330.8441 - mae: 20.8142 - val_loss: 1274.4863 - val_mse: 1274.4863 - val_mae: 20.0970\n",
      "Epoch 143/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1164.2947 - mse: 1164.2947 - mae: 23.5580\n",
      "Epoch 143: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 878.2919 - mse: 878.2919 - mae: 19.0358 - val_loss: 1457.1406 - val_mse: 1457.1406 - val_mae: 21.9220\n",
      "Epoch 144/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2965.9565 - mse: 2965.9565 - mae: 24.6614\n",
      "Epoch 144: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1246.4359 - mse: 1246.4359 - mae: 19.8281 - val_loss: 1776.2739 - val_mse: 1776.2739 - val_mae: 24.8391\n",
      "Epoch 145/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1904.3733 - mse: 1904.3733 - mae: 23.0189\n",
      "Epoch 145: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1130.7706 - mse: 1130.7706 - mae: 19.0676 - val_loss: 1229.3058 - val_mse: 1229.3058 - val_mae: 18.1869\n",
      "Epoch 146/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 655.7913 - mse: 655.7913 - mae: 19.1243\n",
      "Epoch 146: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1422.7677 - mse: 1422.7677 - mae: 21.3708 - val_loss: 2217.8533 - val_mse: 2217.8533 - val_mae: 24.2509\n",
      "Epoch 147/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1083.0817 - mse: 1083.0817 - mae: 18.9520\n",
      "Epoch 147: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1331.0909 - mse: 1331.0909 - mae: 19.4761 - val_loss: 2341.1418 - val_mse: 2341.1418 - val_mae: 23.4623\n",
      "Epoch 148/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1737.0592 - mse: 1737.0592 - mae: 23.5341\n",
      "Epoch 148: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1311.8394 - mse: 1311.8394 - mae: 21.9871 - val_loss: 1802.5011 - val_mse: 1802.5011 - val_mae: 19.0431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 873.0896 - mse: 873.0896 - mae: 20.4712\n",
      "Epoch 149: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1264.1497 - mse: 1264.1497 - mae: 19.1519 - val_loss: 1522.9839 - val_mse: 1522.9839 - val_mae: 20.9650\n",
      "Epoch 150/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3610.8535 - mse: 3610.8535 - mae: 31.9709\n",
      "Epoch 150: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1145.6465 - mse: 1145.6465 - mae: 19.0846 - val_loss: 1742.6558 - val_mse: 1742.6555 - val_mae: 24.7322\n",
      "Epoch 151/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 600.5506 - mse: 600.5506 - mae: 14.6357\n",
      "Epoch 151: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1146.7089 - mse: 1146.7089 - mae: 18.9702 - val_loss: 2321.8337 - val_mse: 2321.8337 - val_mae: 26.9766\n",
      "Epoch 152/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1826.8158 - mse: 1826.8158 - mae: 23.3887\n",
      "Epoch 152: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1317.6768 - mse: 1317.6768 - mae: 21.3093 - val_loss: 1406.0452 - val_mse: 1406.0453 - val_mae: 18.7244\n",
      "Epoch 153/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 900.7917 - mse: 900.7917 - mae: 18.1828\n",
      "Epoch 153: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1614.7139 - mse: 1614.7139 - mae: 21.5339 - val_loss: 2444.4470 - val_mse: 2444.4470 - val_mae: 26.4488\n",
      "Epoch 154/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 692.9536 - mse: 692.9536 - mae: 17.6830\n",
      "Epoch 154: val_loss did not improve from 1213.85571\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1666.9230 - mse: 1666.9230 - mae: 22.8622 - val_loss: 2088.9419 - val_mse: 2088.9419 - val_mae: 22.4777\n",
      "Epoch 155/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 403.7594 - mse: 403.7594 - mae: 13.9770\n",
      "Epoch 155: val_loss improved from 1213.85571 to 1073.55847, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1113.6356 - mse: 1113.6356 - mae: 19.0788 - val_loss: 1073.5585 - val_mse: 1073.5585 - val_mae: 16.7426\n",
      "Epoch 156/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 610.5884 - mse: 610.5884 - mae: 18.0348\n",
      "Epoch 156: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1870.8215 - mse: 1870.8215 - mae: 23.0867 - val_loss: 2431.3354 - val_mse: 2431.3354 - val_mae: 26.9305\n",
      "Epoch 157/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1931.8303 - mse: 1931.8303 - mae: 23.1823\n",
      "Epoch 157: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1252.0708 - mse: 1252.0708 - mae: 21.0400 - val_loss: 2185.2415 - val_mse: 2185.2415 - val_mae: 22.5978\n",
      "Epoch 158/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2241.6377 - mse: 2241.6377 - mae: 28.7182\n",
      "Epoch 158: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 886.0932 - mse: 886.0932 - mae: 18.2592 - val_loss: 1228.9465 - val_mse: 1228.9465 - val_mae: 16.3047\n",
      "Epoch 159/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1941.3868 - mse: 1941.3868 - mae: 24.5984\n",
      "Epoch 159: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1208.0944 - mse: 1208.0944 - mae: 19.9565 - val_loss: 3137.6587 - val_mse: 3137.6587 - val_mae: 30.4114\n",
      "Epoch 160/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1814.2896 - mse: 1814.2896 - mae: 20.0297\n",
      "Epoch 160: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1157.7826 - mse: 1157.7826 - mae: 20.2669 - val_loss: 2643.0027 - val_mse: 2643.0027 - val_mae: 26.4714\n",
      "Epoch 161/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1248.6807 - mse: 1248.6807 - mae: 19.0398\n",
      "Epoch 161: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 838.5103 - mse: 838.5103 - mae: 17.6036 - val_loss: 1809.9127 - val_mse: 1809.9127 - val_mae: 20.6076\n",
      "Epoch 162/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 922.8722 - mse: 922.8722 - mae: 16.8284\n",
      "Epoch 162: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 756.8106 - mse: 756.8106 - mae: 16.2830 - val_loss: 2314.6411 - val_mse: 2314.6411 - val_mae: 23.4755\n",
      "Epoch 163/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 503.9182 - mse: 503.9182 - mae: 16.1046\n",
      "Epoch 163: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 720.4706 - mse: 720.4706 - mae: 16.9866 - val_loss: 1484.0675 - val_mse: 1484.0677 - val_mae: 17.6046\n",
      "Epoch 164/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1247.6201 - mse: 1247.6201 - mae: 15.0227\n",
      "Epoch 164: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1550.2198 - mse: 1550.2198 - mae: 21.3401 - val_loss: 1417.9532 - val_mse: 1417.9532 - val_mae: 18.5124\n",
      "Epoch 165/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 664.2408 - mse: 664.2408 - mae: 16.3835\n",
      "Epoch 165: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1327.7170 - mse: 1327.7170 - mae: 20.1188 - val_loss: 3378.2725 - val_mse: 3378.2722 - val_mae: 32.5187\n",
      "Epoch 166/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 513.5540 - mse: 513.5540 - mae: 15.4066\n",
      "Epoch 166: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1761.1837 - mse: 1761.1837 - mae: 22.9412 - val_loss: 4347.1660 - val_mse: 4347.1660 - val_mae: 35.2230\n",
      "Epoch 167/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4049.8853 - mse: 4049.8853 - mae: 31.9439\n",
      "Epoch 167: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1660.0042 - mse: 1660.0042 - mae: 23.0627 - val_loss: 1238.6200 - val_mse: 1238.6200 - val_mae: 18.0792\n",
      "Epoch 168/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2398.1870 - mse: 2398.1870 - mae: 29.0599\n",
      "Epoch 168: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1259.4261 - mse: 1259.4261 - mae: 20.4023 - val_loss: 2009.9934 - val_mse: 2009.9934 - val_mae: 26.0327\n",
      "Epoch 169/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 404.6312 - mse: 404.6312 - mae: 16.1976\n",
      "Epoch 169: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1022.4304 - mse: 1022.4304 - mae: 18.9541 - val_loss: 2174.2603 - val_mse: 2174.2603 - val_mae: 27.7529\n",
      "Epoch 170/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 670.5236 - mse: 670.5236 - mae: 19.5103\n",
      "Epoch 170: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 825.3343 - mse: 825.3343 - mae: 18.7586 - val_loss: 1289.3765 - val_mse: 1289.3765 - val_mae: 17.3066\n",
      "Epoch 171/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 459.7083 - mse: 459.7083 - mae: 13.1589\n",
      "Epoch 171: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 716.7983 - mse: 716.7983 - mae: 16.8432 - val_loss: 1399.0653 - val_mse: 1399.0653 - val_mae: 17.2423\n",
      "Epoch 172/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 264.4485 - mse: 264.4485 - mae: 10.9819\n",
      "Epoch 172: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1829.7928 - mse: 1829.7928 - mae: 21.7581 - val_loss: 1644.3607 - val_mse: 1644.3607 - val_mae: 20.6562\n",
      "Epoch 173/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2545.4512 - mse: 2545.4512 - mae: 26.4121\n",
      "Epoch 173: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1341.7864 - mse: 1341.7864 - mae: 19.9062 - val_loss: 1464.1139 - val_mse: 1464.1140 - val_mae: 20.7165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 944.8983 - mse: 944.8983 - mae: 18.7484\n",
      "Epoch 174: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 844.5948 - mse: 844.5948 - mae: 17.7852 - val_loss: 2044.5750 - val_mse: 2044.5750 - val_mae: 23.2052\n",
      "Epoch 175/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1497.5154 - mse: 1497.5154 - mae: 23.5815\n",
      "Epoch 175: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1013.0200 - mse: 1013.0200 - mae: 18.8850 - val_loss: 1910.6274 - val_mse: 1910.6274 - val_mae: 21.0610\n",
      "Epoch 176/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1326.6132 - mse: 1326.6132 - mae: 18.3228\n",
      "Epoch 176: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1153.6859 - mse: 1153.6859 - mae: 18.1944 - val_loss: 1442.2489 - val_mse: 1442.2489 - val_mae: 19.2891\n",
      "Epoch 177/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 914.1762 - mse: 914.1762 - mae: 18.1123\n",
      "Epoch 177: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 955.1607 - mse: 955.1607 - mae: 18.6714 - val_loss: 1839.1866 - val_mse: 1839.1866 - val_mae: 23.5048\n",
      "Epoch 178/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1771.5406 - mse: 1771.5406 - mae: 21.6932\n",
      "Epoch 178: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1053.1846 - mse: 1053.1846 - mae: 19.4625 - val_loss: 2103.1074 - val_mse: 2103.1074 - val_mae: 24.0011\n",
      "Epoch 179/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 852.7135 - mse: 852.7135 - mae: 16.3541\n",
      "Epoch 179: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1356.1637 - mse: 1356.1637 - mae: 19.3467 - val_loss: 1667.2030 - val_mse: 1667.2030 - val_mae: 18.8891\n",
      "Epoch 180/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 589.1055 - mse: 589.1055 - mae: 17.7222\n",
      "Epoch 180: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1010.1374 - mse: 1010.1374 - mae: 19.5597 - val_loss: 2124.6746 - val_mse: 2124.6746 - val_mae: 23.3696\n",
      "Epoch 181/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1613.8455 - mse: 1613.8455 - mae: 23.9252\n",
      "Epoch 181: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 941.1406 - mse: 941.1406 - mae: 19.1590 - val_loss: 1374.7719 - val_mse: 1374.7719 - val_mae: 19.6272\n",
      "Epoch 182/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 637.6598 - mse: 637.6598 - mae: 17.5087\n",
      "Epoch 182: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1135.9785 - mse: 1135.9785 - mae: 19.0222 - val_loss: 1862.4828 - val_mse: 1862.4828 - val_mae: 22.4346\n",
      "Epoch 183/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1287.2837 - mse: 1287.2837 - mae: 21.0747\n",
      "Epoch 183: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1264.0332 - mse: 1264.0332 - mae: 18.7880 - val_loss: 1444.6958 - val_mse: 1444.6957 - val_mae: 18.4163\n",
      "Epoch 184/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 672.7026 - mse: 672.7026 - mae: 16.5531\n",
      "Epoch 184: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 983.0389 - mse: 983.0389 - mae: 18.2512 - val_loss: 2187.3855 - val_mse: 2187.3855 - val_mae: 23.5356\n",
      "Epoch 185/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2434.8306 - mse: 2434.8306 - mae: 28.7234\n",
      "Epoch 185: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1506.5861 - mse: 1506.5861 - mae: 20.9182 - val_loss: 2628.6001 - val_mse: 2628.6001 - val_mae: 24.4044\n",
      "Epoch 186/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 352.2570 - mse: 352.2570 - mae: 12.4001\n",
      "Epoch 186: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 621.6055 - mse: 621.6055 - mae: 16.1805 - val_loss: 1160.6414 - val_mse: 1160.6414 - val_mae: 15.8264\n",
      "Epoch 187/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2366.8220 - mse: 2366.8220 - mae: 25.7707\n",
      "Epoch 187: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1232.1597 - mse: 1232.1597 - mae: 18.9321 - val_loss: 2047.7686 - val_mse: 2047.7686 - val_mae: 22.0842\n",
      "Epoch 188/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1296.8250 - mse: 1296.8250 - mae: 22.1915\n",
      "Epoch 188: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 886.2685 - mse: 886.2685 - mae: 18.4881 - val_loss: 1838.0391 - val_mse: 1838.0391 - val_mae: 22.3613\n",
      "Epoch 189/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 452.5253 - mse: 452.5253 - mae: 14.7791\n",
      "Epoch 189: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1084.2748 - mse: 1084.2748 - mae: 19.0091 - val_loss: 1927.7592 - val_mse: 1927.7592 - val_mae: 21.5212\n",
      "Epoch 190/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 818.7181 - mse: 818.7181 - mae: 19.3534\n",
      "Epoch 190: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 964.1655 - mse: 964.1655 - mae: 17.1958 - val_loss: 1799.9156 - val_mse: 1799.9156 - val_mae: 19.9582\n",
      "Epoch 191/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1210.3428 - mse: 1210.3428 - mae: 20.3747\n",
      "Epoch 191: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 770.8906 - mse: 770.8906 - mae: 16.0745 - val_loss: 2642.2622 - val_mse: 2642.2622 - val_mae: 24.2853\n",
      "Epoch 192/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 278.7971 - mse: 278.7971 - mae: 11.8753\n",
      "Epoch 192: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 642.7708 - mse: 642.7708 - mae: 15.1169 - val_loss: 1989.2172 - val_mse: 1989.2172 - val_mae: 19.5929\n",
      "Epoch 193/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3357.2852 - mse: 3357.2852 - mae: 34.1804\n",
      "Epoch 193: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1664.3776 - mse: 1664.3776 - mae: 22.2317 - val_loss: 1218.3907 - val_mse: 1218.3907 - val_mae: 17.2754\n",
      "Epoch 194/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1232.6060 - mse: 1232.6060 - mae: 19.5775\n",
      "Epoch 194: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1092.4485 - mse: 1092.4485 - mae: 19.5423 - val_loss: 4842.0288 - val_mse: 4842.0288 - val_mae: 40.0595\n",
      "Epoch 195/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2639.8420 - mse: 2639.8420 - mae: 24.5683\n",
      "Epoch 195: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1798.2925 - mse: 1798.2925 - mae: 23.4770 - val_loss: 1820.5626 - val_mse: 1820.5626 - val_mae: 20.4747\n",
      "Epoch 196/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1562.8630 - mse: 1562.8630 - mae: 23.0505\n",
      "Epoch 196: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1314.4266 - mse: 1314.4266 - mae: 21.9895 - val_loss: 1589.9330 - val_mse: 1589.9331 - val_mae: 19.6810\n",
      "Epoch 197/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2214.2915 - mse: 2214.2915 - mae: 31.9756\n",
      "Epoch 197: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1075.0858 - mse: 1075.0858 - mae: 19.6769 - val_loss: 1678.1372 - val_mse: 1678.1373 - val_mae: 21.9727\n",
      "Epoch 198/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 676.2885 - mse: 676.2885 - mae: 15.6708\n",
      "Epoch 198: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 954.0588 - mse: 954.0588 - mae: 17.9472 - val_loss: 1873.9713 - val_mse: 1873.9713 - val_mae: 23.4740\n",
      "Epoch 199/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 399.1620 - mse: 399.1620 - mae: 14.1898\n",
      "Epoch 199: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 946.1527 - mse: 946.1527 - mae: 17.9839 - val_loss: 1938.9893 - val_mse: 1938.9893 - val_mae: 22.4662\n",
      "Epoch 200/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1160.4211 - mse: 1160.4211 - mae: 21.6465\n",
      "Epoch 200: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 796.1892 - mse: 796.1892 - mae: 17.1344 - val_loss: 1818.9865 - val_mse: 1818.9865 - val_mae: 20.0333\n",
      "Epoch 201/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1237.8253 - mse: 1237.8253 - mae: 17.0671\n",
      "Epoch 201: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 932.7057 - mse: 932.7057 - mae: 16.4346 - val_loss: 1263.3787 - val_mse: 1263.3787 - val_mae: 17.7977\n",
      "Epoch 202/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1052.3256 - mse: 1052.3256 - mae: 17.2304\n",
      "Epoch 202: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1234.6573 - mse: 1234.6573 - mae: 19.0888 - val_loss: 1920.8496 - val_mse: 1920.8496 - val_mae: 23.2381\n",
      "Epoch 203/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 529.5490 - mse: 529.5490 - mae: 13.9202\n",
      "Epoch 203: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 747.1460 - mse: 747.1460 - mae: 16.2989 - val_loss: 1893.0328 - val_mse: 1893.0328 - val_mae: 22.3137\n",
      "Epoch 204/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1078.4867 - mse: 1078.4867 - mae: 20.3097\n",
      "Epoch 204: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 921.1975 - mse: 921.1975 - mae: 18.2105 - val_loss: 1757.3101 - val_mse: 1757.3101 - val_mae: 19.6346\n",
      "Epoch 205/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 492.9630 - mse: 492.9630 - mae: 15.4705\n",
      "Epoch 205: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 421.2443 - mse: 421.2443 - mae: 14.4599 - val_loss: 3242.8813 - val_mse: 3242.8813 - val_mae: 28.7242\n",
      "Epoch 206/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 511.6713 - mse: 511.6713 - mae: 12.2375\n",
      "Epoch 206: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1059.6110 - mse: 1059.6110 - mae: 18.5772 - val_loss: 1646.9507 - val_mse: 1646.9507 - val_mae: 18.7547\n",
      "Epoch 207/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 118.7078 - mse: 118.7078 - mae: 8.0652\n",
      "Epoch 207: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1460.3076 - mse: 1460.3076 - mae: 19.8431 - val_loss: 1520.2869 - val_mse: 1520.2870 - val_mae: 20.1121\n",
      "Epoch 208/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1172.0906 - mse: 1172.0906 - mae: 19.2155\n",
      "Epoch 208: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1535.5891 - mse: 1535.5891 - mae: 20.4758 - val_loss: 2222.9268 - val_mse: 2222.9268 - val_mae: 26.5786\n",
      "Epoch 209/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 829.1737 - mse: 829.1737 - mae: 19.9584\n",
      "Epoch 209: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 625.6415 - mse: 625.6415 - mae: 16.0251 - val_loss: 1445.4139 - val_mse: 1445.4139 - val_mae: 19.4311\n",
      "Epoch 210/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1763.3778 - mse: 1763.3778 - mae: 25.3173\n",
      "Epoch 210: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1515.6793 - mse: 1515.6793 - mae: 19.4057 - val_loss: 2094.6833 - val_mse: 2094.6833 - val_mae: 21.7926\n",
      "Epoch 211/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1091.5854 - mse: 1091.5854 - mae: 18.4558\n",
      "Epoch 211: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 882.0714 - mse: 882.0714 - mae: 16.7691 - val_loss: 1454.8969 - val_mse: 1454.8969 - val_mae: 17.6747\n",
      "Epoch 212/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1879.4333 - mse: 1879.4333 - mae: 22.4468\n",
      "Epoch 212: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 942.8041 - mse: 942.8041 - mae: 17.3808 - val_loss: 3077.9878 - val_mse: 3077.9878 - val_mae: 26.5669\n",
      "Epoch 213/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4055.1367 - mse: 4055.1367 - mae: 26.7284\n",
      "Epoch 213: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1361.8295 - mse: 1361.8295 - mae: 19.1522 - val_loss: 1550.6759 - val_mse: 1550.6759 - val_mae: 17.1939\n",
      "Epoch 214/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1045.9087 - mse: 1045.9087 - mae: 22.0207\n",
      "Epoch 214: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1208.2664 - mse: 1208.2664 - mae: 19.7270 - val_loss: 2026.8403 - val_mse: 2026.8403 - val_mae: 21.7253\n",
      "Epoch 215/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2424.2800 - mse: 2424.2800 - mae: 22.4180\n",
      "Epoch 215: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1100.1002 - mse: 1100.1002 - mae: 18.2905 - val_loss: 1652.9575 - val_mse: 1652.9575 - val_mae: 19.5506\n",
      "Epoch 216/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1356.6311 - mse: 1356.6311 - mae: 19.7370\n",
      "Epoch 216: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1118.8390 - mse: 1118.8390 - mae: 18.4993 - val_loss: 2290.3003 - val_mse: 2290.3003 - val_mae: 24.5655\n",
      "Epoch 217/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2503.7427 - mse: 2503.7427 - mae: 21.1221\n",
      "Epoch 217: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1617.1451 - mse: 1617.1451 - mae: 19.1454 - val_loss: 1503.4198 - val_mse: 1503.4198 - val_mae: 18.2025\n",
      "Epoch 218/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 492.4282 - mse: 492.4282 - mae: 12.4075\n",
      "Epoch 218: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 994.3789 - mse: 994.3789 - mae: 16.7772 - val_loss: 1416.7854 - val_mse: 1416.7855 - val_mae: 18.2658\n",
      "Epoch 219/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 777.0275 - mse: 777.0275 - mae: 16.5140\n",
      "Epoch 219: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1103.5227 - mse: 1103.5227 - mae: 19.2042 - val_loss: 1465.7491 - val_mse: 1465.7491 - val_mae: 19.9250\n",
      "Epoch 220/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1633.6545 - mse: 1633.6545 - mae: 19.7472\n",
      "Epoch 220: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1242.7454 - mse: 1242.7454 - mae: 18.1182 - val_loss: 2559.1660 - val_mse: 2559.1660 - val_mae: 26.5654\n",
      "Epoch 221/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 547.7006 - mse: 547.7006 - mae: 15.6701\n",
      "Epoch 221: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1214.4880 - mse: 1214.4880 - mae: 18.5636 - val_loss: 1784.2877 - val_mse: 1784.2877 - val_mae: 19.6602\n",
      "Epoch 222/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1122.2762 - mse: 1122.2762 - mae: 18.2850\n",
      "Epoch 222: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1348.1243 - mse: 1348.1243 - mae: 20.4909 - val_loss: 1899.8807 - val_mse: 1899.8807 - val_mae: 20.3693\n",
      "Epoch 223/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1715.2650 - mse: 1715.2650 - mae: 18.6348\n",
      "Epoch 223: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1030.2823 - mse: 1030.2823 - mae: 17.0013 - val_loss: 1615.6294 - val_mse: 1615.6294 - val_mae: 21.4220\n",
      "Epoch 224/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 1013.0596 - mse: 1013.0596 - mae: 20.1573\n",
      "Epoch 224: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 933.5551 - mse: 933.5551 - mae: 18.6824 - val_loss: 3280.9910 - val_mse: 3280.9910 - val_mae: 32.5531\n",
      "Epoch 225/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1474.9406 - mse: 1474.9406 - mae: 21.1773\n",
      "Epoch 225: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1333.0679 - mse: 1333.0679 - mae: 20.0274 - val_loss: 1198.4814 - val_mse: 1198.4814 - val_mae: 16.7564\n",
      "Epoch 226/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1988.4976 - mse: 1988.4976 - mae: 21.5343\n",
      "Epoch 226: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1480.8149 - mse: 1480.8149 - mae: 21.1984 - val_loss: 1926.5167 - val_mse: 1926.5167 - val_mae: 22.4404\n",
      "Epoch 227/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 625.5553 - mse: 625.5553 - mae: 17.3781\n",
      "Epoch 227: val_loss did not improve from 1073.55847\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1392.8091 - mse: 1392.8091 - mae: 20.3768 - val_loss: 2088.1614 - val_mse: 2088.1614 - val_mae: 22.8924\n",
      "Epoch 228/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 383.7364 - mse: 383.7364 - mae: 13.4554\n",
      "Epoch 228: val_loss improved from 1073.55847 to 848.35992, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 849.9471 - mse: 849.9471 - mae: 17.7340 - val_loss: 848.3599 - val_mse: 848.3599 - val_mae: 14.7008\n",
      "Epoch 229/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 705.3629 - mse: 705.3629 - mae: 15.9851\n",
      "Epoch 229: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1052.9828 - mse: 1052.9828 - mae: 17.8667 - val_loss: 1384.9653 - val_mse: 1384.9653 - val_mae: 20.5347\n",
      "Epoch 230/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1165.0222 - mse: 1165.0222 - mae: 19.7661\n",
      "Epoch 230: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 937.6889 - mse: 937.6889 - mae: 18.3464 - val_loss: 1974.9028 - val_mse: 1974.9028 - val_mae: 23.5528\n",
      "Epoch 231/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1030.9548 - mse: 1030.9548 - mae: 18.2083\n",
      "Epoch 231: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 851.5378 - mse: 851.5378 - mae: 17.4452 - val_loss: 957.9330 - val_mse: 957.9330 - val_mae: 15.2221\n",
      "Epoch 232/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 551.1173 - mse: 551.1173 - mae: 12.5351\n",
      "Epoch 232: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1023.7691 - mse: 1023.7691 - mae: 18.4482 - val_loss: 1318.4749 - val_mse: 1318.4749 - val_mae: 19.8830\n",
      "Epoch 233/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 611.1244 - mse: 611.1244 - mae: 13.8566\n",
      "Epoch 233: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1143.0433 - mse: 1143.0433 - mae: 19.5790 - val_loss: 2745.6431 - val_mse: 2745.6431 - val_mae: 29.9215\n",
      "Epoch 234/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1429.7059 - mse: 1429.7059 - mae: 21.0465\n",
      "Epoch 234: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 840.9733 - mse: 840.9733 - mae: 16.8568 - val_loss: 1083.6462 - val_mse: 1083.6462 - val_mae: 14.7411\n",
      "Epoch 235/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2546.5938 - mse: 2546.5938 - mae: 22.4747\n",
      "Epoch 235: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1566.8738 - mse: 1566.8738 - mae: 21.0027 - val_loss: 1255.6122 - val_mse: 1255.6122 - val_mae: 17.5672\n",
      "Epoch 236/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 742.1262 - mse: 742.1262 - mae: 16.7669\n",
      "Epoch 236: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1032.4714 - mse: 1032.4714 - mae: 18.3211 - val_loss: 2410.8884 - val_mse: 2410.8884 - val_mae: 27.4486\n",
      "Epoch 237/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1917.7787 - mse: 1917.7787 - mae: 21.6593\n",
      "Epoch 237: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1177.0356 - mse: 1177.0356 - mae: 18.5793 - val_loss: 1056.5624 - val_mse: 1056.5624 - val_mae: 16.9753\n",
      "Epoch 238/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 670.1787 - mse: 670.1787 - mae: 14.4786\n",
      "Epoch 238: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 940.6996 - mse: 940.6996 - mae: 17.9017 - val_loss: 1247.1699 - val_mse: 1247.1699 - val_mae: 18.1234\n",
      "Epoch 239/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 376.3277 - mse: 376.3277 - mae: 13.1494\n",
      "Epoch 239: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 652.6728 - mse: 652.6728 - mae: 14.4352 - val_loss: 1182.3533 - val_mse: 1182.3533 - val_mae: 19.0000\n",
      "Epoch 240/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 837.1771 - mse: 837.1771 - mae: 18.7322\n",
      "Epoch 240: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1315.5046 - mse: 1315.5046 - mae: 18.2639 - val_loss: 1320.0000 - val_mse: 1320.0000 - val_mae: 20.4288\n",
      "Epoch 241/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2121.8467 - mse: 2121.8467 - mae: 24.2137\n",
      "Epoch 241: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1100.4055 - mse: 1100.4055 - mae: 18.7992 - val_loss: 2102.1658 - val_mse: 2102.1658 - val_mae: 24.2020\n",
      "Epoch 242/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2678.2004 - mse: 2678.2004 - mae: 22.7891\n",
      "Epoch 242: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 846.2460 - mse: 846.2460 - mae: 15.4577 - val_loss: 1371.0082 - val_mse: 1371.0082 - val_mae: 16.1707\n",
      "Epoch 243/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1799.3315 - mse: 1799.3315 - mae: 26.3226\n",
      "Epoch 243: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1259.1333 - mse: 1259.1333 - mae: 18.8918 - val_loss: 944.5446 - val_mse: 944.5446 - val_mae: 13.7391\n",
      "Epoch 244/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1211.5312 - mse: 1211.5312 - mae: 22.2851\n",
      "Epoch 244: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 652.2507 - mse: 652.2507 - mae: 15.1428 - val_loss: 2818.3833 - val_mse: 2818.3838 - val_mae: 27.4456\n",
      "Epoch 245/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1560.4265 - mse: 1560.4265 - mae: 23.3637\n",
      "Epoch 245: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1131.8962 - mse: 1131.8962 - mae: 20.0128 - val_loss: 1678.2323 - val_mse: 1678.2323 - val_mae: 17.7883\n",
      "Epoch 246/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2369.9980 - mse: 2369.9980 - mae: 29.7961\n",
      "Epoch 246: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1622.2368 - mse: 1622.2368 - mae: 20.9059 - val_loss: 2300.5771 - val_mse: 2300.5771 - val_mae: 22.6166\n",
      "Epoch 247/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2009.8317 - mse: 2009.8317 - mae: 21.4850\n",
      "Epoch 247: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 938.5635 - mse: 938.5635 - mae: 16.5911 - val_loss: 1934.6584 - val_mse: 1934.6584 - val_mae: 21.7550\n",
      "Epoch 248/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1507.6934 - mse: 1507.6934 - mae: 23.3285\n",
      "Epoch 248: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 917.3149 - mse: 917.3149 - mae: 17.7721 - val_loss: 1895.4001 - val_mse: 1895.4001 - val_mae: 21.5520\n",
      "Epoch 249/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 687.6499 - mse: 687.6499 - mae: 17.8199\n",
      "Epoch 249: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 955.0648 - mse: 955.0648 - mae: 17.8761 - val_loss: 1873.2349 - val_mse: 1873.2349 - val_mae: 22.4840\n",
      "Epoch 250/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 332.7755 - mse: 332.7755 - mae: 12.1827\n",
      "Epoch 250: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 752.2461 - mse: 752.2461 - mae: 16.8606 - val_loss: 1744.3195 - val_mse: 1744.3195 - val_mae: 22.3446\n",
      "Epoch 251/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 639.1732 - mse: 639.1732 - mae: 16.1501\n",
      "Epoch 251: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 770.2162 - mse: 770.2162 - mae: 16.9315 - val_loss: 915.8962 - val_mse: 915.8962 - val_mae: 17.7896\n",
      "Epoch 252/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1225.0833 - mse: 1225.0833 - mae: 18.0517\n",
      "Epoch 252: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 977.1464 - mse: 977.1464 - mae: 18.1808 - val_loss: 1534.2476 - val_mse: 1534.2476 - val_mae: 22.1329\n",
      "Epoch 253/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 474.5892 - mse: 474.5892 - mae: 14.1011\n",
      "Epoch 253: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 672.7015 - mse: 672.7015 - mae: 16.4034 - val_loss: 1628.2279 - val_mse: 1628.2279 - val_mae: 21.7145\n",
      "Epoch 254/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 563.0723 - mse: 563.0723 - mae: 17.2442\n",
      "Epoch 254: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 789.2377 - mse: 789.2377 - mae: 16.7027 - val_loss: 1279.4736 - val_mse: 1279.4736 - val_mae: 16.9890\n",
      "Epoch 255/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1917.0435 - mse: 1917.0435 - mae: 28.2963\n",
      "Epoch 255: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1767.0361 - mse: 1767.0361 - mae: 19.8056 - val_loss: 1475.5353 - val_mse: 1475.5353 - val_mae: 18.9546\n",
      "Epoch 256/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 721.5526 - mse: 721.5526 - mae: 17.1748\n",
      "Epoch 256: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 717.1234 - mse: 717.1234 - mae: 16.3612 - val_loss: 1513.5502 - val_mse: 1513.5502 - val_mae: 19.5389\n",
      "Epoch 257/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 374.8820 - mse: 374.8820 - mae: 14.3622\n",
      "Epoch 257: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1075.3474 - mse: 1075.3474 - mae: 18.0194 - val_loss: 1698.8326 - val_mse: 1698.8326 - val_mae: 19.5570\n",
      "Epoch 258/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2291.5110 - mse: 2291.5110 - mae: 26.7950\n",
      "Epoch 258: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1297.2916 - mse: 1297.2916 - mae: 18.8380 - val_loss: 1532.7935 - val_mse: 1532.7935 - val_mae: 16.9707\n",
      "Epoch 259/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1973.7000 - mse: 1973.7000 - mae: 26.3176\n",
      "Epoch 259: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1272.8324 - mse: 1272.8324 - mae: 21.1977 - val_loss: 1004.6794 - val_mse: 1004.6794 - val_mae: 14.7280\n",
      "Epoch 260/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 259.3877 - mse: 259.3877 - mae: 10.7750\n",
      "Epoch 260: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1027.2454 - mse: 1027.2454 - mae: 18.1928 - val_loss: 1315.0981 - val_mse: 1315.0981 - val_mae: 20.5857\n",
      "Epoch 261/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 949.1411 - mse: 949.1411 - mae: 21.8848\n",
      "Epoch 261: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 776.1558 - mse: 776.1558 - mae: 16.6599 - val_loss: 1881.8192 - val_mse: 1881.8192 - val_mae: 25.5880\n",
      "Epoch 262/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 435.0694 - mse: 435.0694 - mae: 15.8351\n",
      "Epoch 262: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 910.9188 - mse: 910.9188 - mae: 18.2424 - val_loss: 2161.8083 - val_mse: 2161.8083 - val_mae: 24.4808\n",
      "Epoch 263/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2962.2744 - mse: 2962.2744 - mae: 26.6007\n",
      "Epoch 263: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1414.3673 - mse: 1414.3673 - mae: 20.7736 - val_loss: 1252.8942 - val_mse: 1252.8942 - val_mae: 15.7161\n",
      "Epoch 264/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2445.0146 - mse: 2445.0146 - mae: 28.1799\n",
      "Epoch 264: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1175.8163 - mse: 1175.8163 - mae: 19.4509 - val_loss: 930.4236 - val_mse: 930.4236 - val_mae: 17.7449\n",
      "Epoch 265/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1509.6558 - mse: 1509.6558 - mae: 22.0818\n",
      "Epoch 265: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1478.8875 - mse: 1478.8875 - mae: 21.5011 - val_loss: 3209.9084 - val_mse: 3209.9084 - val_mae: 33.0540\n",
      "Epoch 266/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3768.0630 - mse: 3768.0630 - mae: 29.4595\n",
      "Epoch 266: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1360.2183 - mse: 1360.2183 - mae: 19.1101 - val_loss: 1435.7618 - val_mse: 1435.7618 - val_mae: 17.7793\n",
      "Epoch 267/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 204.8833 - mse: 204.8833 - mae: 8.5215\n",
      "Epoch 267: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1424.0092 - mse: 1424.0092 - mae: 18.9265 - val_loss: 1177.0012 - val_mse: 1177.0012 - val_mae: 16.9926\n",
      "Epoch 268/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 421.4939 - mse: 421.4939 - mae: 13.8723\n",
      "Epoch 268: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1009.2313 - mse: 1009.2313 - mae: 17.7861 - val_loss: 1340.4370 - val_mse: 1340.4370 - val_mae: 20.8005\n",
      "Epoch 269/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 281.3565 - mse: 281.3565 - mae: 11.5491\n",
      "Epoch 269: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 913.3206 - mse: 913.3206 - mae: 17.0886 - val_loss: 1485.3959 - val_mse: 1485.3960 - val_mae: 21.2674\n",
      "Epoch 270/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2489.7712 - mse: 2489.7712 - mae: 28.4795\n",
      "Epoch 270: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1103.7786 - mse: 1103.7786 - mae: 19.1829 - val_loss: 1563.7688 - val_mse: 1563.7688 - val_mae: 19.8927\n",
      "Epoch 271/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 516.8538 - mse: 516.8538 - mae: 14.2067\n",
      "Epoch 271: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 792.8774 - mse: 792.8774 - mae: 16.1856 - val_loss: 1362.6255 - val_mse: 1362.6255 - val_mae: 17.5882\n",
      "Epoch 272/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 756.5717 - mse: 756.5717 - mae: 16.6730\n",
      "Epoch 272: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1192.4941 - mse: 1192.4941 - mae: 18.6961 - val_loss: 1347.0945 - val_mse: 1347.0945 - val_mae: 18.0526\n",
      "Epoch 273/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 734.6867 - mse: 734.6867 - mae: 16.6380\n",
      "Epoch 273: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 984.4178 - mse: 984.4178 - mae: 17.7044 - val_loss: 1331.2504 - val_mse: 1331.2504 - val_mae: 18.5542\n",
      "Epoch 274/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 514.9709 - mse: 514.9709 - mae: 14.1104\n",
      "Epoch 274: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 794.5196 - mse: 794.5196 - mae: 16.4457 - val_loss: 1397.4900 - val_mse: 1397.4900 - val_mae: 20.0441\n",
      "Epoch 275/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 722.0787 - mse: 722.0787 - mae: 16.2455\n",
      "Epoch 275: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 862.6764 - mse: 862.6764 - mae: 16.6320 - val_loss: 1766.7635 - val_mse: 1766.7635 - val_mae: 23.3324\n",
      "Epoch 276/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 672.5184 - mse: 672.5184 - mae: 15.6482\n",
      "Epoch 276: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1192.8259 - mse: 1192.8259 - mae: 19.6581 - val_loss: 1477.3809 - val_mse: 1477.3809 - val_mae: 17.9764\n",
      "Epoch 277/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 404.5522 - mse: 404.5522 - mae: 13.0911\n",
      "Epoch 277: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1234.9355 - mse: 1234.9355 - mae: 19.5053 - val_loss: 1003.2161 - val_mse: 1003.2161 - val_mae: 14.5547\n",
      "Epoch 278/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1934.8573 - mse: 1934.8573 - mae: 23.3877\n",
      "Epoch 278: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 918.1377 - mse: 918.1377 - mae: 17.7020 - val_loss: 3856.2090 - val_mse: 3856.2090 - val_mae: 37.4904\n",
      "Epoch 279/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2315.6025 - mse: 2315.6025 - mae: 25.7279\n",
      "Epoch 279: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1414.2421 - mse: 1414.2421 - mae: 20.8689 - val_loss: 1126.2651 - val_mse: 1126.2651 - val_mae: 16.2521\n",
      "Epoch 280/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1289.4578 - mse: 1289.4578 - mae: 21.7162\n",
      "Epoch 280: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1432.6627 - mse: 1432.6627 - mae: 20.9820 - val_loss: 1571.9302 - val_mse: 1571.9302 - val_mae: 21.9021\n",
      "Epoch 281/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 291.1602 - mse: 291.1602 - mae: 11.8020\n",
      "Epoch 281: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1030.4698 - mse: 1030.4698 - mae: 17.6937 - val_loss: 3024.9294 - val_mse: 3024.9294 - val_mae: 31.1939\n",
      "Epoch 282/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1595.4746 - mse: 1595.4746 - mae: 23.9144\n",
      "Epoch 282: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1159.8615 - mse: 1159.8615 - mae: 19.1031 - val_loss: 1119.2585 - val_mse: 1119.2585 - val_mae: 16.0957\n",
      "Epoch 283/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 269.0176 - mse: 269.0176 - mae: 10.9645\n",
      "Epoch 283: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1171.3418 - mse: 1171.3418 - mae: 19.2113 - val_loss: 1244.4805 - val_mse: 1244.4805 - val_mae: 17.4511\n",
      "Epoch 284/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 432.9327 - mse: 432.9327 - mae: 14.3285\n",
      "Epoch 284: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 837.4527 - mse: 837.4527 - mae: 17.0037 - val_loss: 2527.3098 - val_mse: 2527.3098 - val_mae: 28.2805\n",
      "Epoch 285/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 758.1978 - mse: 758.1978 - mae: 17.3735\n",
      "Epoch 285: val_loss did not improve from 848.35992\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1417.0686 - mse: 1417.0686 - mae: 20.1769 - val_loss: 1852.8187 - val_mse: 1852.8187 - val_mae: 20.8587\n",
      "Epoch 286/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1897.6404 - mse: 1897.6404 - mae: 25.3217\n",
      "Epoch 286: val_loss improved from 848.35992 to 707.27264, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1694.4974 - mse: 1694.4974 - mae: 21.8917 - val_loss: 707.2726 - val_mse: 707.2726 - val_mae: 16.1208\n",
      "Epoch 287/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1232.2109 - mse: 1232.2109 - mae: 18.1097\n",
      "Epoch 287: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1040.9418 - mse: 1040.9418 - mae: 18.6412 - val_loss: 2040.5490 - val_mse: 2040.5490 - val_mae: 27.5563\n",
      "Epoch 288/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1432.0972 - mse: 1432.0972 - mae: 23.4635\n",
      "Epoch 288: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1142.4788 - mse: 1142.4788 - mae: 20.0014 - val_loss: 1912.4585 - val_mse: 1912.4585 - val_mae: 25.8424\n",
      "Epoch 289/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 247.7527 - mse: 247.7527 - mae: 11.5947\n",
      "Epoch 289: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1020.7439 - mse: 1020.7439 - mae: 18.1865 - val_loss: 986.2294 - val_mse: 986.2294 - val_mae: 15.9113\n",
      "Epoch 290/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 754.3784 - mse: 754.3784 - mae: 18.5049\n",
      "Epoch 290: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 753.9012 - mse: 753.9012 - mae: 17.0867 - val_loss: 1536.5308 - val_mse: 1536.5308 - val_mae: 21.5783\n",
      "Epoch 291/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1929.2905 - mse: 1929.2905 - mae: 24.3971\n",
      "Epoch 291: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 885.4157 - mse: 885.4157 - mae: 16.5712 - val_loss: 1212.0555 - val_mse: 1212.0555 - val_mae: 18.6401\n",
      "Epoch 292/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1538.5483 - mse: 1538.5483 - mae: 24.0489\n",
      "Epoch 292: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 723.1012 - mse: 723.1012 - mae: 15.6513 - val_loss: 1396.3197 - val_mse: 1396.3197 - val_mae: 19.4396\n",
      "Epoch 293/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 313.7281 - mse: 313.7281 - mae: 12.9000\n",
      "Epoch 293: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 708.2936 - mse: 708.2936 - mae: 15.9388 - val_loss: 1757.6361 - val_mse: 1757.6361 - val_mae: 21.1208\n",
      "Epoch 294/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 428.0790 - mse: 428.0790 - mae: 14.1126\n",
      "Epoch 294: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 936.2965 - mse: 936.2965 - mae: 16.5789 - val_loss: 1017.3610 - val_mse: 1017.3610 - val_mae: 15.3357\n",
      "Epoch 295/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 322.7794 - mse: 322.7794 - mae: 11.9023\n",
      "Epoch 295: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 981.8845 - mse: 981.8845 - mae: 15.7659 - val_loss: 1276.7963 - val_mse: 1276.7963 - val_mae: 18.3977\n",
      "Epoch 296/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 498.0905 - mse: 498.0905 - mae: 11.8995\n",
      "Epoch 296: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 688.3447 - mse: 688.3447 - mae: 15.3319 - val_loss: 2114.6570 - val_mse: 2114.6570 - val_mae: 25.5877\n",
      "Epoch 297/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 410.9640 - mse: 410.9640 - mae: 12.8353\n",
      "Epoch 297: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 782.4507 - mse: 782.4507 - mae: 16.3742 - val_loss: 1250.1151 - val_mse: 1250.1151 - val_mae: 17.3415\n",
      "Epoch 298/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1823.9146 - mse: 1823.9146 - mae: 24.0345\n",
      "Epoch 298: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 796.5268 - mse: 796.5268 - mae: 16.0336 - val_loss: 1182.2778 - val_mse: 1182.2778 - val_mae: 15.6444\n",
      "Epoch 299/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 871.9346 - mse: 871.9346 - mae: 16.8318\n",
      "Epoch 299: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1201.5656 - mse: 1201.5656 - mae: 18.2709 - val_loss: 1993.9413 - val_mse: 1993.9413 - val_mae: 23.5556\n",
      "Epoch 300/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 364.9917 - mse: 364.9917 - mae: 14.0042\n",
      "Epoch 300: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1102.8052 - mse: 1102.8052 - mae: 18.1391 - val_loss: 1707.4194 - val_mse: 1707.4194 - val_mae: 21.2689\n",
      "Epoch 301/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 441.7384 - mse: 441.7384 - mae: 12.7991\n",
      "Epoch 301: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 995.0859 - mse: 995.0859 - mae: 17.6530 - val_loss: 2276.0959 - val_mse: 2276.0959 - val_mae: 22.4042\n",
      "Epoch 302/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 581.7147 - mse: 581.7147 - mae: 14.0586\n",
      "Epoch 302: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 627.4969 - mse: 627.4969 - mae: 15.4885 - val_loss: 1296.0001 - val_mse: 1296.0001 - val_mae: 13.7474\n",
      "Epoch 303/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1725.5599 - mse: 1725.5599 - mae: 21.5441\n",
      "Epoch 303: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 993.3002 - mse: 993.3002 - mae: 17.1205 - val_loss: 1796.6993 - val_mse: 1796.6993 - val_mae: 20.1288\n",
      "Epoch 304/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1786.9008 - mse: 1786.9008 - mae: 20.9860\n",
      "Epoch 304: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 953.2366 - mse: 953.2366 - mae: 17.6991 - val_loss: 1526.7997 - val_mse: 1526.7997 - val_mae: 20.2264\n",
      "Epoch 305/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 194.7918 - mse: 194.7918 - mae: 11.2955\n",
      "Epoch 305: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 657.4707 - mse: 657.4707 - mae: 15.3605 - val_loss: 1364.6483 - val_mse: 1364.6483 - val_mae: 19.4117\n",
      "Epoch 306/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 562.8988 - mse: 562.8988 - mae: 16.5827\n",
      "Epoch 306: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1138.4543 - mse: 1138.4543 - mae: 18.8557 - val_loss: 1310.7396 - val_mse: 1310.7396 - val_mae: 16.1596\n",
      "Epoch 307/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1296.2008 - mse: 1296.2008 - mae: 22.8634\n",
      "Epoch 307: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 987.1452 - mse: 987.1452 - mae: 17.4937 - val_loss: 1534.5309 - val_mse: 1534.5309 - val_mae: 19.3083\n",
      "Epoch 308/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 900.9795 - mse: 900.9795 - mae: 19.8727\n",
      "Epoch 308: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1053.8217 - mse: 1053.8217 - mae: 18.4145 - val_loss: 2035.7137 - val_mse: 2035.7137 - val_mae: 25.1702\n",
      "Epoch 309/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1635.8926 - mse: 1635.8926 - mae: 18.5922\n",
      "Epoch 309: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 750.1378 - mse: 750.1378 - mae: 14.8938 - val_loss: 1430.2234 - val_mse: 1430.2234 - val_mae: 20.6658\n",
      "Epoch 310/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1096.4670 - mse: 1096.4670 - mae: 21.0430\n",
      "Epoch 310: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1052.0743 - mse: 1052.0743 - mae: 17.5384 - val_loss: 870.1464 - val_mse: 870.1464 - val_mae: 16.4557\n",
      "Epoch 311/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 746.1235 - mse: 746.1235 - mae: 17.3646\n",
      "Epoch 311: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 848.1309 - mse: 848.1309 - mae: 17.4135 - val_loss: 2682.7056 - val_mse: 2682.7056 - val_mae: 29.4764\n",
      "Epoch 312/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2519.5710 - mse: 2519.5710 - mae: 23.1158\n",
      "Epoch 312: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1028.6080 - mse: 1028.6080 - mae: 18.2990 - val_loss: 2184.2493 - val_mse: 2184.2493 - val_mae: 23.2050\n",
      "Epoch 313/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1516.2196 - mse: 1516.2196 - mae: 23.8214\n",
      "Epoch 313: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 807.9063 - mse: 807.9063 - mae: 16.4174 - val_loss: 1336.8196 - val_mse: 1336.8195 - val_mae: 15.7230\n",
      "Epoch 314/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 965.8621 - mse: 965.8621 - mae: 18.0826\n",
      "Epoch 314: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 915.9891 - mse: 915.9891 - mae: 15.8594 - val_loss: 958.6068 - val_mse: 958.6068 - val_mae: 15.8789\n",
      "Epoch 315/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1606.9148 - mse: 1606.9148 - mae: 21.3746\n",
      "Epoch 315: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1016.4009 - mse: 1016.4009 - mae: 17.6545 - val_loss: 2166.7183 - val_mse: 2166.7183 - val_mae: 27.8112\n",
      "Epoch 316/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 905.7313 - mse: 905.7313 - mae: 18.0151\n",
      "Epoch 316: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 939.9390 - mse: 939.9390 - mae: 18.4166 - val_loss: 1368.5325 - val_mse: 1368.5325 - val_mae: 19.5347\n",
      "Epoch 317/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 452.1818 - mse: 452.1818 - mae: 15.8722\n",
      "Epoch 317: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 880.5912 - mse: 880.5912 - mae: 17.1496 - val_loss: 1417.0012 - val_mse: 1417.0012 - val_mae: 17.2487\n",
      "Epoch 318/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 610.8561 - mse: 610.8561 - mae: 12.1111\n",
      "Epoch 318: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 664.0207 - mse: 664.0207 - mae: 15.9586 - val_loss: 1014.5656 - val_mse: 1014.5656 - val_mae: 15.1969\n",
      "Epoch 319/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 496.1456 - mse: 496.1456 - mae: 11.2730\n",
      "Epoch 319: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1158.7789 - mse: 1158.7789 - mae: 16.6977 - val_loss: 2164.1833 - val_mse: 2164.1833 - val_mae: 24.8263\n",
      "Epoch 320/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1417.7583 - mse: 1417.7583 - mae: 18.9029\n",
      "Epoch 320: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 658.6598 - mse: 658.6598 - mae: 14.4365 - val_loss: 1732.7006 - val_mse: 1732.7006 - val_mae: 20.4268\n",
      "Epoch 321/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 904.3536 - mse: 904.3536 - mae: 14.5824\n",
      "Epoch 321: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 767.2971 - mse: 767.2971 - mae: 15.8237 - val_loss: 1372.2854 - val_mse: 1372.2853 - val_mae: 15.4995\n",
      "Epoch 322/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 557.5654 - mse: 557.5654 - mae: 12.8414\n",
      "Epoch 322: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 601.2747 - mse: 601.2747 - mae: 15.2665 - val_loss: 1206.5720 - val_mse: 1206.5720 - val_mae: 16.2912\n",
      "Epoch 323/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 568.5947 - mse: 568.5947 - mae: 14.7742\n",
      "Epoch 323: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 668.2411 - mse: 668.2411 - mae: 15.4247 - val_loss: 2572.0051 - val_mse: 2572.0051 - val_mae: 29.7307\n",
      "Epoch 324/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 929.8946 - mse: 929.8946 - mae: 15.6613\n",
      "Epoch 324: val_loss did not improve from 707.27264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 934.8401 - mse: 934.8401 - mae: 17.5663 - val_loss: 1346.1714 - val_mse: 1346.1714 - val_mae: 18.3782\n",
      "Epoch 325/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 321.4423 - mse: 321.4423 - mae: 12.7177\n",
      "Epoch 325: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 833.5089 - mse: 833.5089 - mae: 16.9165 - val_loss: 796.1499 - val_mse: 796.1499 - val_mae: 14.0483\n",
      "Epoch 326/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 586.8970 - mse: 586.8970 - mae: 15.2131\n",
      "Epoch 326: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 994.9645 - mse: 994.9645 - mae: 18.0656 - val_loss: 2196.2625 - val_mse: 2196.2625 - val_mae: 27.1414\n",
      "Epoch 327/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1077.3405 - mse: 1077.3405 - mae: 15.2411\n",
      "Epoch 327: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 800.8953 - mse: 800.8953 - mae: 16.8629 - val_loss: 1512.0651 - val_mse: 1512.0651 - val_mae: 20.3790\n",
      "Epoch 328/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 537.1564 - mse: 537.1564 - mae: 14.7467\n",
      "Epoch 328: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 491.4516 - mse: 491.4516 - mae: 14.1691 - val_loss: 1517.7797 - val_mse: 1517.7797 - val_mae: 17.5149\n",
      "Epoch 329/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 333.1800 - mse: 333.1800 - mae: 13.0194\n",
      "Epoch 329: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 664.5005 - mse: 664.5005 - mae: 15.1484 - val_loss: 1626.0251 - val_mse: 1626.0251 - val_mae: 18.7511\n",
      "Epoch 330/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 298.8574 - mse: 298.8574 - mae: 12.4112\n",
      "Epoch 330: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 544.1965 - mse: 544.1965 - mae: 14.3534 - val_loss: 1394.4243 - val_mse: 1394.4243 - val_mae: 17.6527\n",
      "Epoch 331/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 290.7913 - mse: 290.7913 - mae: 11.6109\n",
      "Epoch 331: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 629.1323 - mse: 629.1323 - mae: 14.4690 - val_loss: 1296.7526 - val_mse: 1296.7526 - val_mae: 17.2943\n",
      "Epoch 332/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2028.8038 - mse: 2028.8038 - mae: 24.4895\n",
      "Epoch 332: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 786.5177 - mse: 786.5177 - mae: 16.5040 - val_loss: 2114.6108 - val_mse: 2114.6108 - val_mae: 23.1304\n",
      "Epoch 333/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 984.9292 - mse: 984.9292 - mae: 20.0220\n",
      "Epoch 333: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 963.5302 - mse: 963.5302 - mae: 16.9559 - val_loss: 1986.3521 - val_mse: 1986.3521 - val_mae: 21.5148\n",
      "Epoch 334/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 579.5738 - mse: 579.5738 - mae: 12.7335\n",
      "Epoch 334: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 921.1906 - mse: 921.1906 - mae: 16.8736 - val_loss: 1574.1642 - val_mse: 1574.1642 - val_mae: 17.8214\n",
      "Epoch 335/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 349.5990 - mse: 349.5990 - mae: 12.9718\n",
      "Epoch 335: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 943.2696 - mse: 943.2696 - mae: 15.7679 - val_loss: 1533.8983 - val_mse: 1533.8983 - val_mae: 18.5150\n",
      "Epoch 336/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 824.1587 - mse: 824.1587 - mae: 11.3042\n",
      "Epoch 336: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 695.0020 - mse: 695.0020 - mae: 13.6979 - val_loss: 1241.0948 - val_mse: 1241.0948 - val_mae: 17.1187\n",
      "Epoch 337/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 239.7168 - mse: 239.7168 - mae: 10.0450\n",
      "Epoch 337: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 737.5755 - mse: 737.5755 - mae: 15.6323 - val_loss: 1104.9052 - val_mse: 1104.9052 - val_mae: 16.2571\n",
      "Epoch 338/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1224.3500 - mse: 1224.3500 - mae: 19.2181\n",
      "Epoch 338: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 723.7523 - mse: 723.7523 - mae: 15.6898 - val_loss: 1997.3788 - val_mse: 1997.3788 - val_mae: 22.9721\n",
      "Epoch 339/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 402.4944 - mse: 402.4944 - mae: 12.7529\n",
      "Epoch 339: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 661.4534 - mse: 661.4534 - mae: 15.0485 - val_loss: 1437.7974 - val_mse: 1437.7974 - val_mae: 16.7351\n",
      "Epoch 340/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3111.9158 - mse: 3111.9158 - mae: 36.5031\n",
      "Epoch 340: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1051.4292 - mse: 1051.4292 - mae: 18.0850 - val_loss: 1988.9880 - val_mse: 1988.9880 - val_mae: 21.9640\n",
      "Epoch 341/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 480.0376 - mse: 480.0376 - mae: 13.9463\n",
      "Epoch 341: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 582.8939 - mse: 582.8939 - mae: 14.7525 - val_loss: 2563.1792 - val_mse: 2563.1792 - val_mae: 25.2958\n",
      "Epoch 342/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 476.6521 - mse: 476.6521 - mae: 14.2230\n",
      "Epoch 342: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 920.7706 - mse: 920.7706 - mae: 16.3619 - val_loss: 2274.5964 - val_mse: 2274.5964 - val_mae: 20.9792\n",
      "Epoch 343/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1337.5680 - mse: 1337.5680 - mae: 15.6066\n",
      "Epoch 343: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 847.7810 - mse: 847.7810 - mae: 16.2437 - val_loss: 1298.9431 - val_mse: 1298.9431 - val_mae: 15.0101\n",
      "Epoch 344/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1613.0986 - mse: 1613.0986 - mae: 24.0718\n",
      "Epoch 344: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 950.3004 - mse: 950.3004 - mae: 17.8440 - val_loss: 1541.9042 - val_mse: 1541.9041 - val_mae: 20.7050\n",
      "Epoch 345/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 786.1403 - mse: 786.1403 - mae: 17.2468\n",
      "Epoch 345: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 834.3851 - mse: 834.3851 - mae: 16.5807 - val_loss: 1479.9153 - val_mse: 1479.9153 - val_mae: 21.3465\n",
      "Epoch 346/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 638.0735 - mse: 638.0735 - mae: 13.6227\n",
      "Epoch 346: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 463.2279 - mse: 463.2279 - mae: 13.3149 - val_loss: 734.5168 - val_mse: 734.5168 - val_mae: 14.8080\n",
      "Epoch 347/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 601.7913 - mse: 601.7913 - mae: 16.8488\n",
      "Epoch 347: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1004.4140 - mse: 1004.4140 - mae: 17.1960 - val_loss: 1915.0013 - val_mse: 1915.0013 - val_mae: 24.4641\n",
      "Epoch 348/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1692.6217 - mse: 1692.6217 - mae: 23.7678\n",
      "Epoch 348: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 865.6310 - mse: 865.6310 - mae: 17.2731 - val_loss: 2072.3242 - val_mse: 2072.3242 - val_mae: 24.4583\n",
      "Epoch 349/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 545.1879 - mse: 545.1879 - mae: 15.9269\n",
      "Epoch 349: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 919.5979 - mse: 919.5979 - mae: 17.1127 - val_loss: 815.8853 - val_mse: 815.8854 - val_mae: 12.5333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 350/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 810.2344 - mse: 810.2344 - mae: 14.2346\n",
      "Epoch 350: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 741.2611 - mse: 741.2611 - mae: 15.1236 - val_loss: 1703.0861 - val_mse: 1703.0861 - val_mae: 21.5982\n",
      "Epoch 351/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1150.0215 - mse: 1150.0215 - mae: 15.2872\n",
      "Epoch 351: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 680.0201 - mse: 680.0201 - mae: 14.1705 - val_loss: 1036.3783 - val_mse: 1036.3783 - val_mae: 16.5936\n",
      "Epoch 352/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 844.2881 - mse: 844.2881 - mae: 15.9521\n",
      "Epoch 352: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 762.3491 - mse: 762.3491 - mae: 15.9730 - val_loss: 2241.1199 - val_mse: 2241.1199 - val_mae: 25.6613\n",
      "Epoch 353/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 709.1909 - mse: 709.1909 - mae: 18.7609\n",
      "Epoch 353: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 767.8875 - mse: 767.8875 - mae: 16.3358 - val_loss: 1687.2288 - val_mse: 1687.2288 - val_mae: 19.5945\n",
      "Epoch 354/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 767.9472 - mse: 767.9472 - mae: 19.2072\n",
      "Epoch 354: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 506.1656 - mse: 506.1656 - mae: 14.7785 - val_loss: 1433.3770 - val_mse: 1433.3770 - val_mae: 18.1484\n",
      "Epoch 355/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 474.3710 - mse: 474.3710 - mae: 12.9753\n",
      "Epoch 355: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1042.6118 - mse: 1042.6118 - mae: 18.2595 - val_loss: 1703.6504 - val_mse: 1703.6504 - val_mae: 21.0526\n",
      "Epoch 356/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 652.6750 - mse: 652.6750 - mae: 16.3545\n",
      "Epoch 356: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 473.9559 - mse: 473.9559 - mae: 13.5155 - val_loss: 1289.0200 - val_mse: 1289.0200 - val_mae: 17.1001\n",
      "Epoch 357/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 262.2545 - mse: 262.2545 - mae: 9.6165\n",
      "Epoch 357: val_loss did not improve from 707.27264\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 896.2742 - mse: 896.2742 - mae: 15.4917 - val_loss: 1195.1998 - val_mse: 1195.1998 - val_mae: 16.4968\n",
      "Epoch 358/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 420.9969 - mse: 420.9969 - mae: 9.2685\n",
      "Epoch 358: val_loss improved from 707.27264 to 651.15076, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 970.9254 - mse: 970.9254 - mae: 17.6586 - val_loss: 651.1508 - val_mse: 651.1508 - val_mae: 12.8193\n",
      "Epoch 359/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 106.0628 - mse: 106.0628 - mae: 7.6270\n",
      "Epoch 359: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 683.7263 - mse: 683.7263 - mae: 14.8789 - val_loss: 1616.4094 - val_mse: 1616.4094 - val_mae: 23.9722\n",
      "Epoch 360/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 352.8108 - mse: 352.8108 - mae: 12.2257\n",
      "Epoch 360: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 733.3456 - mse: 733.3456 - mae: 15.5710 - val_loss: 1178.2372 - val_mse: 1178.2372 - val_mae: 18.0274\n",
      "Epoch 361/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1058.5800 - mse: 1058.5800 - mae: 17.6566\n",
      "Epoch 361: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 748.8755 - mse: 748.8755 - mae: 15.3811 - val_loss: 733.2272 - val_mse: 733.2272 - val_mae: 13.0412\n",
      "Epoch 362/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 774.9097 - mse: 774.9097 - mae: 17.4825\n",
      "Epoch 362: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 611.7607 - mse: 611.7607 - mae: 14.2461 - val_loss: 1142.2826 - val_mse: 1142.2826 - val_mae: 18.0998\n",
      "Epoch 363/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 288.6517 - mse: 288.6517 - mae: 12.7259\n",
      "Epoch 363: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1130.5341 - mse: 1130.5341 - mae: 16.2628 - val_loss: 1709.8391 - val_mse: 1709.8391 - val_mae: 24.0135\n",
      "Epoch 364/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 671.5649 - mse: 671.5649 - mae: 18.0083\n",
      "Epoch 364: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 953.8384 - mse: 953.8384 - mae: 17.7662 - val_loss: 779.2107 - val_mse: 779.2107 - val_mae: 14.9192\n",
      "Epoch 365/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 858.3500 - mse: 858.3500 - mae: 20.0009\n",
      "Epoch 365: val_loss did not improve from 651.15076\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 819.2718 - mse: 819.2718 - mae: 16.6778 - val_loss: 827.8890 - val_mse: 827.8890 - val_mae: 15.4111\n",
      "Epoch 366/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 595.4186 - mse: 595.4186 - mae: 12.2905\n",
      "Epoch 366: val_loss improved from 651.15076 to 595.87030, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 852.6964 - mse: 852.6964 - mae: 15.6703 - val_loss: 595.8703 - val_mse: 595.8703 - val_mae: 14.5401\n",
      "Epoch 367/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 769.5267 - mse: 769.5267 - mae: 13.8206\n",
      "Epoch 367: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1100.4133 - mse: 1100.4133 - mae: 16.4248 - val_loss: 1189.0800 - val_mse: 1189.0800 - val_mae: 21.3544\n",
      "Epoch 368/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1077.8899 - mse: 1077.8899 - mae: 18.1896\n",
      "Epoch 368: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1224.8821 - mse: 1224.8821 - mae: 19.4252 - val_loss: 1767.2657 - val_mse: 1767.2657 - val_mae: 24.5933\n",
      "Epoch 369/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 314.9179 - mse: 314.9179 - mae: 12.6319\n",
      "Epoch 369: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 654.3409 - mse: 654.3409 - mae: 16.2175 - val_loss: 674.7161 - val_mse: 674.7161 - val_mae: 12.6258\n",
      "Epoch 370/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1612.4250 - mse: 1612.4250 - mae: 22.6250\n",
      "Epoch 370: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 911.0452 - mse: 911.0452 - mae: 16.5206 - val_loss: 1056.1727 - val_mse: 1056.1727 - val_mae: 19.3186\n",
      "Epoch 371/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 479.3569 - mse: 479.3569 - mae: 12.2468\n",
      "Epoch 371: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 525.2580 - mse: 525.2580 - mae: 14.4238 - val_loss: 1497.3597 - val_mse: 1497.3597 - val_mae: 23.2035\n",
      "Epoch 372/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 479.4288 - mse: 479.4288 - mae: 15.4348\n",
      "Epoch 372: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 693.0764 - mse: 693.0764 - mae: 15.6077 - val_loss: 1210.2009 - val_mse: 1210.2009 - val_mae: 19.5676\n",
      "Epoch 373/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 480.1841 - mse: 480.1841 - mae: 14.4021\n",
      "Epoch 373: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 693.5386 - mse: 693.5386 - mae: 15.2303 - val_loss: 1150.1953 - val_mse: 1150.1953 - val_mae: 16.8298\n",
      "Epoch 374/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 938.4699 - mse: 938.4699 - mae: 18.7029\n",
      "Epoch 374: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 737.9705 - mse: 737.9705 - mae: 15.7479 - val_loss: 2188.8608 - val_mse: 2188.8608 - val_mae: 22.7654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 372.1935 - mse: 372.1935 - mae: 12.7852\n",
      "Epoch 375: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 598.2609 - mse: 598.2609 - mae: 14.2955 - val_loss: 2080.6450 - val_mse: 2080.6450 - val_mae: 19.2795\n",
      "Epoch 376/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 458.1721 - mse: 458.1721 - mae: 12.0352\n",
      "Epoch 376: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1114.0116 - mse: 1114.0116 - mae: 17.2968 - val_loss: 846.4291 - val_mse: 846.4291 - val_mae: 13.5800\n",
      "Epoch 377/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3264.5806 - mse: 3264.5806 - mae: 32.6735\n",
      "Epoch 377: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1155.7493 - mse: 1155.7493 - mae: 18.3301 - val_loss: 2521.3965 - val_mse: 2521.3965 - val_mae: 30.9910\n",
      "Epoch 378/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 820.8694 - mse: 820.8694 - mae: 19.0862\n",
      "Epoch 378: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1028.5276 - mse: 1028.5276 - mae: 17.8838 - val_loss: 1399.9938 - val_mse: 1399.9938 - val_mae: 21.2970\n",
      "Epoch 379/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 980.8661 - mse: 980.8661 - mae: 17.8003\n",
      "Epoch 379: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 786.6401 - mse: 786.6401 - mae: 15.6436 - val_loss: 864.1072 - val_mse: 864.1072 - val_mae: 15.4678\n",
      "Epoch 380/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 318.7754 - mse: 318.7754 - mae: 11.6204\n",
      "Epoch 380: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 745.4757 - mse: 745.4757 - mae: 15.6534 - val_loss: 1438.5736 - val_mse: 1438.5736 - val_mae: 20.6620\n",
      "Epoch 381/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 189.3342 - mse: 189.3342 - mae: 9.6893\n",
      "Epoch 381: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 558.0328 - mse: 558.0328 - mae: 14.6743 - val_loss: 1488.4301 - val_mse: 1488.4298 - val_mae: 19.5106\n",
      "Epoch 382/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1157.1711 - mse: 1157.1711 - mae: 19.3079\n",
      "Epoch 382: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 681.2404 - mse: 681.2404 - mae: 15.0937 - val_loss: 939.0325 - val_mse: 939.0325 - val_mae: 12.9793\n",
      "Epoch 383/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 909.8214 - mse: 909.8214 - mae: 18.8397\n",
      "Epoch 383: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1013.4590 - mse: 1013.4590 - mae: 16.5606 - val_loss: 985.9202 - val_mse: 985.9202 - val_mae: 18.1407\n",
      "Epoch 384/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 559.7909 - mse: 559.7909 - mae: 16.2992\n",
      "Epoch 384: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 616.6724 - mse: 616.6724 - mae: 15.4268 - val_loss: 2592.3320 - val_mse: 2592.3320 - val_mae: 30.9269\n",
      "Epoch 385/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1252.5620 - mse: 1252.5620 - mae: 23.0924\n",
      "Epoch 385: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1122.2775 - mse: 1122.2775 - mae: 18.2101 - val_loss: 838.3964 - val_mse: 838.3964 - val_mae: 15.6814\n",
      "Epoch 386/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 599.3457 - mse: 599.3457 - mae: 14.6973\n",
      "Epoch 386: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 752.7441 - mse: 752.7441 - mae: 16.5404 - val_loss: 633.0558 - val_mse: 633.0558 - val_mae: 15.1342\n",
      "Epoch 387/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1143.1013 - mse: 1143.1013 - mae: 21.4035\n",
      "Epoch 387: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 887.9823 - mse: 887.9823 - mae: 18.0336 - val_loss: 1876.9382 - val_mse: 1876.9382 - val_mae: 25.1984\n",
      "Epoch 388/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 158.3561 - mse: 158.3561 - mae: 8.8300\n",
      "Epoch 388: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 978.2811 - mse: 978.2811 - mae: 16.8548 - val_loss: 1048.1567 - val_mse: 1048.1567 - val_mae: 15.2645\n",
      "Epoch 389/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 850.0295 - mse: 850.0295 - mae: 15.2291\n",
      "Epoch 389: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 721.8521 - mse: 721.8521 - mae: 14.7328 - val_loss: 1404.3782 - val_mse: 1404.3782 - val_mae: 17.7497\n",
      "Epoch 390/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 687.1906 - mse: 687.1906 - mae: 18.0289\n",
      "Epoch 390: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 784.6378 - mse: 784.6378 - mae: 15.5188 - val_loss: 950.1694 - val_mse: 950.1694 - val_mae: 15.3382\n",
      "Epoch 391/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 560.8855 - mse: 560.8855 - mae: 16.6507\n",
      "Epoch 391: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 423.7617 - mse: 423.7617 - mae: 13.0983 - val_loss: 1296.5582 - val_mse: 1296.5582 - val_mae: 18.2042\n",
      "Epoch 392/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 468.9191 - mse: 468.9191 - mae: 13.3391\n",
      "Epoch 392: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 704.9987 - mse: 704.9987 - mae: 15.2182 - val_loss: 1136.7588 - val_mse: 1136.7588 - val_mae: 16.0280\n",
      "Epoch 393/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 664.4285 - mse: 664.4285 - mae: 15.8005\n",
      "Epoch 393: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 791.5073 - mse: 791.5073 - mae: 15.0451 - val_loss: 1502.3286 - val_mse: 1502.3286 - val_mae: 19.2733\n",
      "Epoch 394/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1076.4607 - mse: 1076.4607 - mae: 21.1872\n",
      "Epoch 394: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 678.1382 - mse: 678.1382 - mae: 15.4556 - val_loss: 1718.7156 - val_mse: 1718.7156 - val_mae: 21.4520\n",
      "Epoch 395/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 565.5107 - mse: 565.5107 - mae: 12.9061\n",
      "Epoch 395: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 934.2191 - mse: 934.2191 - mae: 15.7923 - val_loss: 718.4073 - val_mse: 718.4073 - val_mae: 12.1690\n",
      "Epoch 396/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 795.7538 - mse: 795.7538 - mae: 12.9603\n",
      "Epoch 396: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1495.8992 - mse: 1495.8992 - mae: 18.3760 - val_loss: 1916.7183 - val_mse: 1916.7183 - val_mae: 22.7767\n",
      "Epoch 397/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 955.5477 - mse: 955.5477 - mae: 20.2238\n",
      "Epoch 397: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1156.2018 - mse: 1156.2018 - mae: 18.0184 - val_loss: 1397.8981 - val_mse: 1397.8981 - val_mae: 16.8812\n",
      "Epoch 398/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 880.0284 - mse: 880.0284 - mae: 15.5249\n",
      "Epoch 398: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 819.6688 - mse: 819.6688 - mae: 15.9283 - val_loss: 931.9990 - val_mse: 931.9990 - val_mae: 14.5391\n",
      "Epoch 399/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1222.4084 - mse: 1222.4084 - mae: 22.3010\n",
      "Epoch 399: val_loss did not improve from 595.87030\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 713.7679 - mse: 713.7679 - mae: 15.1544 - val_loss: 1271.0470 - val_mse: 1271.0470 - val_mae: 20.7384\n",
      "Epoch 400/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 442.6445 - mse: 442.6445 - mae: 13.5947\n",
      "Epoch 400: val_loss did not improve from 595.87030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 927.2587 - mse: 927.2587 - mae: 16.7926 - val_loss: 1369.3890 - val_mse: 1369.3890 - val_mae: 20.7530\n",
      "Epoch 401/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1402.8893 - mse: 1402.8893 - mae: 22.7517\n",
      "Epoch 401: val_loss improved from 595.87030 to 503.76294, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 600.7744 - mse: 600.7744 - mae: 14.2588 - val_loss: 503.7629 - val_mse: 503.7629 - val_mae: 12.2821\n",
      "Epoch 402/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1262.2590 - mse: 1262.2590 - mae: 17.4478\n",
      "Epoch 402: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 827.0158 - mse: 827.0158 - mae: 15.7848 - val_loss: 1934.7877 - val_mse: 1934.7877 - val_mae: 26.0814\n",
      "Epoch 403/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1364.0146 - mse: 1364.0146 - mae: 19.2513\n",
      "Epoch 403: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 763.5469 - mse: 763.5469 - mae: 15.8397 - val_loss: 1396.0798 - val_mse: 1396.0801 - val_mae: 17.7872\n",
      "Epoch 404/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 228.8686 - mse: 228.8686 - mae: 9.3933\n",
      "Epoch 404: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 783.9546 - mse: 783.9546 - mae: 15.4730 - val_loss: 620.1940 - val_mse: 620.1940 - val_mae: 10.2672\n",
      "Epoch 405/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1018.7614 - mse: 1018.7614 - mae: 18.0526\n",
      "Epoch 405: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1222.8442 - mse: 1222.8442 - mae: 17.0220 - val_loss: 1006.8554 - val_mse: 1006.8554 - val_mae: 15.3954\n",
      "Epoch 406/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 330.5785 - mse: 330.5785 - mae: 11.2978\n",
      "Epoch 406: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 927.3751 - mse: 927.3751 - mae: 15.9455 - val_loss: 758.6406 - val_mse: 758.6406 - val_mae: 16.3374\n",
      "Epoch 407/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1134.1106 - mse: 1134.1106 - mae: 15.6792\n",
      "Epoch 407: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 911.7271 - mse: 911.7271 - mae: 16.3095 - val_loss: 1199.2755 - val_mse: 1199.2755 - val_mae: 22.9601\n",
      "Epoch 408/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1254.7793 - mse: 1254.7793 - mae: 18.8645\n",
      "Epoch 408: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 679.9637 - mse: 679.9637 - mae: 14.5508 - val_loss: 931.5401 - val_mse: 931.5401 - val_mae: 19.9751\n",
      "Epoch 409/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 822.7556 - mse: 822.7556 - mae: 17.9415\n",
      "Epoch 409: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 961.9769 - mse: 961.9769 - mae: 16.7897 - val_loss: 1325.8101 - val_mse: 1325.8101 - val_mae: 20.7547\n",
      "Epoch 410/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1452.9080 - mse: 1452.9080 - mae: 20.0507\n",
      "Epoch 410: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 764.8088 - mse: 764.8088 - mae: 15.0510 - val_loss: 1691.8490 - val_mse: 1691.8490 - val_mae: 19.6519\n",
      "Epoch 411/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1235.5205 - mse: 1235.5205 - mae: 17.6394\n",
      "Epoch 411: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 821.9528 - mse: 821.9528 - mae: 15.2949 - val_loss: 952.9594 - val_mse: 952.9594 - val_mae: 13.4525\n",
      "Epoch 412/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 251.7258 - mse: 251.7258 - mae: 10.1418\n",
      "Epoch 412: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 842.6002 - mse: 842.6002 - mae: 15.9138 - val_loss: 1395.7888 - val_mse: 1395.7888 - val_mae: 18.5933\n",
      "Epoch 413/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 364.0250 - mse: 364.0250 - mae: 11.9259\n",
      "Epoch 413: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 931.2810 - mse: 931.2810 - mae: 16.1044 - val_loss: 1836.5061 - val_mse: 1836.5061 - val_mae: 22.8722\n",
      "Epoch 414/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 276.2150 - mse: 276.2150 - mae: 11.5733\n",
      "Epoch 414: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 878.2492 - mse: 878.2492 - mae: 15.6292 - val_loss: 1561.7928 - val_mse: 1561.7928 - val_mae: 19.2034\n",
      "Epoch 415/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 790.2622 - mse: 790.2622 - mae: 19.4235\n",
      "Epoch 415: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 603.1653 - mse: 603.1653 - mae: 15.6328 - val_loss: 1339.9839 - val_mse: 1339.9839 - val_mae: 16.8074\n",
      "Epoch 416/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 897.3331 - mse: 897.3331 - mae: 16.4244\n",
      "Epoch 416: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 768.1413 - mse: 768.1413 - mae: 16.3565 - val_loss: 818.4296 - val_mse: 818.4296 - val_mae: 14.9850\n",
      "Epoch 417/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 685.8878 - mse: 685.8878 - mae: 16.3297\n",
      "Epoch 417: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 426.9340 - mse: 426.9340 - mae: 12.8689 - val_loss: 1116.8986 - val_mse: 1116.8986 - val_mae: 19.4959\n",
      "Epoch 418/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 900.6613 - mse: 900.6613 - mae: 18.2833\n",
      "Epoch 418: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1024.7604 - mse: 1024.7604 - mae: 16.8501 - val_loss: 1186.4761 - val_mse: 1186.4761 - val_mae: 19.1836\n",
      "Epoch 419/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 693.4831 - mse: 693.4831 - mae: 15.0342\n",
      "Epoch 419: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 709.3632 - mse: 709.3632 - mae: 15.1662 - val_loss: 659.7966 - val_mse: 659.7966 - val_mae: 16.0192\n",
      "Epoch 420/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 603.0084 - mse: 603.0084 - mae: 13.7186\n",
      "Epoch 420: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 655.1974 - mse: 655.1974 - mae: 14.7406 - val_loss: 1298.6764 - val_mse: 1298.6764 - val_mae: 21.3087\n",
      "Epoch 421/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 145.3381 - mse: 145.3381 - mae: 8.8157\n",
      "Epoch 421: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 440.5542 - mse: 440.5542 - mae: 12.2860 - val_loss: 880.6827 - val_mse: 880.6827 - val_mae: 16.3897\n",
      "Epoch 422/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1853.6068 - mse: 1853.6068 - mae: 22.3944\n",
      "Epoch 422: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 799.0753 - mse: 799.0753 - mae: 15.6168 - val_loss: 929.4534 - val_mse: 929.4534 - val_mae: 15.4766\n",
      "Epoch 423/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 920.2913 - mse: 920.2913 - mae: 19.0706\n",
      "Epoch 423: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 795.9237 - mse: 795.9237 - mae: 15.7541 - val_loss: 661.8578 - val_mse: 661.8578 - val_mae: 12.9728\n",
      "Epoch 424/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 173.7041 - mse: 173.7041 - mae: 9.2954\n",
      "Epoch 424: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 705.7947 - mse: 705.7947 - mae: 14.7692 - val_loss: 2442.2053 - val_mse: 2442.2053 - val_mae: 27.2521\n",
      "Epoch 425/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 575.7929 - mse: 575.7929 - mae: 15.7610\n",
      "Epoch 425: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 756.5778 - mse: 756.5778 - mae: 15.7429 - val_loss: 1463.8347 - val_mse: 1463.8347 - val_mae: 16.8169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3191.3311 - mse: 3191.3311 - mae: 25.6554\n",
      "Epoch 426: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1148.3400 - mse: 1148.3400 - mae: 16.4959 - val_loss: 1112.9050 - val_mse: 1112.9050 - val_mae: 12.9416\n",
      "Epoch 427/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 478.0977 - mse: 478.0977 - mae: 10.0759\n",
      "Epoch 427: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 575.8963 - mse: 575.8963 - mae: 13.2986 - val_loss: 1212.3730 - val_mse: 1212.3730 - val_mae: 16.4496\n",
      "Epoch 428/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 506.8869 - mse: 506.8869 - mae: 15.8863\n",
      "Epoch 428: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 728.7686 - mse: 728.7686 - mae: 15.0909 - val_loss: 1434.6218 - val_mse: 1434.6217 - val_mae: 19.9046\n",
      "Epoch 429/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 173.4220 - mse: 173.4220 - mae: 8.3362\n",
      "Epoch 429: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 637.4222 - mse: 637.4222 - mae: 14.5248 - val_loss: 1169.1221 - val_mse: 1169.1221 - val_mae: 18.2380\n",
      "Epoch 430/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 257.6020 - mse: 257.6020 - mae: 10.8347\n",
      "Epoch 430: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 997.8602 - mse: 997.8602 - mae: 17.0474 - val_loss: 1664.2175 - val_mse: 1664.2175 - val_mae: 20.3378\n",
      "Epoch 431/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 287.1810 - mse: 287.1810 - mae: 12.1836\n",
      "Epoch 431: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1005.9266 - mse: 1005.9266 - mae: 17.1251 - val_loss: 1428.2156 - val_mse: 1428.2157 - val_mae: 16.6197\n",
      "Epoch 432/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 873.5384 - mse: 873.5384 - mae: 16.0930\n",
      "Epoch 432: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 983.4360 - mse: 983.4360 - mae: 17.0210 - val_loss: 819.1198 - val_mse: 819.1198 - val_mae: 12.8041\n",
      "Epoch 433/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 877.6656 - mse: 877.6656 - mae: 15.3387\n",
      "Epoch 433: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 984.1814 - mse: 984.1814 - mae: 17.5198 - val_loss: 2502.4224 - val_mse: 2502.4224 - val_mae: 30.8581\n",
      "Epoch 434/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1574.9342 - mse: 1574.9342 - mae: 20.4857\n",
      "Epoch 434: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 692.7488 - mse: 692.7488 - mae: 14.1099 - val_loss: 987.3951 - val_mse: 987.3951 - val_mae: 16.6242\n",
      "Epoch 435/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 402.7239 - mse: 402.7239 - mae: 13.2551\n",
      "Epoch 435: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 645.9891 - mse: 645.9891 - mae: 13.5860 - val_loss: 1281.2650 - val_mse: 1281.2650 - val_mae: 17.8502\n",
      "Epoch 436/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 836.1859 - mse: 836.1859 - mae: 17.0766\n",
      "Epoch 436: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 551.1588 - mse: 551.1588 - mae: 14.7614 - val_loss: 1645.3796 - val_mse: 1645.3796 - val_mae: 18.5061\n",
      "Epoch 437/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2025.0330 - mse: 2025.0330 - mae: 18.8560\n",
      "Epoch 437: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1051.9999 - mse: 1051.9999 - mae: 17.2086 - val_loss: 1177.6567 - val_mse: 1177.6567 - val_mae: 14.7980\n",
      "Epoch 438/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 459.5602 - mse: 459.5602 - mae: 14.2573\n",
      "Epoch 438: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 686.5130 - mse: 686.5130 - mae: 15.2295 - val_loss: 1400.0754 - val_mse: 1400.0756 - val_mae: 19.5754\n",
      "Epoch 439/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 378.8389 - mse: 378.8389 - mae: 11.6690\n",
      "Epoch 439: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 452.2579 - mse: 452.2579 - mae: 12.2040 - val_loss: 1375.7177 - val_mse: 1375.7177 - val_mae: 19.5643\n",
      "Epoch 440/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1947.4801 - mse: 1947.4801 - mae: 22.3779\n",
      "Epoch 440: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 922.4767 - mse: 922.4767 - mae: 16.6963 - val_loss: 1106.2672 - val_mse: 1106.2672 - val_mae: 16.6885\n",
      "Epoch 441/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 260.6971 - mse: 260.6971 - mae: 11.7051\n",
      "Epoch 441: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 455.4491 - mse: 455.4491 - mae: 13.7489 - val_loss: 1533.9485 - val_mse: 1533.9485 - val_mae: 21.5653\n",
      "Epoch 442/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 443.1697 - mse: 443.1697 - mae: 13.7483\n",
      "Epoch 442: val_loss did not improve from 503.76294\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 738.6642 - mse: 738.6642 - mae: 15.6891 - val_loss: 1727.9960 - val_mse: 1727.9960 - val_mae: 22.2017\n",
      "Epoch 443/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 538.8870 - mse: 538.8870 - mae: 13.4419\n",
      "Epoch 443: val_loss improved from 503.76294 to 488.89267, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 757.4014 - mse: 757.4014 - mae: 14.0255 - val_loss: 488.8927 - val_mse: 488.8927 - val_mae: 10.5798\n",
      "Epoch 444/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1594.1997 - mse: 1594.1997 - mae: 21.3381\n",
      "Epoch 444: val_loss did not improve from 488.89267\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 931.1077 - mse: 931.1077 - mae: 16.4879 - val_loss: 1669.0350 - val_mse: 1669.0350 - val_mae: 24.1881\n",
      "Epoch 445/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 658.2871 - mse: 658.2871 - mae: 14.5847\n",
      "Epoch 445: val_loss did not improve from 488.89267\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 724.9111 - mse: 724.9111 - mae: 15.8139 - val_loss: 1252.2188 - val_mse: 1252.2188 - val_mae: 20.5150\n",
      "Epoch 446/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 279.7440 - mse: 279.7440 - mae: 11.4338\n",
      "Epoch 446: val_loss improved from 488.89267 to 476.57266, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 734.3821 - mse: 734.3821 - mae: 15.1453 - val_loss: 476.5727 - val_mse: 476.5727 - val_mae: 11.8573\n",
      "Epoch 447/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 3052.5786 - mse: 3052.5786 - mae: 30.3864\n",
      "Epoch 447: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 859.2338 - mse: 859.2338 - mae: 14.6825 - val_loss: 2269.8774 - val_mse: 2269.8774 - val_mae: 26.2727\n",
      "Epoch 448/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 564.1860 - mse: 564.1860 - mae: 14.7087\n",
      "Epoch 448: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 647.5426 - mse: 647.5426 - mae: 14.8501 - val_loss: 1269.9656 - val_mse: 1269.9656 - val_mae: 16.4258\n",
      "Epoch 449/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1056.9221 - mse: 1056.9221 - mae: 16.7794\n",
      "Epoch 449: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 681.8123 - mse: 681.8123 - mae: 15.0160 - val_loss: 553.2274 - val_mse: 553.2274 - val_mae: 10.1500\n",
      "Epoch 450/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1884.8047 - mse: 1884.8047 - mae: 24.2309\n",
      "Epoch 450: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 730.7729 - mse: 730.7729 - mae: 14.4580 - val_loss: 1455.2462 - val_mse: 1455.2462 - val_mae: 22.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 451/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 770.5562 - mse: 770.5562 - mae: 18.0595\n",
      "Epoch 451: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 916.9649 - mse: 916.9649 - mae: 18.6149 - val_loss: 2236.7527 - val_mse: 2236.7527 - val_mae: 26.0097\n",
      "Epoch 452/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1142.6960 - mse: 1142.6960 - mae: 21.2859\n",
      "Epoch 452: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1014.1091 - mse: 1014.1091 - mae: 17.5505 - val_loss: 714.6982 - val_mse: 714.6982 - val_mae: 11.6046\n",
      "Epoch 453/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 429.8785 - mse: 429.8785 - mae: 14.1321\n",
      "Epoch 453: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.9626 - mse: 595.9626 - mae: 14.3981 - val_loss: 1187.0576 - val_mse: 1187.0576 - val_mae: 17.4634\n",
      "Epoch 454/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 526.2997 - mse: 526.2997 - mae: 15.1510\n",
      "Epoch 454: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 886.5748 - mse: 886.5748 - mae: 16.4060 - val_loss: 1705.9578 - val_mse: 1705.9578 - val_mae: 24.1458\n",
      "Epoch 455/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 176.3750 - mse: 176.3750 - mae: 9.5376\n",
      "Epoch 455: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 755.1853 - mse: 755.1853 - mae: 15.8059 - val_loss: 748.5983 - val_mse: 748.5983 - val_mae: 15.3738\n",
      "Epoch 456/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 4342.2700 - mse: 4342.2700 - mae: 31.7966\n",
      "Epoch 456: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1658.1663 - mse: 1658.1663 - mae: 20.0943 - val_loss: 1730.0320 - val_mse: 1730.0320 - val_mae: 22.1122\n",
      "Epoch 457/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 807.4521 - mse: 807.4521 - mae: 15.9794\n",
      "Epoch 457: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 707.9897 - mse: 707.9897 - mae: 15.0419 - val_loss: 1111.0603 - val_mse: 1111.0603 - val_mae: 14.4968\n",
      "Epoch 458/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 479.9800 - mse: 479.9800 - mae: 13.9047\n",
      "Epoch 458: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 576.6455 - mse: 576.6455 - mae: 14.4751 - val_loss: 779.2665 - val_mse: 779.2665 - val_mae: 12.0530\n",
      "Epoch 459/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 559.0870 - mse: 559.0870 - mae: 14.6806\n",
      "Epoch 459: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 778.0401 - mse: 778.0401 - mae: 16.0622 - val_loss: 3052.1777 - val_mse: 3052.1777 - val_mae: 32.2420\n",
      "Epoch 460/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 654.8800 - mse: 654.8800 - mae: 18.0380\n",
      "Epoch 460: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1294.7076 - mse: 1294.7076 - mae: 19.0592 - val_loss: 1178.2739 - val_mse: 1178.2739 - val_mae: 16.3051\n",
      "Epoch 461/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 283.7422 - mse: 283.7422 - mae: 10.7552\n",
      "Epoch 461: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 808.3804 - mse: 808.3804 - mae: 15.7413 - val_loss: 904.6735 - val_mse: 904.6735 - val_mae: 12.9081\n",
      "Epoch 462/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1038.2343 - mse: 1038.2343 - mae: 22.1466\n",
      "Epoch 462: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 817.0151 - mse: 817.0151 - mae: 16.0721 - val_loss: 1744.4186 - val_mse: 1744.4186 - val_mae: 20.5111\n",
      "Epoch 463/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 512.0319 - mse: 512.0319 - mae: 12.4708\n",
      "Epoch 463: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 892.0186 - mse: 892.0186 - mae: 16.0214 - val_loss: 1345.1907 - val_mse: 1345.1907 - val_mae: 18.2661\n",
      "Epoch 464/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 416.8376 - mse: 416.8376 - mae: 11.8103\n",
      "Epoch 464: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 735.3012 - mse: 735.3012 - mae: 15.1807 - val_loss: 845.3680 - val_mse: 845.3680 - val_mae: 13.9299\n",
      "Epoch 465/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 234.5825 - mse: 234.5825 - mae: 10.7600\n",
      "Epoch 465: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 707.7751 - mse: 707.7751 - mae: 15.1504 - val_loss: 572.3969 - val_mse: 572.3969 - val_mae: 11.3494\n",
      "Epoch 466/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 917.2366 - mse: 917.2366 - mae: 19.6274\n",
      "Epoch 466: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1039.1604 - mse: 1039.1604 - mae: 15.3962 - val_loss: 1919.0942 - val_mse: 1919.0942 - val_mae: 24.7266\n",
      "Epoch 467/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 224.9951 - mse: 224.9951 - mae: 11.2176\n",
      "Epoch 467: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 661.3249 - mse: 661.3249 - mae: 14.4372 - val_loss: 782.8792 - val_mse: 782.8792 - val_mae: 15.2831\n",
      "Epoch 468/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 463.0320 - mse: 463.0320 - mae: 12.7515\n",
      "Epoch 468: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1571.1205 - mse: 1571.1205 - mae: 17.6051 - val_loss: 1088.5035 - val_mse: 1088.5035 - val_mae: 16.6930\n",
      "Epoch 469/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 425.2386 - mse: 425.2386 - mae: 11.9225\n",
      "Epoch 469: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 938.8768 - mse: 938.8768 - mae: 16.7390 - val_loss: 3598.6191 - val_mse: 3598.6187 - val_mae: 32.4021\n",
      "Epoch 470/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1283.2269 - mse: 1283.2269 - mae: 20.5425\n",
      "Epoch 470: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1023.9224 - mse: 1023.9224 - mae: 17.7710 - val_loss: 753.4928 - val_mse: 753.4929 - val_mae: 11.3213\n",
      "Epoch 471/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1146.8689 - mse: 1146.8689 - mae: 17.2572\n",
      "Epoch 471: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1186.0604 - mse: 1186.0604 - mae: 18.7365 - val_loss: 499.2033 - val_mse: 499.2033 - val_mae: 10.8237\n",
      "Epoch 472/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1158.7872 - mse: 1158.7872 - mae: 16.5183\n",
      "Epoch 472: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 825.2795 - mse: 825.2795 - mae: 15.6404 - val_loss: 2571.7124 - val_mse: 2571.7124 - val_mae: 30.5241\n",
      "Epoch 473/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1376.8234 - mse: 1376.8234 - mae: 21.2776\n",
      "Epoch 473: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1191.6326 - mse: 1191.6326 - mae: 19.6894 - val_loss: 2741.2869 - val_mse: 2741.2869 - val_mae: 30.0247\n",
      "Epoch 474/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 447.6161 - mse: 447.6161 - mae: 11.8128\n",
      "Epoch 474: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 833.5946 - mse: 833.5946 - mae: 17.0184 - val_loss: 768.3542 - val_mse: 768.3542 - val_mae: 13.1753\n",
      "Epoch 475/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 303.3979 - mse: 303.3979 - mae: 12.1873\n",
      "Epoch 475: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 502.1780 - mse: 502.1780 - mae: 14.0493 - val_loss: 1222.0422 - val_mse: 1222.0422 - val_mae: 17.5698\n",
      "Epoch 476/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 551.8600 - mse: 551.8600 - mae: 18.1127\n",
      "Epoch 476: val_loss did not improve from 476.57266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 553.5020 - mse: 553.5020 - mae: 15.0778 - val_loss: 1100.1913 - val_mse: 1100.1913 - val_mae: 16.2341\n",
      "Epoch 477/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 343.5591 - mse: 343.5591 - mae: 12.0259\n",
      "Epoch 477: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 349.6800 - mse: 349.6800 - mae: 12.4728 - val_loss: 837.4982 - val_mse: 837.4982 - val_mae: 15.0954\n",
      "Epoch 478/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 150.3495 - mse: 150.3495 - mae: 8.3762\n",
      "Epoch 478: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 539.3593 - mse: 539.3593 - mae: 13.9023 - val_loss: 1417.9777 - val_mse: 1417.9777 - val_mae: 21.0485\n",
      "Epoch 479/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 389.2752 - mse: 389.2752 - mae: 14.2446\n",
      "Epoch 479: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 655.2435 - mse: 655.2435 - mae: 14.8186 - val_loss: 720.7587 - val_mse: 720.7586 - val_mae: 12.3746\n",
      "Epoch 480/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 728.6383 - mse: 728.6383 - mae: 13.7571\n",
      "Epoch 480: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 948.3812 - mse: 948.3812 - mae: 16.1756 - val_loss: 731.2056 - val_mse: 731.2056 - val_mae: 14.5657\n",
      "Epoch 481/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1238.6085 - mse: 1238.6085 - mae: 18.1459\n",
      "Epoch 481: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 980.1024 - mse: 980.1024 - mae: 16.6475 - val_loss: 1167.3202 - val_mse: 1167.3202 - val_mae: 21.4943\n",
      "Epoch 482/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 590.5206 - mse: 590.5206 - mae: 13.8936\n",
      "Epoch 482: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 967.1609 - mse: 967.1609 - mae: 17.4350 - val_loss: 2177.2476 - val_mse: 2177.2476 - val_mae: 26.4140\n",
      "Epoch 483/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1471.9680 - mse: 1471.9680 - mae: 24.9510\n",
      "Epoch 483: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 647.3422 - mse: 647.3422 - mae: 14.8895 - val_loss: 587.2830 - val_mse: 587.2830 - val_mae: 10.1048\n",
      "Epoch 484/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 342.3396 - mse: 342.3396 - mae: 12.5636\n",
      "Epoch 484: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 747.4200 - mse: 747.4200 - mae: 16.1814 - val_loss: 1504.3058 - val_mse: 1504.3058 - val_mae: 20.5790\n",
      "Epoch 485/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 757.3666 - mse: 757.3666 - mae: 16.5971\n",
      "Epoch 485: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1293.2911 - mse: 1293.2911 - mae: 18.2793 - val_loss: 2863.6038 - val_mse: 2863.6038 - val_mae: 29.5666\n",
      "Epoch 486/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 752.4485 - mse: 752.4485 - mae: 14.5707\n",
      "Epoch 486: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 601.7097 - mse: 601.7097 - mae: 14.0024 - val_loss: 638.8900 - val_mse: 638.8900 - val_mae: 11.2541\n",
      "Epoch 487/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 884.5214 - mse: 884.5214 - mae: 17.4127\n",
      "Epoch 487: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1489.5135 - mse: 1489.5135 - mae: 17.1851 - val_loss: 2037.7396 - val_mse: 2037.7396 - val_mae: 21.4443\n",
      "Epoch 488/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 350.3311 - mse: 350.3311 - mae: 12.3550\n",
      "Epoch 488: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 784.2064 - mse: 784.2064 - mae: 14.9206 - val_loss: 2632.4587 - val_mse: 2632.4587 - val_mae: 23.2827\n",
      "Epoch 489/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1186.9093 - mse: 1186.9093 - mae: 20.0109\n",
      "Epoch 489: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 672.1624 - mse: 672.1624 - mae: 14.2379 - val_loss: 883.9589 - val_mse: 883.9589 - val_mae: 10.6010\n",
      "Epoch 490/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 951.6393 - mse: 951.6393 - mae: 16.4778\n",
      "Epoch 490: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 715.7747 - mse: 715.7747 - mae: 14.3220 - val_loss: 1796.1172 - val_mse: 1796.1172 - val_mae: 20.7108\n",
      "Epoch 491/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 726.8691 - mse: 726.8691 - mae: 13.4442\n",
      "Epoch 491: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 734.8826 - mse: 734.8826 - mae: 14.6538 - val_loss: 1755.5806 - val_mse: 1755.5806 - val_mae: 19.6932\n",
      "Epoch 492/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 359.3074 - mse: 359.3074 - mae: 12.6609\n",
      "Epoch 492: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 735.3278 - mse: 735.3278 - mae: 14.6846 - val_loss: 1048.4788 - val_mse: 1048.4788 - val_mae: 13.7503\n",
      "Epoch 493/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 953.7671 - mse: 953.7671 - mae: 17.0567\n",
      "Epoch 493: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 719.7490 - mse: 719.7490 - mae: 15.0550 - val_loss: 986.2587 - val_mse: 986.2587 - val_mae: 16.1309\n",
      "Epoch 494/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 613.1160 - mse: 613.1160 - mae: 16.9213\n",
      "Epoch 494: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 488.7827 - mse: 488.7827 - mae: 13.0144 - val_loss: 1315.4127 - val_mse: 1315.4127 - val_mae: 19.8488\n",
      "Epoch 495/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1669.7222 - mse: 1669.7222 - mae: 20.8163\n",
      "Epoch 495: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 887.3883 - mse: 887.3883 - mae: 15.9368 - val_loss: 1796.7719 - val_mse: 1796.7719 - val_mae: 22.5970\n",
      "Epoch 496/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1018.0958 - mse: 1018.0958 - mae: 19.1587\n",
      "Epoch 496: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 660.5031 - mse: 660.5031 - mae: 15.1160 - val_loss: 705.4842 - val_mse: 705.4842 - val_mae: 12.8170\n",
      "Epoch 497/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 788.8756 - mse: 788.8756 - mae: 16.5755\n",
      "Epoch 497: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 755.2640 - mse: 755.2640 - mae: 15.2699 - val_loss: 1478.2567 - val_mse: 1478.2567 - val_mae: 20.0571\n",
      "Epoch 498/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1504.3143 - mse: 1504.3143 - mae: 18.1514\n",
      "Epoch 498: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 973.7077 - mse: 973.7077 - mae: 16.1782 - val_loss: 1435.8583 - val_mse: 1435.8583 - val_mae: 20.2736\n",
      "Epoch 499/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 229.5303 - mse: 229.5303 - mae: 10.5119\n",
      "Epoch 499: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 677.7756 - mse: 677.7756 - mae: 15.0316 - val_loss: 1014.6769 - val_mse: 1014.6769 - val_mae: 15.5877\n",
      "Epoch 500/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 549.8779 - mse: 549.8779 - mae: 13.2310\n",
      "Epoch 500: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 441.3715 - mse: 441.3715 - mae: 12.9394 - val_loss: 2155.8342 - val_mse: 2155.8342 - val_mae: 24.1459\n",
      "Epoch 501/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 801.4718 - mse: 801.4718 - mae: 17.5151\n",
      "Epoch 501: val_loss did not improve from 476.57266\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1303.0441 - mse: 1303.0441 - mae: 17.1782 - val_loss: 894.7490 - val_mse: 894.7490 - val_mae: 11.0342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 502/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 312.6278 - mse: 312.6278 - mae: 10.9992\n",
      "Epoch 502: val_loss improved from 476.57266 to 467.16498, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 921.6826 - mse: 921.6826 - mae: 15.4363 - val_loss: 467.1650 - val_mse: 467.1650 - val_mae: 11.1263\n",
      "Epoch 503/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 404.0462 - mse: 404.0462 - mae: 12.3552\n",
      "Epoch 503: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 807.2524 - mse: 807.2524 - mae: 15.4501 - val_loss: 2595.3708 - val_mse: 2595.3708 - val_mae: 30.4863\n",
      "Epoch 504/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1589.3604 - mse: 1589.3604 - mae: 22.4631\n",
      "Epoch 504: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 793.1483 - mse: 793.1483 - mae: 16.9872 - val_loss: 1765.7109 - val_mse: 1765.7109 - val_mae: 23.4587\n",
      "Epoch 505/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 594.2528 - mse: 594.2528 - mae: 17.0366\n",
      "Epoch 505: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 553.7729 - mse: 553.7729 - mae: 14.9387 - val_loss: 1196.1708 - val_mse: 1196.1708 - val_mae: 15.0425\n",
      "Epoch 506/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 839.9667 - mse: 839.9667 - mae: 16.3604\n",
      "Epoch 506: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 522.0355 - mse: 522.0355 - mae: 13.8610 - val_loss: 1903.3142 - val_mse: 1903.3142 - val_mae: 20.9258\n",
      "Epoch 507/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 578.6303 - mse: 578.6303 - mae: 14.9727\n",
      "Epoch 507: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 704.0004 - mse: 704.0004 - mae: 14.0687 - val_loss: 1460.1155 - val_mse: 1460.1155 - val_mae: 16.2962\n",
      "Epoch 508/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 204.7449 - mse: 204.7449 - mae: 9.5345\n",
      "Epoch 508: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 497.9781 - mse: 497.9781 - mae: 13.2589 - val_loss: 937.4221 - val_mse: 937.4221 - val_mae: 12.1719\n",
      "Epoch 509/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 522.2357 - mse: 522.2357 - mae: 15.8269\n",
      "Epoch 509: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 828.0264 - mse: 828.0264 - mae: 14.7944 - val_loss: 2102.1875 - val_mse: 2102.1875 - val_mae: 23.7509\n",
      "Epoch 510/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 398.1135 - mse: 398.1135 - mae: 13.3489\n",
      "Epoch 510: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1036.1404 - mse: 1036.1404 - mae: 16.3475 - val_loss: 831.8187 - val_mse: 831.8187 - val_mae: 13.3649\n",
      "Epoch 511/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 623.9211 - mse: 623.9211 - mae: 14.7152\n",
      "Epoch 511: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 931.1786 - mse: 931.1786 - mae: 16.3469 - val_loss: 722.4385 - val_mse: 722.4385 - val_mae: 13.3557\n",
      "Epoch 512/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 338.1038 - mse: 338.1038 - mae: 11.3295\n",
      "Epoch 512: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 663.8964 - mse: 663.8964 - mae: 14.3412 - val_loss: 1334.1028 - val_mse: 1334.1028 - val_mae: 20.9610\n",
      "Epoch 513/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 441.2899 - mse: 441.2899 - mae: 12.9659\n",
      "Epoch 513: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 484.4932 - mse: 484.4932 - mae: 12.9366 - val_loss: 674.0790 - val_mse: 674.0790 - val_mae: 13.8423\n",
      "Epoch 514/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 779.6584 - mse: 779.6584 - mae: 14.7257\n",
      "Epoch 514: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 826.0368 - mse: 826.0368 - mae: 15.5272 - val_loss: 1473.7559 - val_mse: 1473.7559 - val_mae: 21.3204\n",
      "Epoch 515/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2716.2383 - mse: 2716.2383 - mae: 31.1859\n",
      "Epoch 515: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 887.6063 - mse: 887.6063 - mae: 16.0999 - val_loss: 1437.2708 - val_mse: 1437.2708 - val_mae: 18.4493\n",
      "Epoch 516/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1125.2476 - mse: 1125.2476 - mae: 18.8082\n",
      "Epoch 516: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 722.4503 - mse: 722.4503 - mae: 14.2726 - val_loss: 1579.9510 - val_mse: 1579.9509 - val_mae: 17.9108\n",
      "Epoch 517/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 422.2989 - mse: 422.2989 - mae: 14.0334\n",
      "Epoch 517: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 595.7817 - mse: 595.7817 - mae: 14.8047 - val_loss: 1812.8458 - val_mse: 1812.8458 - val_mae: 19.1975\n",
      "Epoch 518/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 129.7446 - mse: 129.7446 - mae: 8.0207\n",
      "Epoch 518: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 457.5703 - mse: 457.5703 - mae: 12.8570 - val_loss: 1721.1780 - val_mse: 1721.1780 - val_mae: 17.7388\n",
      "Epoch 519/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1084.2240 - mse: 1084.2240 - mae: 20.9518\n",
      "Epoch 519: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 583.8761 - mse: 583.8761 - mae: 13.8406 - val_loss: 1822.5034 - val_mse: 1822.5034 - val_mae: 20.2387\n",
      "Epoch 520/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1039.8455 - mse: 1039.8455 - mae: 15.7376\n",
      "Epoch 520: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 717.3058 - mse: 717.3058 - mae: 14.8117 - val_loss: 2179.9790 - val_mse: 2179.9790 - val_mae: 25.5062\n",
      "Epoch 521/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2397.7891 - mse: 2397.7891 - mae: 26.2593\n",
      "Epoch 521: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 894.0658 - mse: 894.0658 - mae: 15.5022 - val_loss: 960.0035 - val_mse: 960.0035 - val_mae: 15.4389\n",
      "Epoch 522/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 420.5269 - mse: 420.5269 - mae: 14.3270\n",
      "Epoch 522: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1037.1743 - mse: 1037.1743 - mae: 15.5221 - val_loss: 1228.7159 - val_mse: 1228.7159 - val_mae: 18.6015\n",
      "Epoch 523/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 473.2282 - mse: 473.2282 - mae: 15.3083\n",
      "Epoch 523: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 607.2455 - mse: 607.2455 - mae: 14.4649 - val_loss: 2162.2058 - val_mse: 2162.2058 - val_mae: 27.1052\n",
      "Epoch 524/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 875.2197 - mse: 875.2197 - mae: 16.3562\n",
      "Epoch 524: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 722.1463 - mse: 722.1463 - mae: 14.4323 - val_loss: 1122.7643 - val_mse: 1122.7643 - val_mae: 16.8944\n",
      "Epoch 525/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 452.6859 - mse: 452.6859 - mae: 12.1545\n",
      "Epoch 525: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 642.8329 - mse: 642.8329 - mae: 14.0619 - val_loss: 1377.5784 - val_mse: 1377.5784 - val_mae: 17.9042\n",
      "Epoch 526/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1867.1770 - mse: 1867.1770 - mae: 23.3987\n",
      "Epoch 526: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 770.4969 - mse: 770.4969 - mae: 15.6824 - val_loss: 1446.7191 - val_mse: 1446.7191 - val_mae: 17.7851\n",
      "Epoch 527/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 551.4275 - mse: 551.4275 - mae: 13.5078\n",
      "Epoch 527: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 690.5385 - mse: 690.5385 - mae: 14.3603 - val_loss: 1607.0077 - val_mse: 1607.0077 - val_mae: 18.3269\n",
      "Epoch 528/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 269.7633 - mse: 269.7633 - mae: 11.4823\n",
      "Epoch 528: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 485.3835 - mse: 485.3835 - mae: 13.3817 - val_loss: 996.2084 - val_mse: 996.2084 - val_mae: 13.6963\n",
      "Epoch 529/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 137.4386 - mse: 137.4386 - mae: 8.2529\n",
      "Epoch 529: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 783.2947 - mse: 783.2947 - mae: 14.6566 - val_loss: 1632.2030 - val_mse: 1632.2030 - val_mae: 21.5794\n",
      "Epoch 530/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1511.6870 - mse: 1511.6870 - mae: 18.8293\n",
      "Epoch 530: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 688.1077 - mse: 688.1077 - mae: 14.0391 - val_loss: 2089.5117 - val_mse: 2089.5117 - val_mae: 24.8418\n",
      "Epoch 531/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1078.7615 - mse: 1078.7615 - mae: 17.7878\n",
      "Epoch 531: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 580.7455 - mse: 580.7455 - mae: 13.2861 - val_loss: 745.1061 - val_mse: 745.1061 - val_mae: 11.3285\n",
      "Epoch 532/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1063.3304 - mse: 1063.3304 - mae: 18.1595\n",
      "Epoch 532: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 534.0540 - mse: 534.0540 - mae: 13.3576 - val_loss: 1678.0486 - val_mse: 1678.0486 - val_mae: 20.2699\n",
      "Epoch 533/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 423.8654 - mse: 423.8654 - mae: 13.2908\n",
      "Epoch 533: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 611.5095 - mse: 611.5095 - mae: 14.0945 - val_loss: 1375.6377 - val_mse: 1375.6377 - val_mae: 17.2520\n",
      "Epoch 534/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 645.9935 - mse: 645.9935 - mae: 13.9592\n",
      "Epoch 534: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 660.4897 - mse: 660.4897 - mae: 15.3510 - val_loss: 1074.0364 - val_mse: 1074.0364 - val_mae: 14.1983\n",
      "Epoch 535/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 457.8541 - mse: 457.8541 - mae: 12.8564\n",
      "Epoch 535: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 815.3327 - mse: 815.3327 - mae: 15.1559 - val_loss: 963.0471 - val_mse: 963.0471 - val_mae: 15.3890\n",
      "Epoch 536/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 847.8112 - mse: 847.8112 - mae: 16.6961\n",
      "Epoch 536: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 540.6547 - mse: 540.6547 - mae: 12.6991 - val_loss: 1115.1570 - val_mse: 1115.1570 - val_mae: 17.9254\n",
      "Epoch 537/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 340.4012 - mse: 340.4012 - mae: 10.3005\n",
      "Epoch 537: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 467.9502 - mse: 467.9502 - mae: 12.7220 - val_loss: 1673.3458 - val_mse: 1673.3457 - val_mae: 22.3134\n",
      "Epoch 538/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 327.8528 - mse: 327.8528 - mae: 9.2643\n",
      "Epoch 538: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 931.7537 - mse: 931.7537 - mae: 15.9612 - val_loss: 1284.3943 - val_mse: 1284.3943 - val_mae: 18.3931\n",
      "Epoch 539/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 624.7460 - mse: 624.7460 - mae: 13.0499\n",
      "Epoch 539: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 676.0375 - mse: 676.0375 - mae: 14.2776 - val_loss: 954.8991 - val_mse: 954.8991 - val_mae: 14.1224\n",
      "Epoch 540/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 419.1734 - mse: 419.1734 - mae: 12.4884\n",
      "Epoch 540: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 660.1837 - mse: 660.1837 - mae: 14.3097 - val_loss: 744.7516 - val_mse: 744.7516 - val_mae: 12.6190\n",
      "Epoch 541/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2346.2029 - mse: 2346.2029 - mae: 18.8937\n",
      "Epoch 541: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 930.7224 - mse: 930.7224 - mae: 15.8520 - val_loss: 1948.9415 - val_mse: 1948.9415 - val_mae: 23.0522\n",
      "Epoch 542/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 743.1946 - mse: 743.1946 - mae: 18.8636\n",
      "Epoch 542: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 711.1526 - mse: 711.1526 - mae: 15.9617 - val_loss: 1583.5630 - val_mse: 1583.5630 - val_mae: 18.9762\n",
      "Epoch 543/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 804.8284 - mse: 804.8284 - mae: 16.3233\n",
      "Epoch 543: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1332.2300 - mse: 1332.2300 - mae: 17.2798 - val_loss: 1189.3468 - val_mse: 1189.3468 - val_mae: 14.5540\n",
      "Epoch 544/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 652.2106 - mse: 652.2106 - mae: 15.9905\n",
      "Epoch 544: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 802.9130 - mse: 802.9130 - mae: 16.2909 - val_loss: 1918.4536 - val_mse: 1918.4536 - val_mae: 20.7350\n",
      "Epoch 545/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 351.6456 - mse: 351.6456 - mae: 11.2478\n",
      "Epoch 545: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 555.8881 - mse: 555.8881 - mae: 14.2467 - val_loss: 1172.2573 - val_mse: 1172.2573 - val_mae: 13.3107\n",
      "Epoch 546/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 658.1066 - mse: 658.1066 - mae: 11.3252\n",
      "Epoch 546: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 700.4042 - mse: 700.4042 - mae: 14.8277 - val_loss: 2423.0562 - val_mse: 2423.0562 - val_mae: 21.5681\n",
      "Epoch 547/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2280.4897 - mse: 2280.4897 - mae: 29.1594\n",
      "Epoch 547: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 751.1735 - mse: 751.1735 - mae: 15.3810 - val_loss: 1512.6132 - val_mse: 1512.6132 - val_mae: 13.7796\n",
      "Epoch 548/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 359.8168 - mse: 359.8168 - mae: 10.8943\n",
      "Epoch 548: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 799.8874 - mse: 799.8874 - mae: 15.1325 - val_loss: 1973.6899 - val_mse: 1973.6899 - val_mae: 19.1759\n",
      "Epoch 549/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1504.3234 - mse: 1504.3234 - mae: 17.1529\n",
      "Epoch 549: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 623.8927 - mse: 623.8927 - mae: 13.2368 - val_loss: 1116.3230 - val_mse: 1116.3230 - val_mae: 16.2818\n",
      "Epoch 550/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 596.9948 - mse: 596.9948 - mae: 12.9262\n",
      "Epoch 550: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1039.3112 - mse: 1039.3112 - mae: 16.3528 - val_loss: 2634.3826 - val_mse: 2634.3826 - val_mae: 27.8506\n",
      "Epoch 551/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 515.9814 - mse: 515.9814 - mae: 12.7138\n",
      "Epoch 551: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 760.7164 - mse: 760.7164 - mae: 16.0955 - val_loss: 2589.0857 - val_mse: 2589.0857 - val_mae: 24.1173\n",
      "Epoch 552/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 418.8820 - mse: 418.8820 - mae: 14.8033\n",
      "Epoch 552: val_loss did not improve from 467.16498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 866.5704 - mse: 866.5704 - mae: 15.5716 - val_loss: 1060.9236 - val_mse: 1060.9236 - val_mae: 12.7060\n",
      "Epoch 553/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 761.5322 - mse: 761.5322 - mae: 16.5886\n",
      "Epoch 553: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 832.7305 - mse: 832.7305 - mae: 17.1668 - val_loss: 871.0514 - val_mse: 871.0514 - val_mae: 11.4077\n",
      "Epoch 554/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 388.4362 - mse: 388.4362 - mae: 12.2210\n",
      "Epoch 554: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1107.3096 - mse: 1107.3096 - mae: 17.1136 - val_loss: 2244.4998 - val_mse: 2244.4998 - val_mae: 26.6946\n",
      "Epoch 555/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1048.3396 - mse: 1048.3396 - mae: 20.1088\n",
      "Epoch 555: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1061.0862 - mse: 1061.0862 - mae: 17.6909 - val_loss: 1421.1294 - val_mse: 1421.1294 - val_mae: 20.8271\n",
      "Epoch 556/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1403.5181 - mse: 1403.5181 - mae: 17.3816\n",
      "Epoch 556: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 728.5082 - mse: 728.5082 - mae: 14.9497 - val_loss: 766.6865 - val_mse: 766.6865 - val_mae: 12.8753\n",
      "Epoch 557/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 785.4895 - mse: 785.4895 - mae: 18.1505\n",
      "Epoch 557: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 501.3117 - mse: 501.3117 - mae: 13.7599 - val_loss: 1333.5974 - val_mse: 1333.5974 - val_mae: 18.3440\n",
      "Epoch 558/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 265.5804 - mse: 265.5804 - mae: 10.7178\n",
      "Epoch 558: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 684.8805 - mse: 684.8805 - mae: 14.3519 - val_loss: 1354.8599 - val_mse: 1354.8599 - val_mae: 18.1132\n",
      "Epoch 559/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 484.4962 - mse: 484.4962 - mae: 11.2791\n",
      "Epoch 559: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 502.5039 - mse: 502.5039 - mae: 13.1002 - val_loss: 959.6655 - val_mse: 959.6655 - val_mae: 13.3602\n",
      "Epoch 560/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 805.3456 - mse: 805.3456 - mae: 15.0481\n",
      "Epoch 560: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 742.2762 - mse: 742.2762 - mae: 15.8554 - val_loss: 2085.0361 - val_mse: 2085.0361 - val_mae: 24.3786\n",
      "Epoch 561/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 348.8917 - mse: 348.8917 - mae: 11.8772\n",
      "Epoch 561: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 828.1487 - mse: 828.1487 - mae: 14.1008 - val_loss: 1105.3055 - val_mse: 1105.3055 - val_mae: 16.9757\n",
      "Epoch 562/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1075.8286 - mse: 1075.8286 - mae: 19.0796\n",
      "Epoch 562: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 757.4951 - mse: 757.4951 - mae: 13.7749 - val_loss: 2058.4893 - val_mse: 2058.4893 - val_mae: 23.2615\n",
      "Epoch 563/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 134.8226 - mse: 134.8226 - mae: 8.0077\n",
      "Epoch 563: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 889.9432 - mse: 889.9432 - mae: 15.4197 - val_loss: 2723.4167 - val_mse: 2723.4167 - val_mae: 24.7135\n",
      "Epoch 564/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 794.4747 - mse: 794.4747 - mae: 17.5964\n",
      "Epoch 564: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.0706 - mse: 595.0706 - mae: 14.0867 - val_loss: 1083.6208 - val_mse: 1083.6208 - val_mae: 12.7011\n",
      "Epoch 565/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 669.3438 - mse: 669.3438 - mae: 15.5097\n",
      "Epoch 565: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 699.2667 - mse: 699.2667 - mae: 15.2025 - val_loss: 1454.6487 - val_mse: 1454.6487 - val_mae: 16.7902\n",
      "Epoch 566/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 297.9298 - mse: 297.9298 - mae: 12.2758\n",
      "Epoch 566: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 616.5304 - mse: 616.5304 - mae: 15.0924 - val_loss: 2053.7844 - val_mse: 2053.7844 - val_mae: 23.6147\n",
      "Epoch 567/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1531.1370 - mse: 1531.1370 - mae: 18.2558\n",
      "Epoch 567: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 769.5032 - mse: 769.5032 - mae: 15.0803 - val_loss: 916.4799 - val_mse: 916.4799 - val_mae: 13.2309\n",
      "Epoch 568/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 833.4941 - mse: 833.4941 - mae: 19.0009\n",
      "Epoch 568: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 640.8994 - mse: 640.8994 - mae: 14.9262 - val_loss: 1019.3178 - val_mse: 1019.3178 - val_mae: 17.1863\n",
      "Epoch 569/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 386.1855 - mse: 386.1855 - mae: 11.6231\n",
      "Epoch 569: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1052.3854 - mse: 1052.3854 - mae: 15.3541 - val_loss: 1991.5491 - val_mse: 1991.5491 - val_mae: 26.4361\n",
      "Epoch 570/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 792.4958 - mse: 792.4958 - mae: 15.2762\n",
      "Epoch 570: val_loss did not improve from 467.16498\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1426.2355 - mse: 1426.2355 - mae: 18.6745 - val_loss: 984.9492 - val_mse: 984.9492 - val_mae: 16.2879\n",
      "Epoch 571/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 536.3674 - mse: 536.3674 - mae: 13.1327\n",
      "Epoch 571: val_loss improved from 467.16498 to 419.97427, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 676.3146 - mse: 676.3146 - mae: 13.9877 - val_loss: 419.9743 - val_mse: 419.9743 - val_mae: 11.1143\n",
      "Epoch 572/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 308.6496 - mse: 308.6496 - mae: 11.1869\n",
      "Epoch 572: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1223.1199 - mse: 1223.1199 - mae: 16.6059 - val_loss: 2493.3303 - val_mse: 2493.3303 - val_mae: 28.6333\n",
      "Epoch 573/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1001.8514 - mse: 1001.8514 - mae: 16.4309\n",
      "Epoch 573: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 992.5438 - mse: 992.5438 - mae: 16.8852 - val_loss: 1904.1086 - val_mse: 1904.1086 - val_mae: 20.8559\n",
      "Epoch 574/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2008.3508 - mse: 2008.3508 - mae: 19.4296\n",
      "Epoch 574: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1004.7307 - mse: 1004.7307 - mae: 16.8186 - val_loss: 809.8902 - val_mse: 809.8902 - val_mae: 12.5444\n",
      "Epoch 575/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2087.1335 - mse: 2087.1335 - mae: 23.2874\n",
      "Epoch 575: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 785.9207 - mse: 785.9207 - mae: 14.4013 - val_loss: 1449.1682 - val_mse: 1449.1682 - val_mae: 18.9097\n",
      "Epoch 576/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 317.2145 - mse: 317.2145 - mae: 13.6809\n",
      "Epoch 576: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 725.5929 - mse: 725.5929 - mae: 15.3085 - val_loss: 1145.3766 - val_mse: 1145.3766 - val_mae: 16.9998\n",
      "Epoch 577/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 341.8595 - mse: 341.8595 - mae: 13.3772\n",
      "Epoch 577: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 695.1517 - mse: 695.1517 - mae: 15.5659 - val_loss: 1545.8005 - val_mse: 1545.8005 - val_mae: 19.8503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 578/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 575.9861 - mse: 575.9861 - mae: 13.8366\n",
      "Epoch 578: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 691.0693 - mse: 691.0693 - mae: 14.9389 - val_loss: 712.9758 - val_mse: 712.9758 - val_mae: 11.2236\n",
      "Epoch 579/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 127.7761 - mse: 127.7761 - mae: 8.4525\n",
      "Epoch 579: val_loss did not improve from 419.97427\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 744.2316 - mse: 744.2316 - mae: 13.8000 - val_loss: 1721.4602 - val_mse: 1721.4602 - val_mae: 22.8629\n",
      "Epoch 580/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2611.7744 - mse: 2611.7744 - mae: 22.4446\n",
      "Epoch 580: val_loss improved from 419.97427 to 416.96744, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 972.3123 - mse: 972.3123 - mae: 15.3677 - val_loss: 416.9674 - val_mse: 416.9674 - val_mae: 11.4722\n",
      "Epoch 581/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1019.7606 - mse: 1019.7606 - mae: 19.6601\n",
      "Epoch 581: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 787.7334 - mse: 787.7334 - mae: 15.8833 - val_loss: 552.1550 - val_mse: 552.1550 - val_mae: 14.8668\n",
      "Epoch 582/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1310.4308 - mse: 1310.4308 - mae: 16.4223\n",
      "Epoch 582: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 752.7586 - mse: 752.7586 - mae: 15.2449 - val_loss: 1838.7667 - val_mse: 1838.7667 - val_mae: 25.1479\n",
      "Epoch 583/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 755.8722 - mse: 755.8722 - mae: 17.5395\n",
      "Epoch 583: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 796.2853 - mse: 796.2853 - mae: 15.3266 - val_loss: 907.3984 - val_mse: 907.3984 - val_mae: 14.7318\n",
      "Epoch 584/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 504.8951 - mse: 504.8951 - mae: 13.8069\n",
      "Epoch 584: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 903.3120 - mse: 903.3120 - mae: 16.3461 - val_loss: 631.2221 - val_mse: 631.2221 - val_mae: 11.3834\n",
      "Epoch 585/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 574.5115 - mse: 574.5115 - mae: 16.5200\n",
      "Epoch 585: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 500.4227 - mse: 500.4227 - mae: 13.3688 - val_loss: 1756.6361 - val_mse: 1756.6361 - val_mae: 21.9183\n",
      "Epoch 586/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 312.7507 - mse: 312.7507 - mae: 11.6209\n",
      "Epoch 586: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 942.1898 - mse: 942.1898 - mae: 16.1611 - val_loss: 1339.0546 - val_mse: 1339.0546 - val_mae: 17.6162\n",
      "Epoch 587/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 348.7321 - mse: 348.7321 - mae: 12.8038\n",
      "Epoch 587: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 580.5328 - mse: 580.5328 - mae: 14.6297 - val_loss: 891.0652 - val_mse: 891.0652 - val_mae: 12.4260\n",
      "Epoch 588/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 273.3148 - mse: 273.3148 - mae: 12.1093\n",
      "Epoch 588: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 866.1703 - mse: 866.1703 - mae: 15.3493 - val_loss: 1564.0521 - val_mse: 1564.0521 - val_mae: 18.4620\n",
      "Epoch 589/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 293.9079 - mse: 293.9079 - mae: 10.9776\n",
      "Epoch 589: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 653.0731 - mse: 653.0731 - mae: 14.8555 - val_loss: 919.4689 - val_mse: 919.4689 - val_mae: 13.1362\n",
      "Epoch 590/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 871.8557 - mse: 871.8557 - mae: 17.7668\n",
      "Epoch 590: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 682.0792 - mse: 682.0792 - mae: 14.8352 - val_loss: 1127.2604 - val_mse: 1127.2604 - val_mae: 16.5625\n",
      "Epoch 591/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2278.6694 - mse: 2278.6694 - mae: 18.4987\n",
      "Epoch 591: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 928.4712 - mse: 928.4712 - mae: 14.9956 - val_loss: 1172.0945 - val_mse: 1172.0945 - val_mae: 19.6790\n",
      "Epoch 592/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 838.7438 - mse: 838.7438 - mae: 16.5113\n",
      "Epoch 592: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 728.1696 - mse: 728.1696 - mae: 15.5744 - val_loss: 1392.6774 - val_mse: 1392.6774 - val_mae: 19.6263\n",
      "Epoch 593/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 540.2578 - mse: 540.2578 - mae: 14.4907\n",
      "Epoch 593: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 460.0552 - mse: 460.0552 - mae: 13.4809 - val_loss: 1345.7867 - val_mse: 1345.7867 - val_mae: 15.8371\n",
      "Epoch 594/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 479.2859 - mse: 479.2859 - mae: 12.9561\n",
      "Epoch 594: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 293.7435 - mse: 293.7435 - mae: 11.1731 - val_loss: 1317.7798 - val_mse: 1317.7798 - val_mae: 13.1087\n",
      "Epoch 595/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 338.1939 - mse: 338.1939 - mae: 11.3302\n",
      "Epoch 595: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 774.3234 - mse: 774.3234 - mae: 13.9193 - val_loss: 1872.3844 - val_mse: 1872.3844 - val_mae: 18.5753\n",
      "Epoch 596/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 203.0988 - mse: 203.0988 - mae: 10.5303\n",
      "Epoch 596: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 597.7145 - mse: 597.7145 - mae: 14.1345 - val_loss: 1269.8250 - val_mse: 1269.8250 - val_mae: 16.3601\n",
      "Epoch 597/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 723.0624 - mse: 723.0624 - mae: 18.3598\n",
      "Epoch 597: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1171.9845 - mse: 1171.9845 - mae: 18.1842 - val_loss: 2003.5326 - val_mse: 2003.5326 - val_mae: 24.8513\n",
      "Epoch 598/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 397.6741 - mse: 397.6741 - mae: 12.2756\n",
      "Epoch 598: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 886.8010 - mse: 886.8010 - mae: 15.4496 - val_loss: 1215.4349 - val_mse: 1215.4349 - val_mae: 18.7769\n",
      "Epoch 599/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 200.0807 - mse: 200.0807 - mae: 7.8887\n",
      "Epoch 599: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 721.4333 - mse: 721.4333 - mae: 13.8629 - val_loss: 702.2549 - val_mse: 702.2549 - val_mae: 13.5549\n",
      "Epoch 600/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 794.5726 - mse: 794.5726 - mae: 13.4438\n",
      "Epoch 600: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 689.8197 - mse: 689.8197 - mae: 14.6348 - val_loss: 1481.6050 - val_mse: 1481.6049 - val_mae: 22.2218\n",
      "Epoch 601/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1806.3433 - mse: 1806.3433 - mae: 22.6006\n",
      "Epoch 601: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 653.2121 - mse: 653.2121 - mae: 13.9821 - val_loss: 1484.0996 - val_mse: 1484.0995 - val_mae: 21.3258\n",
      "Epoch 602/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 483.2999 - mse: 483.2999 - mae: 14.3097\n",
      "Epoch 602: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 548.2537 - mse: 548.2537 - mae: 13.0621 - val_loss: 654.4852 - val_mse: 654.4852 - val_mae: 11.4274\n",
      "Epoch 603/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 369.9287 - mse: 369.9287 - mae: 12.6946\n",
      "Epoch 603: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 508.2009 - mse: 508.2009 - mae: 12.5498 - val_loss: 1232.6370 - val_mse: 1232.6370 - val_mae: 18.1993\n",
      "Epoch 604/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 262.2377 - mse: 262.2377 - mae: 11.0508\n",
      "Epoch 604: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 754.8553 - mse: 754.8553 - mae: 15.1833 - val_loss: 985.4769 - val_mse: 985.4769 - val_mae: 16.0421\n",
      "Epoch 605/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 357.0338 - mse: 357.0338 - mae: 13.3532\n",
      "Epoch 605: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 864.4231 - mse: 864.4231 - mae: 14.6676 - val_loss: 993.9029 - val_mse: 993.9029 - val_mae: 15.7882\n",
      "Epoch 606/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 499.1424 - mse: 499.1424 - mae: 12.8784\n",
      "Epoch 606: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 660.4929 - mse: 660.4929 - mae: 15.5951 - val_loss: 1279.9921 - val_mse: 1279.9921 - val_mae: 19.1064\n",
      "Epoch 607/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1244.3422 - mse: 1244.3422 - mae: 15.1530\n",
      "Epoch 607: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 625.4636 - mse: 625.4636 - mae: 13.2520 - val_loss: 947.3670 - val_mse: 947.3670 - val_mae: 16.8826\n",
      "Epoch 608/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 841.0345 - mse: 841.0345 - mae: 16.8699\n",
      "Epoch 608: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 834.7299 - mse: 834.7299 - mae: 15.3899 - val_loss: 1353.8230 - val_mse: 1353.8230 - val_mae: 20.0654\n",
      "Epoch 609/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 356.0487 - mse: 356.0487 - mae: 12.1398\n",
      "Epoch 609: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 470.3409 - mse: 470.3409 - mae: 13.1330 - val_loss: 1332.5724 - val_mse: 1332.5724 - val_mae: 17.8944\n",
      "Epoch 610/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 341.3297 - mse: 341.3297 - mae: 12.0015\n",
      "Epoch 610: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 664.1998 - mse: 664.1998 - mae: 16.1556 - val_loss: 621.1039 - val_mse: 621.1039 - val_mae: 10.4089\n",
      "Epoch 611/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 976.2966 - mse: 976.2966 - mae: 18.1256\n",
      "Epoch 611: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 756.8217 - mse: 756.8217 - mae: 15.7095 - val_loss: 1873.8611 - val_mse: 1873.8611 - val_mae: 24.3055\n",
      "Epoch 612/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 305.8610 - mse: 305.8610 - mae: 11.9628\n",
      "Epoch 612: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 981.7290 - mse: 981.7290 - mae: 16.8756 - val_loss: 1090.4137 - val_mse: 1090.4137 - val_mae: 17.4679\n",
      "Epoch 613/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 118.3018 - mse: 118.3018 - mae: 8.1444\n",
      "Epoch 613: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 632.0964 - mse: 632.0964 - mae: 14.3430 - val_loss: 868.5505 - val_mse: 868.5505 - val_mae: 13.7642\n",
      "Epoch 614/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 317.7640 - mse: 317.7640 - mae: 11.4398\n",
      "Epoch 614: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 688.3580 - mse: 688.3580 - mae: 13.9906 - val_loss: 2554.3765 - val_mse: 2554.3765 - val_mae: 27.4647\n",
      "Epoch 615/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 660.1404 - mse: 660.1404 - mae: 18.0215\n",
      "Epoch 615: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 570.7662 - mse: 570.7662 - mae: 13.9272 - val_loss: 1370.5111 - val_mse: 1370.5111 - val_mae: 18.1823\n",
      "Epoch 616/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2122.0461 - mse: 2122.0461 - mae: 21.0933\n",
      "Epoch 616: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 837.0767 - mse: 837.0767 - mae: 14.6976 - val_loss: 541.0743 - val_mse: 541.0743 - val_mae: 11.7648\n",
      "Epoch 617/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 917.9734 - mse: 917.9734 - mae: 15.9713\n",
      "Epoch 617: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 886.4622 - mse: 886.4622 - mae: 16.1211 - val_loss: 1265.0132 - val_mse: 1265.0132 - val_mae: 18.5750\n",
      "Epoch 618/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 396.9440 - mse: 396.9440 - mae: 12.2847\n",
      "Epoch 618: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 560.2130 - mse: 560.2130 - mae: 14.3346 - val_loss: 1784.4913 - val_mse: 1784.4913 - val_mae: 22.7060\n",
      "Epoch 619/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 868.3840 - mse: 868.3840 - mae: 15.8866\n",
      "Epoch 619: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 657.8468 - mse: 657.8468 - mae: 13.7479 - val_loss: 685.2365 - val_mse: 685.2365 - val_mae: 13.1538\n",
      "Epoch 620/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 427.8410 - mse: 427.8410 - mae: 13.6703\n",
      "Epoch 620: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 513.9991 - mse: 513.9991 - mae: 13.8547 - val_loss: 844.5504 - val_mse: 844.5504 - val_mae: 15.7656\n",
      "Epoch 621/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 717.2709 - mse: 717.2709 - mae: 14.4924\n",
      "Epoch 621: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 553.4310 - mse: 553.4310 - mae: 13.5755 - val_loss: 1024.4999 - val_mse: 1024.4999 - val_mae: 17.6765\n",
      "Epoch 622/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 385.8541 - mse: 385.8541 - mae: 14.1364\n",
      "Epoch 622: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 398.5309 - mse: 398.5309 - mae: 12.5976 - val_loss: 1005.0750 - val_mse: 1005.0750 - val_mae: 16.1690\n",
      "Epoch 623/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 649.6881 - mse: 649.6881 - mae: 17.9989\n",
      "Epoch 623: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1058.5942 - mse: 1058.5942 - mae: 15.7148 - val_loss: 861.8265 - val_mse: 861.8265 - val_mae: 13.7799\n",
      "Epoch 624/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 493.3381 - mse: 493.3381 - mae: 13.0779\n",
      "Epoch 624: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 698.1572 - mse: 698.1572 - mae: 14.7787 - val_loss: 1057.0441 - val_mse: 1057.0441 - val_mae: 17.0060\n",
      "Epoch 625/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 816.2189 - mse: 816.2189 - mae: 15.7632\n",
      "Epoch 625: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 484.6474 - mse: 484.6474 - mae: 13.2275 - val_loss: 935.7716 - val_mse: 935.7716 - val_mae: 16.4921\n",
      "Epoch 626/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 461.8096 - mse: 461.8096 - mae: 12.7934\n",
      "Epoch 626: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 441.6445 - mse: 441.6445 - mae: 12.5405 - val_loss: 808.4860 - val_mse: 808.4860 - val_mae: 14.9277\n",
      "Epoch 627/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 224.0327 - mse: 224.0327 - mae: 11.4004\n",
      "Epoch 627: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 543.3215 - mse: 543.3215 - mae: 14.5751 - val_loss: 922.6355 - val_mse: 922.6355 - val_mae: 14.7846\n",
      "Epoch 628/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 166.2203 - mse: 166.2203 - mae: 8.4687\n",
      "Epoch 628: val_loss did not improve from 416.96744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 553.8887 - mse: 553.8887 - mae: 13.2905 - val_loss: 1978.5662 - val_mse: 1978.5662 - val_mae: 22.5874\n",
      "Epoch 629/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 181.3014 - mse: 181.3014 - mae: 10.1805\n",
      "Epoch 629: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 534.8708 - mse: 534.8708 - mae: 13.2769 - val_loss: 2098.9822 - val_mse: 2098.9822 - val_mae: 21.3652\n",
      "Epoch 630/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 987.7231 - mse: 987.7231 - mae: 14.3087\n",
      "Epoch 630: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 966.7196 - mse: 966.7196 - mae: 15.6613 - val_loss: 825.5438 - val_mse: 825.5438 - val_mae: 10.4345\n",
      "Epoch 631/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 714.2316 - mse: 714.2316 - mae: 14.0112\n",
      "Epoch 631: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 780.3659 - mse: 780.3659 - mae: 15.2728 - val_loss: 1081.4557 - val_mse: 1081.4557 - val_mae: 15.1077\n",
      "Epoch 632/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 802.4663 - mse: 802.4663 - mae: 18.7993\n",
      "Epoch 632: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 825.4672 - mse: 825.4672 - mae: 16.5372 - val_loss: 3762.2532 - val_mse: 3762.2532 - val_mae: 35.0937\n",
      "Epoch 633/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 936.2120 - mse: 936.2120 - mae: 16.1467\n",
      "Epoch 633: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1214.6210 - mse: 1214.6210 - mae: 17.2471 - val_loss: 1006.9951 - val_mse: 1006.9951 - val_mae: 14.1032\n",
      "Epoch 634/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 430.6384 - mse: 430.6384 - mae: 11.7439\n",
      "Epoch 634: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1019.6959 - mse: 1019.6959 - mae: 18.6368 - val_loss: 784.9924 - val_mse: 784.9924 - val_mae: 13.5204\n",
      "Epoch 635/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 605.6185 - mse: 605.6185 - mae: 14.3153\n",
      "Epoch 635: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 456.7826 - mse: 456.7826 - mae: 12.8392 - val_loss: 1618.5554 - val_mse: 1618.5554 - val_mae: 23.1870\n",
      "Epoch 636/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 451.7492 - mse: 451.7492 - mae: 14.5439\n",
      "Epoch 636: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 459.7778 - mse: 459.7778 - mae: 13.1065 - val_loss: 1593.1853 - val_mse: 1593.1853 - val_mae: 22.0254\n",
      "Epoch 637/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1423.9253 - mse: 1423.9253 - mae: 19.6465\n",
      "Epoch 637: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 576.6376 - mse: 576.6376 - mae: 13.1723 - val_loss: 2713.2556 - val_mse: 2713.2556 - val_mae: 25.4244\n",
      "Epoch 638/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 169.9685 - mse: 169.9685 - mae: 8.9436\n",
      "Epoch 638: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 673.7091 - mse: 673.7091 - mae: 14.8458 - val_loss: 1172.4390 - val_mse: 1172.4390 - val_mae: 13.4282\n",
      "Epoch 639/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 615.8381 - mse: 615.8381 - mae: 13.1538\n",
      "Epoch 639: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1181.1173 - mse: 1181.1173 - mae: 16.1450 - val_loss: 1186.3885 - val_mse: 1186.3885 - val_mae: 14.3323\n",
      "Epoch 640/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 166.5970 - mse: 166.5970 - mae: 8.3164\n",
      "Epoch 640: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 626.7543 - mse: 626.7543 - mae: 13.8973 - val_loss: 2508.7473 - val_mse: 2508.7473 - val_mae: 25.7098\n",
      "Epoch 641/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1467.1644 - mse: 1467.1644 - mae: 19.4663\n",
      "Epoch 641: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1191.3533 - mse: 1191.3533 - mae: 17.6649 - val_loss: 1034.0955 - val_mse: 1034.0955 - val_mae: 13.8029\n",
      "Epoch 642/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1527.9460 - mse: 1527.9460 - mae: 22.0948\n",
      "Epoch 642: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 846.6462 - mse: 846.6462 - mae: 15.0228 - val_loss: 1237.7598 - val_mse: 1237.7598 - val_mae: 16.8749\n",
      "Epoch 643/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 327.5735 - mse: 327.5735 - mae: 11.7120\n",
      "Epoch 643: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 892.1604 - mse: 892.1604 - mae: 15.5102 - val_loss: 603.3715 - val_mse: 603.3715 - val_mae: 12.6684\n",
      "Epoch 644/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 603.1372 - mse: 603.1372 - mae: 14.2777\n",
      "Epoch 644: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 652.9873 - mse: 652.9873 - mae: 13.9094 - val_loss: 915.9402 - val_mse: 915.9402 - val_mae: 16.5169\n",
      "Epoch 645/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 380.5088 - mse: 380.5088 - mae: 12.1421\n",
      "Epoch 645: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 427.6908 - mse: 427.6908 - mae: 12.2568 - val_loss: 2894.3774 - val_mse: 2894.3774 - val_mae: 29.4328\n",
      "Epoch 646/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 783.4805 - mse: 783.4805 - mae: 15.9820\n",
      "Epoch 646: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1137.2422 - mse: 1137.2422 - mae: 17.6959 - val_loss: 1681.6024 - val_mse: 1681.6027 - val_mae: 19.3622\n",
      "Epoch 647/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1140.1104 - mse: 1140.1104 - mae: 18.7177\n",
      "Epoch 647: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1111.4662 - mse: 1111.4662 - mae: 16.3145 - val_loss: 760.6041 - val_mse: 760.6041 - val_mae: 11.0712\n",
      "Epoch 648/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 539.9495 - mse: 539.9495 - mae: 11.8507\n",
      "Epoch 648: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 580.6875 - mse: 580.6875 - mae: 14.2401 - val_loss: 689.7998 - val_mse: 689.7998 - val_mae: 13.4796\n",
      "Epoch 649/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 407.8201 - mse: 407.8201 - mae: 12.1639\n",
      "Epoch 649: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 578.5920 - mse: 578.5920 - mae: 12.7719 - val_loss: 1667.7808 - val_mse: 1667.7808 - val_mae: 24.1244\n",
      "Epoch 650/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 890.0803 - mse: 890.0803 - mae: 17.1516\n",
      "Epoch 650: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 650.6094 - mse: 650.6094 - mae: 14.3391 - val_loss: 1351.2642 - val_mse: 1351.2642 - val_mae: 21.4408\n",
      "Epoch 651/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 442.6839 - mse: 442.6839 - mae: 15.1503\n",
      "Epoch 651: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 626.9050 - mse: 626.9050 - mae: 14.1301 - val_loss: 1310.6414 - val_mse: 1310.6414 - val_mae: 19.4928\n",
      "Epoch 652/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 875.3378 - mse: 875.3378 - mae: 17.9451\n",
      "Epoch 652: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 538.4868 - mse: 538.4868 - mae: 13.4862 - val_loss: 496.8315 - val_mse: 496.8315 - val_mae: 10.4507\n",
      "Epoch 653/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 284.4911 - mse: 284.4911 - mae: 9.8374\n",
      "Epoch 653: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 628.2296 - mse: 628.2296 - mae: 13.9269 - val_loss: 1466.6516 - val_mse: 1466.6516 - val_mae: 22.1132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 654/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 425.0049 - mse: 425.0049 - mae: 13.4505\n",
      "Epoch 654: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 756.6930 - mse: 756.6930 - mae: 16.2053 - val_loss: 1741.8204 - val_mse: 1741.8204 - val_mae: 24.9097\n",
      "Epoch 655/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 604.2764 - mse: 604.2764 - mae: 13.7060\n",
      "Epoch 655: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 536.4194 - mse: 536.4194 - mae: 13.8657 - val_loss: 606.5729 - val_mse: 606.5729 - val_mae: 13.1696\n",
      "Epoch 656/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 343.0239 - mse: 343.0239 - mae: 12.5702\n",
      "Epoch 656: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 679.3857 - mse: 679.3857 - mae: 14.7470 - val_loss: 684.2629 - val_mse: 684.2629 - val_mae: 12.7611\n",
      "Epoch 657/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 845.5540 - mse: 845.5540 - mae: 15.9478\n",
      "Epoch 657: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 622.8818 - mse: 622.8818 - mae: 13.2741 - val_loss: 1681.4799 - val_mse: 1681.4799 - val_mae: 23.1264\n",
      "Epoch 658/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 432.2289 - mse: 432.2289 - mae: 12.0944\n",
      "Epoch 658: val_loss did not improve from 416.96744\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 723.4745 - mse: 723.4745 - mae: 14.7481 - val_loss: 865.2929 - val_mse: 865.2929 - val_mae: 15.1084\n",
      "Epoch 659/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1062.3167 - mse: 1062.3167 - mae: 18.7320\n",
      "Epoch 659: val_loss improved from 416.96744 to 341.46194, saving model to checkpoint-epoch-1000-batch-32-trial-001.h5\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 619.8516 - mse: 619.8516 - mae: 14.6233 - val_loss: 341.4619 - val_mse: 341.4619 - val_mae: 10.4529\n",
      "Epoch 660/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 790.6168 - mse: 790.6168 - mae: 16.4348\n",
      "Epoch 660: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 468.5805 - mse: 468.5805 - mae: 13.4815 - val_loss: 1269.2661 - val_mse: 1269.2661 - val_mae: 20.1184\n",
      "Epoch 661/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 436.5531 - mse: 436.5531 - mae: 13.4310\n",
      "Epoch 661: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 581.7199 - mse: 581.7199 - mae: 14.2131 - val_loss: 791.1523 - val_mse: 791.1523 - val_mae: 15.6571\n",
      "Epoch 662/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 684.5630 - mse: 684.5630 - mae: 14.9358\n",
      "Epoch 662: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 661.4967 - mse: 661.4967 - mae: 14.4765 - val_loss: 585.6395 - val_mse: 585.6395 - val_mae: 13.0932\n",
      "Epoch 663/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 932.9623 - mse: 932.9623 - mae: 17.1636\n",
      "Epoch 663: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 639.8401 - mse: 639.8401 - mae: 14.4222 - val_loss: 1236.4512 - val_mse: 1236.4512 - val_mae: 19.5670\n",
      "Epoch 664/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 338.4781 - mse: 338.4781 - mae: 11.2642\n",
      "Epoch 664: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 734.4996 - mse: 734.4996 - mae: 14.8730 - val_loss: 993.2851 - val_mse: 993.2851 - val_mae: 16.2040\n",
      "Epoch 665/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 275.2521 - mse: 275.2521 - mae: 10.9741\n",
      "Epoch 665: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 419.9634 - mse: 419.9634 - mae: 12.2092 - val_loss: 785.1392 - val_mse: 785.1392 - val_mae: 13.8702\n",
      "Epoch 666/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 184.0393 - mse: 184.0393 - mae: 10.1770\n",
      "Epoch 666: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 443.8625 - mse: 443.8625 - mae: 13.3348 - val_loss: 1851.4935 - val_mse: 1851.4935 - val_mae: 23.4106\n",
      "Epoch 667/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1782.8479 - mse: 1782.8479 - mae: 20.9596\n",
      "Epoch 667: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 730.4502 - mse: 730.4502 - mae: 14.9346 - val_loss: 1135.7461 - val_mse: 1135.7461 - val_mae: 17.8990\n",
      "Epoch 668/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 521.0648 - mse: 521.0648 - mae: 10.4557\n",
      "Epoch 668: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 482.0586 - mse: 482.0586 - mae: 12.9079 - val_loss: 915.1378 - val_mse: 915.1378 - val_mae: 15.4159\n",
      "Epoch 669/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 384.2878 - mse: 384.2878 - mae: 10.0026\n",
      "Epoch 669: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 810.0835 - mse: 810.0835 - mae: 15.0469 - val_loss: 1948.6378 - val_mse: 1948.6378 - val_mae: 23.7212\n",
      "Epoch 670/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 557.3174 - mse: 557.3174 - mae: 12.7529\n",
      "Epoch 670: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1118.4489 - mse: 1118.4489 - mae: 16.6912 - val_loss: 2561.9741 - val_mse: 2561.9741 - val_mae: 24.4586\n",
      "Epoch 671/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1570.6459 - mse: 1570.6459 - mae: 20.5691\n",
      "Epoch 671: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 743.6285 - mse: 743.6285 - mae: 14.8058 - val_loss: 1092.4875 - val_mse: 1092.4875 - val_mae: 12.4619\n",
      "Epoch 672/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 361.1339 - mse: 361.1339 - mae: 12.3735\n",
      "Epoch 672: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 523.4623 - mse: 523.4623 - mae: 12.8272 - val_loss: 1201.0920 - val_mse: 1201.0920 - val_mae: 14.7703\n",
      "Epoch 673/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 328.8233 - mse: 328.8233 - mae: 11.9927\n",
      "Epoch 673: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 805.5940 - mse: 805.5940 - mae: 15.7179 - val_loss: 910.8821 - val_mse: 910.8821 - val_mae: 14.5875\n",
      "Epoch 674/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1061.0054 - mse: 1061.0054 - mae: 17.1719\n",
      "Epoch 674: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 687.7052 - mse: 687.7052 - mae: 13.8424 - val_loss: 451.6560 - val_mse: 451.6560 - val_mae: 12.4789\n",
      "Epoch 675/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1761.6941 - mse: 1761.6941 - mae: 24.3281\n",
      "Epoch 675: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1178.9087 - mse: 1178.9087 - mae: 16.7337 - val_loss: 1191.4684 - val_mse: 1191.4684 - val_mae: 22.1454\n",
      "Epoch 676/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 369.2767 - mse: 369.2767 - mae: 12.4763\n",
      "Epoch 676: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1180.4230 - mse: 1180.4230 - mae: 19.6827 - val_loss: 1838.2649 - val_mse: 1838.2649 - val_mae: 26.8154\n",
      "Epoch 677/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 394.9645 - mse: 394.9645 - mae: 12.4096\n",
      "Epoch 677: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 990.7155 - mse: 990.7155 - mae: 15.3195 - val_loss: 1129.8770 - val_mse: 1129.8770 - val_mae: 18.9151\n",
      "Epoch 678/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 603.0248 - mse: 603.0248 - mae: 14.0528\n",
      "Epoch 678: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 741.9691 - mse: 741.9691 - mae: 14.9954 - val_loss: 1646.9910 - val_mse: 1646.9910 - val_mae: 21.1083\n",
      "Epoch 679/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/6 [====>.........................] - ETA: 0s - loss: 326.9654 - mse: 326.9654 - mae: 10.4067\n",
      "Epoch 679: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 685.5154 - mse: 685.5154 - mae: 13.8422 - val_loss: 586.7076 - val_mse: 586.7076 - val_mae: 10.4933\n",
      "Epoch 680/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 565.1931 - mse: 565.1931 - mae: 16.5452\n",
      "Epoch 680: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 302.4464 - mse: 302.4464 - mae: 11.6571 - val_loss: 656.1135 - val_mse: 656.1135 - val_mae: 12.4689\n",
      "Epoch 681/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 261.7900 - mse: 261.7900 - mae: 10.9366\n",
      "Epoch 681: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 477.9530 - mse: 477.9530 - mae: 12.7194 - val_loss: 751.9361 - val_mse: 751.9361 - val_mae: 14.3616\n",
      "Epoch 682/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 852.8378 - mse: 852.8378 - mae: 12.7440\n",
      "Epoch 682: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 941.6702 - mse: 941.6702 - mae: 14.4526 - val_loss: 2118.4180 - val_mse: 2118.4180 - val_mae: 25.4862\n",
      "Epoch 683/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 616.4991 - mse: 616.4991 - mae: 16.1902\n",
      "Epoch 683: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 839.0001 - mse: 839.0001 - mae: 15.9738 - val_loss: 1172.9659 - val_mse: 1172.9659 - val_mae: 15.8410\n",
      "Epoch 684/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 617.6326 - mse: 617.6326 - mae: 12.3737\n",
      "Epoch 684: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 762.4757 - mse: 762.4757 - mae: 14.0892 - val_loss: 791.0086 - val_mse: 791.0086 - val_mae: 11.4237\n",
      "Epoch 685/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 940.8044 - mse: 940.8044 - mae: 17.8410\n",
      "Epoch 685: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 692.6747 - mse: 692.6747 - mae: 14.5030 - val_loss: 1443.1339 - val_mse: 1443.1338 - val_mae: 18.4195\n",
      "Epoch 686/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1241.4995 - mse: 1241.4995 - mae: 20.4265\n",
      "Epoch 686: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 881.9210 - mse: 881.9210 - mae: 15.1572 - val_loss: 1105.0831 - val_mse: 1105.0831 - val_mae: 14.6581\n",
      "Epoch 687/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 564.1523 - mse: 564.1523 - mae: 15.2183\n",
      "Epoch 687: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 611.8275 - mse: 611.8275 - mae: 14.2585 - val_loss: 744.4897 - val_mse: 744.4897 - val_mae: 12.4366\n",
      "Epoch 688/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 960.9150 - mse: 960.9150 - mae: 14.0835\n",
      "Epoch 688: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 968.4490 - mse: 968.4490 - mae: 15.9622 - val_loss: 1315.6324 - val_mse: 1315.6324 - val_mae: 19.0181\n",
      "Epoch 689/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 700.8611 - mse: 700.8611 - mae: 11.3748\n",
      "Epoch 689: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 530.0541 - mse: 530.0541 - mae: 12.3952 - val_loss: 1175.9014 - val_mse: 1175.9014 - val_mae: 18.5675\n",
      "Epoch 690/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 576.5906 - mse: 576.5906 - mae: 14.2636\n",
      "Epoch 690: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 822.8776 - mse: 822.8776 - mae: 15.2877 - val_loss: 1281.9843 - val_mse: 1281.9843 - val_mae: 19.2912\n",
      "Epoch 691/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 808.7050 - mse: 808.7050 - mae: 18.2909\n",
      "Epoch 691: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 518.9038 - mse: 518.9038 - mae: 13.7294 - val_loss: 993.0391 - val_mse: 993.0391 - val_mae: 15.6305\n",
      "Epoch 692/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 435.1929 - mse: 435.1929 - mae: 13.4090\n",
      "Epoch 692: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 678.6439 - mse: 678.6439 - mae: 13.7648 - val_loss: 1299.7306 - val_mse: 1299.7306 - val_mae: 17.6063\n",
      "Epoch 693/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 591.6954 - mse: 591.6954 - mae: 13.6296\n",
      "Epoch 693: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 599.8306 - mse: 599.8306 - mae: 13.5351 - val_loss: 732.6155 - val_mse: 732.6155 - val_mae: 12.6532\n",
      "Epoch 694/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 853.4053 - mse: 853.4053 - mae: 13.5397\n",
      "Epoch 694: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 856.7267 - mse: 856.7267 - mae: 13.7804 - val_loss: 748.0215 - val_mse: 748.0215 - val_mae: 16.0289\n",
      "Epoch 695/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1539.1603 - mse: 1539.1603 - mae: 19.1975\n",
      "Epoch 695: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 707.2780 - mse: 707.2780 - mae: 14.7983 - val_loss: 982.4152 - val_mse: 982.4152 - val_mae: 20.0477\n",
      "Epoch 696/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 870.8730 - mse: 870.8730 - mae: 13.7707\n",
      "Epoch 696: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 859.1747 - mse: 859.1747 - mae: 14.9954 - val_loss: 389.5648 - val_mse: 389.5648 - val_mae: 13.1990\n",
      "Epoch 697/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2945.1411 - mse: 2945.1411 - mae: 26.8669\n",
      "Epoch 697: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1287.0382 - mse: 1287.0382 - mae: 17.9265 - val_loss: 3262.4775 - val_mse: 3262.4775 - val_mae: 35.2948\n",
      "Epoch 698/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 746.8212 - mse: 746.8212 - mae: 15.8525\n",
      "Epoch 698: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1623.2483 - mse: 1623.2483 - mae: 20.9992 - val_loss: 1107.7666 - val_mse: 1107.7666 - val_mae: 17.0797\n",
      "Epoch 699/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 386.8345 - mse: 386.8345 - mae: 12.3967\n",
      "Epoch 699: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 823.0347 - mse: 823.0347 - mae: 15.4178 - val_loss: 601.5441 - val_mse: 601.5441 - val_mae: 10.9532\n",
      "Epoch 700/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 617.3060 - mse: 617.3060 - mae: 14.2251\n",
      "Epoch 700: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 767.4127 - mse: 767.4127 - mae: 14.7658 - val_loss: 2403.2131 - val_mse: 2403.2131 - val_mae: 26.3702\n",
      "Epoch 701/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1320.8712 - mse: 1320.8712 - mae: 21.0676\n",
      "Epoch 701: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1242.8530 - mse: 1242.8530 - mae: 17.3902 - val_loss: 1480.1040 - val_mse: 1480.1040 - val_mae: 19.7674\n",
      "Epoch 702/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1305.6038 - mse: 1305.6038 - mae: 16.4900\n",
      "Epoch 702: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 809.8221 - mse: 809.8221 - mae: 14.8885 - val_loss: 555.0887 - val_mse: 555.0887 - val_mae: 10.8455\n",
      "Epoch 703/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 403.3998 - mse: 403.3998 - mae: 14.8027\n",
      "Epoch 703: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 780.1491 - mse: 780.1491 - mae: 14.9027 - val_loss: 1399.0773 - val_mse: 1399.0773 - val_mae: 19.0705\n",
      "Epoch 704/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 271.3326 - mse: 271.3326 - mae: 10.9399\n",
      "Epoch 704: val_loss did not improve from 341.46194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 10ms/step - loss: 501.7874 - mse: 501.7874 - mae: 12.7458 - val_loss: 913.4304 - val_mse: 913.4304 - val_mae: 13.8794\n",
      "Epoch 705/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 316.4249 - mse: 316.4249 - mae: 12.1414\n",
      "Epoch 705: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 483.0463 - mse: 483.0463 - mae: 13.1955 - val_loss: 1030.9397 - val_mse: 1030.9397 - val_mae: 15.1720\n",
      "Epoch 706/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 457.7943 - mse: 457.7943 - mae: 13.3009\n",
      "Epoch 706: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 733.3237 - mse: 733.3237 - mae: 15.0417 - val_loss: 982.5657 - val_mse: 982.5657 - val_mae: 15.0630\n",
      "Epoch 707/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 172.4151 - mse: 172.4151 - mae: 8.1772\n",
      "Epoch 707: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 387.0954 - mse: 387.0954 - mae: 11.4838 - val_loss: 1090.5640 - val_mse: 1090.5640 - val_mae: 17.6689\n",
      "Epoch 708/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 297.2894 - mse: 297.2894 - mae: 11.9437\n",
      "Epoch 708: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 803.2962 - mse: 803.2962 - mae: 15.4883 - val_loss: 845.3363 - val_mse: 845.3362 - val_mae: 16.0649\n",
      "Epoch 709/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 179.3071 - mse: 179.3071 - mae: 9.7093\n",
      "Epoch 709: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 470.1058 - mse: 470.1058 - mae: 12.9199 - val_loss: 1369.3796 - val_mse: 1369.3796 - val_mae: 19.9180\n",
      "Epoch 710/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2363.1899 - mse: 2363.1899 - mae: 22.4586\n",
      "Epoch 710: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 909.3341 - mse: 909.3341 - mae: 14.8697 - val_loss: 558.4152 - val_mse: 558.4152 - val_mae: 11.0122\n",
      "Epoch 711/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 734.9580 - mse: 734.9580 - mae: 15.4389\n",
      "Epoch 711: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 550.5578 - mse: 550.5578 - mae: 12.4175 - val_loss: 1560.5447 - val_mse: 1560.5447 - val_mae: 20.6948\n",
      "Epoch 712/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 310.5457 - mse: 310.5457 - mae: 10.8835\n",
      "Epoch 712: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1069.4374 - mse: 1069.4374 - mae: 15.9672 - val_loss: 2487.2769 - val_mse: 2487.2769 - val_mae: 25.7646\n",
      "Epoch 713/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1118.6764 - mse: 1118.6764 - mae: 15.7803\n",
      "Epoch 713: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 794.4688 - mse: 794.4688 - mae: 14.5262 - val_loss: 1001.0911 - val_mse: 1001.0911 - val_mae: 13.5126\n",
      "Epoch 714/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 528.4565 - mse: 528.4565 - mae: 11.7130\n",
      "Epoch 714: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 612.6464 - mse: 612.6464 - mae: 13.6797 - val_loss: 994.1020 - val_mse: 994.1020 - val_mae: 14.9040\n",
      "Epoch 715/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 380.6053 - mse: 380.6053 - mae: 12.4420\n",
      "Epoch 715: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 552.9534 - mse: 552.9534 - mae: 13.4680 - val_loss: 941.6429 - val_mse: 941.6429 - val_mae: 15.4213\n",
      "Epoch 716/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1210.7053 - mse: 1210.7053 - mae: 17.6112\n",
      "Epoch 716: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 659.3655 - mse: 659.3655 - mae: 12.6501 - val_loss: 1292.1726 - val_mse: 1292.1726 - val_mae: 19.0712\n",
      "Epoch 717/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 481.8929 - mse: 481.8929 - mae: 13.2251\n",
      "Epoch 717: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 692.9207 - mse: 692.9207 - mae: 14.5916 - val_loss: 872.6965 - val_mse: 872.6965 - val_mae: 15.2403\n",
      "Epoch 718/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 310.3819 - mse: 310.3819 - mae: 10.1781\n",
      "Epoch 718: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 520.2631 - mse: 520.2631 - mae: 13.4028 - val_loss: 1536.0066 - val_mse: 1536.0066 - val_mae: 19.9502\n",
      "Epoch 719/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1482.4159 - mse: 1482.4159 - mae: 18.9168\n",
      "Epoch 719: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 675.5343 - mse: 675.5343 - mae: 14.3236 - val_loss: 1182.4644 - val_mse: 1182.4644 - val_mae: 14.5961\n",
      "Epoch 720/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 293.8278 - mse: 293.8278 - mae: 9.4641\n",
      "Epoch 720: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 378.9483 - mse: 378.9483 - mae: 11.5871 - val_loss: 1428.6094 - val_mse: 1428.6095 - val_mae: 17.1826\n",
      "Epoch 721/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 321.2621 - mse: 321.2621 - mae: 12.0815\n",
      "Epoch 721: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 621.7260 - mse: 621.7260 - mae: 13.9493 - val_loss: 1299.1464 - val_mse: 1299.1464 - val_mae: 15.5960\n",
      "Epoch 722/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 154.5355 - mse: 154.5355 - mae: 8.4591\n",
      "Epoch 722: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 520.3650 - mse: 520.3650 - mae: 12.1713 - val_loss: 1078.5161 - val_mse: 1078.5161 - val_mae: 13.8860\n",
      "Epoch 723/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 450.8514 - mse: 450.8514 - mae: 12.7271\n",
      "Epoch 723: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 572.8372 - mse: 572.8372 - mae: 14.2914 - val_loss: 1270.9041 - val_mse: 1270.9041 - val_mae: 16.9435\n",
      "Epoch 724/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1976.4186 - mse: 1976.4186 - mae: 20.8520\n",
      "Epoch 724: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 788.6307 - mse: 788.6307 - mae: 14.2607 - val_loss: 710.9030 - val_mse: 710.9030 - val_mae: 13.1265\n",
      "Epoch 725/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 875.3258 - mse: 875.3258 - mae: 16.4708\n",
      "Epoch 725: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 684.1398 - mse: 684.1398 - mae: 13.1975 - val_loss: 673.1360 - val_mse: 673.1360 - val_mae: 14.0635\n",
      "Epoch 726/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 489.5851 - mse: 489.5851 - mae: 14.0524\n",
      "Epoch 726: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 595.3131 - mse: 595.3131 - mae: 13.8942 - val_loss: 796.1097 - val_mse: 796.1097 - val_mae: 17.1307\n",
      "Epoch 727/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 568.4248 - mse: 568.4248 - mae: 11.6268\n",
      "Epoch 727: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1081.1353 - mse: 1081.1353 - mae: 17.0484 - val_loss: 1775.0851 - val_mse: 1775.0851 - val_mae: 25.0814\n",
      "Epoch 728/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 858.0649 - mse: 858.0649 - mae: 13.5250\n",
      "Epoch 728: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 931.1682 - mse: 931.1682 - mae: 16.5233 - val_loss: 1726.9305 - val_mse: 1726.9305 - val_mae: 22.4896\n",
      "Epoch 729/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 415.7073 - mse: 415.7073 - mae: 14.8440\n",
      "Epoch 729: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 654.4448 - mse: 654.4448 - mae: 14.9841 - val_loss: 920.3398 - val_mse: 920.3398 - val_mae: 13.5851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 730/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 587.4537 - mse: 587.4537 - mae: 14.0812\n",
      "Epoch 730: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 734.7360 - mse: 734.7360 - mae: 14.8532 - val_loss: 711.2316 - val_mse: 711.2316 - val_mae: 13.3251\n",
      "Epoch 731/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 530.3241 - mse: 530.3241 - mae: 13.1068\n",
      "Epoch 731: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 704.1786 - mse: 704.1786 - mae: 14.7558 - val_loss: 1655.0601 - val_mse: 1655.0601 - val_mae: 20.8631\n",
      "Epoch 732/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 950.0964 - mse: 950.0964 - mae: 16.9525\n",
      "Epoch 732: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 748.0469 - mse: 748.0469 - mae: 14.8210 - val_loss: 929.3590 - val_mse: 929.3590 - val_mae: 15.6045\n",
      "Epoch 733/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 894.3894 - mse: 894.3894 - mae: 16.8501\n",
      "Epoch 733: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 505.5341 - mse: 505.5341 - mae: 12.9448 - val_loss: 828.3073 - val_mse: 828.3073 - val_mae: 15.4147\n",
      "Epoch 734/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1172.6775 - mse: 1172.6775 - mae: 18.2306\n",
      "Epoch 734: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 627.0558 - mse: 627.0558 - mae: 13.0786 - val_loss: 1486.4136 - val_mse: 1486.4136 - val_mae: 20.4633\n",
      "Epoch 735/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 892.6287 - mse: 892.6287 - mae: 15.4919\n",
      "Epoch 735: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 743.3207 - mse: 743.3207 - mae: 14.2486 - val_loss: 1074.2542 - val_mse: 1074.2542 - val_mae: 14.6588\n",
      "Epoch 736/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 294.9434 - mse: 294.9434 - mae: 11.4726\n",
      "Epoch 736: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 563.7319 - mse: 563.7319 - mae: 13.4304 - val_loss: 909.1011 - val_mse: 909.1011 - val_mae: 13.7897\n",
      "Epoch 737/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 186.0723 - mse: 186.0723 - mae: 9.1856\n",
      "Epoch 737: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 468.9599 - mse: 468.9599 - mae: 12.2851 - val_loss: 970.4545 - val_mse: 970.4545 - val_mae: 16.2075\n",
      "Epoch 738/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 306.5268 - mse: 306.5268 - mae: 11.3624\n",
      "Epoch 738: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 579.1497 - mse: 579.1497 - mae: 13.1120 - val_loss: 3164.2854 - val_mse: 3164.2854 - val_mae: 29.9763\n",
      "Epoch 739/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 485.1333 - mse: 485.1333 - mae: 12.9709\n",
      "Epoch 739: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1124.6678 - mse: 1124.6678 - mae: 16.9496 - val_loss: 857.8150 - val_mse: 857.8150 - val_mae: 10.7991\n",
      "Epoch 740/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 799.6948 - mse: 799.6948 - mae: 16.9335\n",
      "Epoch 740: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 932.8086 - mse: 932.8086 - mae: 17.3655 - val_loss: 471.1573 - val_mse: 471.1573 - val_mae: 10.8415\n",
      "Epoch 741/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 2744.8970 - mse: 2744.8970 - mae: 24.0222\n",
      "Epoch 741: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1121.0454 - mse: 1121.0454 - mae: 16.3506 - val_loss: 2644.4565 - val_mse: 2644.4565 - val_mae: 28.9084\n",
      "Epoch 742/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1075.3015 - mse: 1075.3015 - mae: 21.0498\n",
      "Epoch 742: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1017.6879 - mse: 1017.6879 - mae: 16.9598 - val_loss: 1308.4958 - val_mse: 1308.4958 - val_mae: 18.8299\n",
      "Epoch 743/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 398.8721 - mse: 398.8721 - mae: 12.5322\n",
      "Epoch 743: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 389.3903 - mse: 389.3903 - mae: 11.7410 - val_loss: 411.2623 - val_mse: 411.2623 - val_mae: 11.5592\n",
      "Epoch 744/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 347.5844 - mse: 347.5844 - mae: 12.7044\n",
      "Epoch 744: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 569.1215 - mse: 569.1215 - mae: 14.1358 - val_loss: 915.4666 - val_mse: 915.4666 - val_mae: 16.6362\n",
      "Epoch 745/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 220.3070 - mse: 220.3070 - mae: 9.7391\n",
      "Epoch 745: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 438.2057 - mse: 438.2057 - mae: 12.3781 - val_loss: 931.9465 - val_mse: 931.9466 - val_mae: 17.2673\n",
      "Epoch 746/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 555.1117 - mse: 555.1117 - mae: 13.7815\n",
      "Epoch 746: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 857.1174 - mse: 857.1174 - mae: 15.7476 - val_loss: 839.4887 - val_mse: 839.4887 - val_mae: 16.3627\n",
      "Epoch 747/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 550.4061 - mse: 550.4061 - mae: 14.5633\n",
      "Epoch 747: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 712.4895 - mse: 712.4896 - mae: 15.1984 - val_loss: 1002.0452 - val_mse: 1002.0452 - val_mae: 16.7076\n",
      "Epoch 748/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 274.7023 - mse: 274.7023 - mae: 10.0831\n",
      "Epoch 748: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 496.8625 - mse: 496.8625 - mae: 13.1812 - val_loss: 806.8964 - val_mse: 806.8964 - val_mae: 15.4878\n",
      "Epoch 749/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 689.1210 - mse: 689.1210 - mae: 14.6644\n",
      "Epoch 749: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 604.4019 - mse: 604.4019 - mae: 13.5526 - val_loss: 943.2967 - val_mse: 943.2967 - val_mae: 16.6581\n",
      "Epoch 750/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 538.2544 - mse: 538.2544 - mae: 12.8911\n",
      "Epoch 750: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 590.9798 - mse: 590.9798 - mae: 13.6088 - val_loss: 510.5278 - val_mse: 510.5278 - val_mae: 13.0985\n",
      "Epoch 751/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 915.9710 - mse: 915.9710 - mae: 14.2190\n",
      "Epoch 751: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1114.0667 - mse: 1114.0667 - mae: 16.1753 - val_loss: 1917.1254 - val_mse: 1917.1254 - val_mae: 25.0794\n",
      "Epoch 752/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 564.7128 - mse: 564.7128 - mae: 15.3691\n",
      "Epoch 752: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 610.1827 - mse: 610.1827 - mae: 14.2425 - val_loss: 1194.3549 - val_mse: 1194.3549 - val_mae: 18.2179\n",
      "Epoch 753/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 857.3782 - mse: 857.3782 - mae: 14.9283\n",
      "Epoch 753: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 714.1247 - mse: 714.1247 - mae: 13.9080 - val_loss: 393.7752 - val_mse: 393.7752 - val_mae: 10.1080\n",
      "Epoch 754/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 521.1650 - mse: 521.1650 - mae: 14.6249\n",
      "Epoch 754: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 508.4695 - mse: 508.4695 - mae: 14.3888 - val_loss: 878.0356 - val_mse: 878.0356 - val_mae: 14.6842\n",
      "Epoch 755/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 264.2690 - mse: 264.2690 - mae: 10.2840\n",
      "Epoch 755: val_loss did not improve from 341.46194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 9ms/step - loss: 626.5079 - mse: 626.5079 - mae: 13.5249 - val_loss: 1006.1224 - val_mse: 1006.1224 - val_mae: 16.5249\n",
      "Epoch 756/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 544.9999 - mse: 544.9999 - mae: 12.4134\n",
      "Epoch 756: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 537.7071 - mse: 537.7071 - mae: 12.8065 - val_loss: 673.2550 - val_mse: 673.2550 - val_mae: 13.4167\n",
      "Epoch 757/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1980.2305 - mse: 1980.2305 - mae: 19.6267\n",
      "Epoch 757: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 718.5400 - mse: 718.5400 - mae: 13.2326 - val_loss: 1355.4547 - val_mse: 1355.4547 - val_mae: 18.4521\n",
      "Epoch 758/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1923.4940 - mse: 1923.4940 - mae: 23.9339\n",
      "Epoch 758: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 677.4564 - mse: 677.4564 - mae: 13.9825 - val_loss: 728.2115 - val_mse: 728.2115 - val_mae: 12.5803\n",
      "Epoch 759/1000\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 1060.3068 - mse: 1060.3068 - mae: 14.5594\n",
      "Epoch 759: val_loss did not improve from 341.46194\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 870.1273 - mse: 870.1273 - mae: 14.8299 - val_loss: 1245.9241 - val_mse: 1245.9241 - val_mae: 17.7758\n"
     ]
    }
   ],
   "source": [
    "hist_custom = model_custom.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data =(X_val, Y_val),callbacks=[earlystopping,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSGtzcBHohRt",
    "outputId": "475053d1-f125-436c-9536-f6b468fe8219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "예측값 :  [162.66104]\n",
      "정답 :  tf.Tensor(171, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "model_custom.load_weights(ckpt_name)\n",
    "preds = model_custom.predict(X_val, batch_size=128)\n",
    "print('예측값 : ', preds[0])\n",
    "print('정답 : ', Y_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.980988226638114\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(Y_val, preds)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "day6_정답_코드.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
